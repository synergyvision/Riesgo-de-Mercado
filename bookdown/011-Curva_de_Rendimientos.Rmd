---
output:
  html_document: default
  pdf_document: default
---
\mainmatter

# Curva de Rendimientos

<!-- La curva de rendimientos es una herramienta utilizada ampliamente, por quienes toman las decisiones de política monetaria o planifican sus inversiones, de acuerdo con la valoración, negociación o cobertura sobre instrumentos financieros. Se ha comprobado que la geometría de la curva de rendimientos en los mercados desarrollados incorpora información relevante para la predicción de las recesiones económicas. De igual manera, en finanzas la curva de rendimientos se convierte en un vector de precios de referencia importante para la fijación de las tasas de interés a diferentes plazos y por riesgo de crédito para bancos, prestamistas, colocadores de bonos y, en general, para todos los participantes del mercado de dinero. -->

<!-- Los resultados revelan las bondades en el ajuste de las redes neuronales artificiales (RNA), la curva de Svensson, la curva de Nelson-Siegel y los polinomios locales. No obstante, se recomienda utilizar la curva de Svensson en la estimación de las tasas de interés, debido a la interpretabilidad de sus parámetros y a su superioridad sobre la -->
<!-- Curva de Nelson-Siegel. -->

La curva de rendimientos es una representación grafica que muestra la relación que existe entre los rendimientos de una clase particular de títulos valores y el tiempo que falta para su vencimiento, lo cual es conocido como la estructura temporal de la tasa de interés (ETTI) para instrumentos con riesgo similar pero con diferentes plazos de maduración. La ETTI es un indicador de la evolución futura de los tipos de interés y de inflación, además, la mayoría de los activos financieros se valoran mediante este indicador, por lo cual también se considera básico en el diseño de estrategias de gestión de riesgos y en la toma de decisiones de inversión y financiación. Existen cuatro formas que puede adoptar una curva de rendimientos:


+ **Curva ascendente**: generalmente, la curva de rendimientos tiene esta      forma, lo que indica que los inversionistas requieren mayores rendimientos     para vencimientos de más largo plazo, es decir, que los rendimientos         varían directamente con los plazos. 
+ **Curva descendente**: indica que los rendimientos disminuyen a medida que   aumentan los plazos.
+ **Curva horizontal**: indica que independientemente del plazo de vencimient   o, los rendimientos son los mismos; para períodos muy largos, todas las      curvas de rendimientos tienden a aplanarse.

+ **Curva horizontal**: indica que independientemente del plazo de vencimiento, los rendimientos son los mismos; para períodos muy largos, todas las      curvas de rendimientos tienden a aplanarse.

+ **Curva creciente y decreciente**: es el reflejo de una situación en la    que los rendimientos de corto y largo plazo son los mismos y los rendimientos  de mediano plazo son los que varían.

<!-- \begin{figure}[h] -->
<!--   \scalebox{0.80}{\includegraphics{images/tipo_curvas.jpg}} -->
<!-- \caption{Tipos de curva de rendimiento.} -->
<!-- \label{tipos_c} -->
<!-- \end{figure} -->


<!-- ![](images/tipo_curvas.jpg "Figura 1: Tipo de Curvas") -->


```{r , echo=FALSE, fig.align='center',fig.cap="Tipos de Curvas",label=tipos}
knitr::include_graphics("images/tipo_curvas.jpg")
```

Es de esperar que una pendiente negativa de la curva de rendimientos (Ver Figura \@ref(fig:tipos) ) o curva invertida (tasas de largo plazo menores a las de corto plazo) indique expectativas de una recesión futura y, por lo tanto, menores tasas de interés futuras; esto se puede explicar ya que los rendimientos esperados contienen información sobre los planes de consumo de los agentes. 

Entre las teorías que explican la pendiente de la curva de rendimientos, se encuentran:


+ **La teoría de la preferencia por la liquidez**: consiste en que los inversionistas prefieren manejar títulos a corto plazo, pues éstos tienen una sensibilidad menor a los cambios en las tasas de interés y ofrecen una mayor flexibilidad en las inversiones si se compara con los títulos de largo plazo. Además, los prestatarios prefieren deuda a largo plazo, pues la de corto plazo los expone al riesgo de hacer una refinanciación de la deuda en condiciones adversas. Ambas situaciones, generan entonces, tasas de corto plazo relativamente bajas. En su conjunto, estos dos grupos de preferencias implican que en condiciones normales existe una Prima de Riesgo por Vencimiento (PRV) que aumenta en función de los años de vencimiento, haciendo que la curva de rendimientos posea una pendiente ascendente \cite{DO}.

+ **La teoría de la segmentación del mercado**: considera el mercado de renta fija como una serie de distintos mercados, los inversionistas y los emisores están restringidos por el sector específico de maduración. De acuerdo con esta teoría, la curva de rendimientos refleja una serie de condiciones de oferta y demanda que crean una secuencia de precios de equilibrio de mercado (tasas de interés) de los fondos \cite{DO}.

+ **La teoría del Hábitat Preferido**: plantea que los inversionistas intentarán liquidar sus inversiones en el menor plazo posible mientras que los prestamistas querrán tomar un plazo más largo; por lo tanto, dado que no se encuentran oferta y demanda de fondos para un mismo plazo, algunos inversionistas o prestatarios se verán motivados a cambiar el plazo de la inversión o el financiamiento pero, para lograrlo, deben ser compensados con un premio por el riesgo cuyo tamano reflejará la extesión de la aversión al riesgo.

+ **La Hipótesis de las Expectativas (HE)**: plantea que las tasas de interés de largo plazo deben reflejar por completo la información revelada por las futuras tasas de interés de corto plazo esperadas \cite{YS}, o sea que los tipos de largo plazo no son más que una suma ponderada de los tipos de corto plazo esperados \cite{FR}. Así, se puede afirmar entonces que la HE es una teoría que plantea que las tasas de interés exclusivamente representan las tasas previstas en el futuro.

##Metodologías de estimación de la Curva de Rendimientos.

La curva de rendimientos presenta empíricamente una serie de dificultades, debido a que se construye a través de una serie de precios (tasas) de instrumentos financieros discontinuos en el tiempo que, por lo general, están lejos de ser una curva suave. Para su estimación existen diversas metodologías, las paramétricas y las no paramétricas. Las metodologías paramétricas se basan en modelos asociados a una familia funcional que obedece al comportamiento de alguna distribución de probabilidad, sobre la cual suponemos que las características de la población de interés pueden ser descritas. Es así como, los modelos diseñados en este contexto, basados en regresión, buscan describir el comportamiento de una variable de interés con otras llamadas exógenas, a través de funciones de vínculo lineales o no lineales.


## Metodologías Paramétricas.

Estadísticamente, un modelo paramétrico es una familia funcional que
obedece al comportamiento de alguna distribución de probabilidad, sobre la cual suponemos que las características de la población de interés
pueden ser descritas. Es así como, los modelos diseñados en este contexto,
basados en regresión, buscan describir el comportamiento de una
variable de interés con otras llamadas exógenas, a través de funciones de
vínculo lineales o no lineales.


### Metodología Nelson y Siegel

Nelson y Siegel introducen un modelo paramétrico para el ajuste
de los rendimientos hasta la madurez de los bonos del tesoro de Estados
Unidos que se caracteriza por ser parsimonioso y flexible en modelar
cualquier forma típica asociada con las curvas de rendimientos. La estructura
paramétrica asociada a este modelo permite analizar el comportamiento
a corto y a largo plazo de los rendimientos y ajustar -sin
esfuerzos adicionales-, curvas monótonas, unimodales o del tipo S.


Una clase de funciones que genera fácilmente las formas usuales de las
curvas de rendimientos es la asociada con la solución de ecuaciones en
diferencia. La teoría de expectativas sobre la estructura de las tasas de
interés promueve la investigación en este sentido, dado que si las tasas
spot son producidas por medio de una ecuación diferencial, entonces las
tasas forward -siendo pronósticos-, serán la solución de las ecuaciones
diferenciales. La expresión paramétrica propuesta por Nelson y Siegel
 que describe las tasas forward es exhibida a continuación:


$$\displaystyle{f(m) = \beta_{0} + \beta_{1} e^{\frac{-m}{\tau}} +\beta_{2} \left(\frac{-m}{\tau}\right)e^{\frac{-m}{\tau}}}$$

donde $m$ denota la madurez del activo y $\beta_{0}$, $\beta_{1}$, $\beta_{2}$ y $\tau$ los parámetros a ser
estimados. Puesto que las tasas spot pueden ser obtenidas a través de tasas
forward por medio de la expresión:

$$\displaystyle{s(m) = \int_{0}^{m}f(x)dx}$$

la ecuación que determina las tasas spot $s(m)$ de activos con madurez m es dada por:

$$\displaystyle{s(m) = \beta_{0}+ \beta_{1}\frac{\left(1-e^\frac{-m}{\tau}\right)}{m/\tau} + \beta_{2} \left(\frac{\left(1-e^\frac{-m}{\tau}\right)}{m/\tau} -  e^\frac{-m}{\tau}\right)}$$

cuya ecuación es lineal si conocemos $\tau$ .

El valor límite del rendimiento es $\beta_{0}$ cuando el plazo al vencimiento $m$ es grande, mientras que, cuando el plazo al vencimiento $m$ es pequeño el
rendimiento en el límite es $\beta_{0}+\beta_{1}$. Igualmente, los coeficientes del
modelo de tasas forward pueden ser interpretados como medidas de
fortaleza al corto, mediano y largo plazo. La contribución al largo plazo
es determinada por $\beta_{0}$, $\beta_{1}$ lo hace al corto plazo ponderado por la
función monótona creciente (decreciente) $e^{\frac{-m}{\tau}}$ cuando $\beta_{1}$ es negativo
(positivo) y $\beta_{2}$ lo hace al mediano plazo ponderado por la función
monótona creciente (decreciente) $(\frac{-m}{\tau}) e^{\frac{-m}{\tau}}$ cuando $\beta_{2}$ es negativo
(positivo). Una de las principales utilidades de la curva ha sido para
propósitos de control de la política monetaria.

Consecuentemente, $s(m)$ será la ecuación utilizada para captar la relación
subyacente entre los rendimientos y los plazos al vencimiento o madurez,
sin recurrir a modelos más complejos que involucren un mayor número
de parámetros. Adicionalmente, dado que la curva de Nelson-Siegel
proporciona tasas spot compuestas continuas, estas deben transformarse
en cantidades discretas, a través de la función de descuento.


$$\displaystyle{s_{d}(m) = e^{\frac{s(m)}{100}} - 1}$$

### Metodología Svensson

En la curva de Nelson-Siegel se destaca que cada coeficiente del modelo
contribuye en el comportamiento de las tasas forward en el corto,
mediano y largo plazo; no obstante, Svensson \cite{Sv} propone una nueva
versión de la curva de Nelson-Siegel donde un cuarto término es incluido
para producir un efecto adicional y semejante al proporcionado por:
$\beta_{3}(\frac{m}{\tau_{2}})e^{\frac{-m}{\tau_{2}}}$.

En este caso, la función para describir la dinámica de las tasas forward es,


$$\displaystyle{f(m) = \beta_{0} + \beta_{1} e^{\frac{-m}{\tau_{1}}} +\beta_{2} \left(\frac{-m}{\tau_{1}}\right)e^{\frac{-m}{\tau_{1}}} + \beta_{3}\left(\frac{-m}{\tau_{2}}\right)e^{\frac{-m}{\tau_{2}}} }$$


La curva spot de Svensson puede ser derivada a partir de la curva
forward en forma semejante a la descrita para el modelo de Nelson-
Siegel, obteniendo la siguiente expresión:

$$\displaystyle{s(m) = \beta_{0}+ \beta_{1}\frac{\left(1-e^\frac{-m}{\tau_{1}}\right)}{m/\tau_{1}} + \beta_{2} \left(\frac{\left(1-e^\frac{-m}{\tau_{1}}\right)}{m/\tau_{1}} -  e^\frac{-m}{\tau_{1}}\right) + \beta_{3} \left(\frac{\left(1-e^\frac{-m}{\tau_{2}}\right)}{m/\tau_{2}} -  e^\frac{-m}{\tau_{2}}\right)}$$


La función de descuento tiene que ser utilizada con el fin de obtener las
tasas estimadas para cada día de negociación o trading. Svensson \cite{Sv}
propone estimar los parámetros de la curva cero cupón (curva spot),
minimizando una medida de ajuste tal como la suma de cuadrados del
error sobre los precios spot; sin embargo, enfatiza en que los precios
pueden llegar a ser mal ajustados para los activos de madurez corta. En
lugar de llevar el análisis por este camino, propone estimar los
rendimientos fundamentado, principalmente, en que las decisiones de la
política económica se basan en el comportamiento de las tasas y que
obteniendo las tasas a través de la curva, los precios pueden ser
calculados una vez la función de descuento es evaluada. De esta manera,
los parámetros son escogidos minimizando la suma de cuadrados de la
diferencia entre los rendimientos observados y estimados por la curva.

La estimación es realizada por medio de máxima verosimilitud, mínimos
cuadrados no lineales o el método de momentos generalizados. En
muchos casos, como afirma Svensson \cite{Sv}, el modelo de Nelson-
Siegel proporciona ajustes satisfactorios, aunque en algunos casos
cuando la estructura de las tasas de interés es más compleja, el ajuste del
modelo de Nelson-Siegel es poco satisfactorio y el modelo de Svensson
logra desempeñarse mejor.

### Polinomios de componentes principales.

Hunt y Terry \cite{HT} propone un ajuste de la curva de rendimientos
utilizando polinomios. Si frecuentemente la curva es especificada como,

(@ecu1) 
$$
y(\tau) = \beta_{0} + \beta_{1}\tau +\beta_{2}\tau^2 +\beta_{3}\tau^3
$$

La cual puede captar todas la formas que puede asumir la curva, su
principal problema recae en el ajuste para aquellas tasas con períodos de
vencimiento bastante largos. Aunque los autores conocen sobre las
propiedades de parsimonia y de ajuste asociados con la curva de Nelson-
Siegel, critican los problemas que acarrea la estimación de sus
parámetros, proponiendo el ajuste de la curva de polinomios, bajo
algunas modificaciones.

Una transformación sobre el término de plazos ($\tau$) que remueve la
inestabilidad asociada con las tasas a largo plazo del polinomio de la ecuación @ecu1 es
sugerida. El modelo recomendado, siguiendo la notación de Hunt y
Terry (1998) es:

(@ecu2)
$$
y(\tau) = \beta_{0} + \sum_{i=1}^{p} \beta_{i} \frac{1}{(1+\tau)^i}
$$

donde,

$$\displaystyle{y(0) = \sum_{i=0}^{p}\beta_{i},   \text{ con } y(\infty) = \beta_{0}   }$$


Investigaciones relacionadas con curvas de rendimientos, han llegado a
la conclusión que modelos con tres o cuatro parámetros son suficientes
para obtener un buen ajuste de los datos (Hunt \cite{H}). Por tal motivo,
Hunt y Terry \cite{HT} proponen restringir $p$ a tres o cuatro. Aunque este
número de parámetros no necesariamente determina si realmente la
bondad de ajuste pueda llegar a ser satisfactoria, los autores proponen
utilizar componentes principales sobre los primeros $p$ términos
polinomiales $1/(1 + \tau)$, con el fin de seleccionar $k<p$ variables, a ser
incluidas en la ecuación @ecu2. Utilizar las componentes principales
proporcionará un menor error de ajuste en comparación con la ecuación @ecu1,
debido a su capacidad para captar variabilidad. Una descripción
detallada respecto al cálculo de las componentes principales en el
esquema polinomial es dada por Hunt y Terry \cite{HT}.

### Polinomios trigonométricos.

Las funciones trigonométricas pueden ser utilizadas para capturar de
forma satisfactoria las distintas configuraciones que pueden asumir las
curvas de rendimientos. En este caso, el modelo puede ser descrito como
$y(\tau) = \beta_{0} + \beta_{1}cos(\gamma_{1}\tau) + \beta_{2}sen(\gamma_{2}\tau)$; donde $\tau$ representa la duración o la
madurez del papel, en tanto que $\beta_{0}$, $\beta_{1}$, $\beta_{2}$, $\gamma_{1}$ y $\gamma_{2}$ son los parámetros
objeto de interés. Cualquier metodología de optimización no lineal puede
ser utilizada para estimar los parámetros del modelo (Nocedal y Wright
\cite{NW}). Aunque podría asumirse un parámetro de fase en el modelo, este
no es considerado por motivos de parsimonia.


### Metodología Diebold-Li

#### Modelos de factores de la curva de rendimientos

Estos modelos son conocidos porque utilizan factores latentes asociados directamente a los componentes de la curva; generalmente el nivel, la pendiente y la curvatura de la misma.

El modelo más simple y conocido es el de Nelson y Siegel, pues genera aproximaciones muy acertadas en cuanto a la simulación de la curva, además de representar de manera adecuada sus características: como lo mencionan en (Christensen, Diebold, & Rudebusch, 2009) se pueden encontrar las diferentes formas de la curvatura, es más volátil la parte corta en comparación con el mediano y largo plazo y está acotado con límites superiores e inferiores para las tasas.

$$\displaystyle{s(m) = \beta_{0}+ \beta_{1}\frac{\left(1-e^\frac{-m}{\tau}\right)}{m/\tau} + \beta_{2} \left(\frac{\left(1-e^\frac{-m}{\tau}\right)}{m/\tau} -  e^\frac{-m}{\tau}\right)}$$

La ecuación anterior corresponde al modelo de Nelson – Siegel, donde $s(m)$ es el rendimiento de los títulos con vencimiento $m$. Este modelo propone interpretar los parámetros $\beta_0$ como el factor de largo plazo o nivel, $\beta_2-\beta_1$ como el factor de corto plazo o pendiente y $\beta_2$ como el factor de mediano plazo o curvatura. Siendo el anterior modelo la base para muchos desarrollos, se han encontrado diferentes estudios que buscan utilizar las características favorables de éste y un mejor ajuste a través de modificaciones, tal y como lo hacen en (Christensen et al., 2009) con el modelo sin arbitraje de Nelson y Siegel el cual es una generalización a través de cinco factores.

$$\displaystyle{s(m) = L_t + S_{t}^{1}(\frac{1-e^{-\lambda_1 m}}{\lambda_1 m})+S_{t}^{2}(\frac{1-e^{-\lambda_2 m}}{\lambda_2 m})+C_{t}^{1}(\frac{1-e^{-\lambda_1 m}}{\lambda_1 m}-e^{-\lambda_1 m})+C_{t}^{2}(\frac{1-e^{-\lambda_2 m}}{\lambda_2 m}-e^{-\lambda_2 m})}$$

Donde,

* $L_t$ corresponde al nivel.
* $S_{t}^{1}$ y $S_{t}^{2}$ corresponden a dos factores de pendiente.
* $C_{t}^{1}$ y  $C_{t2}^{2}$ hacen referencia a dos factores asociados a la curvatura.

Dentro de sus conclusiones, los autores resaltan de este modelo que además de ser libre de arbitraje, tiene un aumento en la cantidad de factores y un muy buen desempeño en el ajuste de la curva.

Dentro del pool de autores que se ha dedicado a encontrar las mejores aproximaciones para el modelado de la estructura a plazo se encuentre Diebold, quien en compañía de Li y Francis, en su artículo (Francis Diebold & Li, 2005) proponen un ajuste a través de un modelo con variaciones del modelo de Nelson y Siegel para modelar la curva, periodo a periodo, con factores. Básicamente realizan una reinterpretación de la curva de Nelson y Siegel con un modelo dinámico en el tiempo que utiliza tres factores (nivel, pendiente y curvatura) teniendo muy buenos resultados para plazos superiores a un año, y aunque no es libre de arbitraje, resaltan su buen desempeño por su sencillez y parsimonia.

$$\displaystyle{s_t(m) = \beta_{0t}+ \beta_{1t}\frac{\left(1-e^\frac{-m}{\tau_t}\right)}{m/\tau_t} + \beta_{2t} \left(\frac{\left(1-e^\frac{-m}{\tau_t}\right)}{m/\tau_t} -  e^\frac{-m}{\tau_t}\right)}$$

Donde $\beta_{0t}$, $\beta_{1t}$ y $\beta_{2t}$ son interpretados como los factores latentes. De manera que $\beta_{0t}$, al igual que en el modelo de Nelson y Siegel corresponde al factor de largo plazo nivel, $\beta_{1t}$ es el factor negativo asociado a la pendiente de la curva o factor de corto plazo y $\beta_{2t}$ se asocia a la curvatura. Con respecto al modelo de Nelson y Siegel, la interpretación es más sencilla pues los factores tienen una interpretación directa.


## Metodologías no Paramétricas

La regresión no paramétrica se ha convertido en los últimos años en un
área de excesivo estudio, debido a sus ventajas relativas respecto a los
modelos de regresión basado en funciones. Entre las características más
importantes de estos modelos tenemos, la flexibilidad en los supuestos y
el ajuste dirigido específicamente a través de los datos.


Dentro de un marco estadístico supondremos que tenemos un conjunto
de n observaciones $(x_{i}, y_{i})$, $i= 1, 2,., n$, independientes, donde se intenta
establecer las relaciones existentes entre una respuesta y un conjunto de
variables explicativas de forma semejante a los modelos de regresión
clásica.


El modelo que relaciona este conjunto de variables es dado por:

$$\displaystyle{y_{i} = m(x_{i}) + \epsilon_{i}}$$


donde la función $m(.)$ no específica una relación paramétrica, sino
permitir que los datos determinen la relación funcional apropiada. Bajo
estas condiciones la idea es que la media m(.) sea suave, suavidad que
puede controlarse acotando la segunda derivada, $|m''(x)| \leq M$, para todo
$x$ y $M$ una constante.

### Regresión Kernel

El método más simple de suavizamiento es el suavizador Kernel. Un
punto x se fija en el soporte de la función $m(.)$ y una ventana de
suavizamiento es definida alrededor de x. Frecuentemente, la ventana de
suavizamiento es simplemente un intervalo de la forma $(x - h, x + h)$,
donde h es un parámetro conocido como bandwidth.

La estimación Kernel es un promedio ponderado de las observaciones
dentro de la ventana de suavizamiento,

(@ecu3)
$$
\hat{m}(x) = \frac{\sum_{i=1}^{n} K(\frac{x_{i}-x}{h}) y_{i}}{\sum_{i=1}^{n} K(\frac{x_{i}-x}{h})}
$$

donde $K(.)$ es la función Kernel de ponderación. La función Kernel es escogida
de tal forma que las observaciones más próximas a x reciben mayor peso. Una
función frecuentemente utilizada es la bicuadrática:


$$
K(x) = \left\{
\begin{array}{ll}
(1-x^2)^2 &  \text{ si }  -1 \leq x  \leq 1 \\
0   & \text{ si }  x \ge 1,  x<-1
\end{array}
\right.
$$

Sin embargo, otro tipo de funciones de peso son utilizadas, tal como la
gaussiana, $K(x) = (2 \sqrt{\pi})^{-1} e^{\frac{-x^2}{2}}$ y la familia beta simétrica $K(x) = \frac{(1-x^2)_{+}^{\gamma}}{Beta(0.5,\gamma+1)}, \text{ con } \gamma = 0,1,...$

Note que cuando escogemos $\gamma = 0, 1, 2$ y $3$ obtenemos las funciones Kernel uniforme (Box), de Epanechnikov, la bipeso y la tripeso, respectivamente.

El suavizador Kernel puede ser representado como,


(@ecu4)
$$
\hat{m}(x) = \sum_{i=1}^{n} l_{i}(x) y_{i} 
$$

donde,

$$\displaystyle{l_{i} = \frac{K(\frac{x_{i}-x}{h})}{\sum_{j=1}^{n} K(\frac{x_{j}-x}{h})} }$$


La estimación Kernel en la ecuación @ecu3 es llamada la estimación de Nadaray- Watson, en honor a sus creadores. Su simplicidad lo hace de fácil comprensión e implementación; no obstante, se sabe que los ajustes en los extremos son sesgados. Una referencia ideal para un desarrollo más completo sobre este tema puede encontrarse en Fan y Gijbels \cite{FG}.

### Polinomios locales


Conocida también como regresión local, la idea es aproximar la función suave m(.) por medio de un polinomio de bajo orden en una vecindad entorno de un punto x. Por ejemplo, una aproximación lineal local es $m(x_{i}) \approx a_{0} + a_{1}(x_{i}-x)$ para $x - h \leq x_{i} \leq x+h$. Una aproximación local cuadrática es:

$$\displaystyle{m(x_{i}) \approx  a_{0} + a_{1}(x_{i}-x) + \frac{a_{2}}{2}(x_{i}-x)^2}$$


La aproximación local puede ser ajustada a través de mínimos cuadrados ponderados localmente. Una función Kernel y un bandwidth son definidos como en la regresión Kernel. Los coeficientes $\hat{a}_{0}$ y $\hat{a}_{1}$, son escogidos de tal forma que se pueda minimizar la expresión,


(@ecu5)
$$
 \sum_{i=1}^{n} K(\frac{x_{i}-x}{h}) (y_{i}-a_{0}-a_{1}(x_{i}-x))^2 
$$

Reescribiendo la ecuación @ecu5 en términos matriciales obtenemos, $X^{T}W(\tilde{Y}-X \tilde{a})$. Donde X es la matriz diseño para cada regresión lineal, $\tilde{a}$ el vector de
parámetros, W la matriz diagonal de pesos $K(\frac{x_{i}-x}{h})$ y $\tilde{Y}$ el vector de observaciones de orden n.

El vector de parámetros estimado está dado por $ \hat{\tilde{a}} = (X^{T}WX)X^{T}W \tilde{Y} $
y en forma semejante con la ecuación @ecu4, tenemos que: $l(x)_{nx1} = e_{1}^{T} (X^{T}WX)X^{T} \tilde{Y}$ donde $e_{1}^{T}$
es un vector de ceros de tamaño n, exceptuando la primera entrada cuyo valor es 1.

Finalmente, la selección del h está basado en procedimientos de bondad de ajuste que permite obtener el mejor modelo. Entre los más utilizados sobresalen los métodos de validación cruzada generalizada y plug-in, los cuales son descritos detalladamente en Fan y Gijbels \cite{FG}.

### Splines suavizados

Las funciones polinomiales se caracterizan por tener todas las derivadas en cualquier punto de su soporte; no obstante, cuando ciertas funciones no poseen un alto grado de suavidad en determinados puntos, el ajuste debido a estas funciones de polinomios no siempre será satisfactoria en estos tramos.

Para sobrellevar esta desventaja, el ajuste de polinomios de bajo orden localmente, con discontinuidades en ciertos puntos (knots), resulta en el conocido método de splines.


### Splines de polinomios

Suponga que queremos aproximar la función $m(.)$ por una función spline. Frecuentemente, el spline cúbico es utilizado para esta aproximación, sin embargo, otro tipo de splines pueden ser definidos.


Siguiendo la notación de Fan y Gijbels \cite{FG}, sea $t_{1}, t_{2}, t_{3},...,t_{J}$ el conjunto de nodos o knots en orden creciente, tal que en cada intervalo  ($-\infty$, $t_{1}$], $[t_{1}, t_{2}],..., [t_{J-1}, t_{J}]$, [$t_{J}, \infty$), funciones cúbicas continuas diferenciables son ajustadas. En este caso el espacio parámetrico es (J+4)-dimensional.

Un conjunto de splines cúbicos son ampliamente utilizados en la obtención de la función de splines, estas son las bases de potencias. Las mismas se definen como sigue, $(x- t_{j})_{+}^{3}, j= 1,2,...,J,1,x,x^2,x^3$ donde $x_{+}$ es la parte positiva de x. Así por ejemplo, la función de suavizamiento puede ser expresada como,

(@ecu6)
$$
m(x) = \sum_{j=1}^{J+4} \theta_{j}B_{j}(x) 
$$

siendo $B_{j}(x), j = 1, 2,... , J+4$, la base polinomial descrita anteriormente.

Los regresores definidos de esta forma pueden ocasionar problemas de estimación (multicolinealidad), motivo por el cual, los $B_{j}(x)$ son
redefinidos como,


$$\displaystyle{ B_{j}(x) = \frac{x-x_{j}}{x_{j+k-1}-x_{j}} B_{j,k-1}(x) + \frac{x_{j+k}-x}{x_{j+k}-x_{j}} B_{j+1,k-1}(x) }$$
suponiendo que $B_{j,1}=1$ para $x_{j} \leq x \leq x_{j+1}$ y cero en caso contrario. El proceso de estimación de la ecuación @ecu6 es realizado a través de mínimos cuadrados penalizados.

Adicionalmente, una desventaja del método, es su sensibilidad al número y ubicación de los nodos, motivo por el cual han sido propuestos diferentes procedimientos para su selección.

### Splines suavizados

El proceso de suavizamiento a través de este método está basado en la minimización de la funcíon,

$$\displaystyle{ \sum_{i=1}^{n} (y_{i}-m(x_{i})^2 + \lambda \int m''(x)^2dx  }$$
donde $\lambda$ es una constante especificada de suavizamiento. El mecanismo de optimización intenta crear un balance entre el sesgo de estimación y la suavidad de la curva ajustada. El parámetro $\lambda$ puede asumirse variable (Abramovich y Steinberg \cite{AS}) y estimado a través de validación cruzada generalizada.

### Supersuavizador de Friedmann


Las metodologías usuales de suavizamiento asumen que el parámetro suavizador es constante, factor que sumado a la forma de la curva subyacente puede hacer que surjan problemas, tal como el aumento en la varianza de la componente del error y/o a variaciones incontrolables de la segunda derivada de la función subyacente sobre el conjunto predictor. El suavizador propuesto por Friedman \cite{F} intenta corregir estos problemas, asumiendo que el bandwidth es variable sobre el conjunto de predictores.


Formalmente, se puede estimar un bandwidth para cada $x$, al igual que el correspondiente valor óptimo de suavizamiento, minimizando la expresión $e^{2}(m,h) = E(Y - m(X|h(X)))^2$ con respecto a las funciones $m(x)$ y $h(x)$. La expresión anterior puede reescribirse como,

(@ecu7)
$$
e^{2}(m,h) = E(E(Y - m(X|h(X)))^2|X)
$$

de tal forma que podemos minimizar el error con respecto a $m$ y $h$ para cada valor de $x$.

Como en el caso del bandwidth constante, se comienza aplicando un suavizador lineal local muchas veces sobre diferentes valores discretos de $h$, $0 < h < n$. Friedman \cite{F} propone utilizar tres conjuntos de valores, $h=0,05n$, $h=0,2n$ y $h=0,5n$, los cuales llama suavizadores ``tweeter", ``midrange" y ``woofer", respectivamente.

Para estimar la ecuación @ecu7 se utiliza el residual de la validación cruzada de la ecuación @ecu8 cuya descripción completa puede encontrarse en Friedman \cite{F},


(@ecu8)
$$
 r_{i}(h)= \frac{|y_{i}-m(x_{i}|h)|}{\left(1-\frac{1}{h}-\frac{(x_{i}-\bar{x}_{h})^2}{V_{h}}\right)}  
$$

siendo $\bar{x}_{h}$ y $V_{h}$, la media y varianza de los $x$, bajo un $h$ preeestablecido. Igualmente, Friedman \cite{F} aconseja suavizar $|r_{i}(h)|$ contra $x_{i}$, utilizando los $\hat{e}(m, h|x_{i})$, en procura de seleccionar la mejor amplitud de intervalo o bandwith, $\hat{e}(m, h_{vc}(x_{i})|x_{i})=min_{h} \hat{e}(m, h|x_{i})$, donde $h_{vc} (x_{i})$ es el mejor bandwidth bajo la validación cruzada respecto a cada $x_{i}$ , mientras que $h$ toma los valores de los suavizadores antes definidos.

De esta manera, el mejor valor suavizado dado $x_{i}$, siguiendo la notación de Friedman \cite{F}, $s^{*}(x_{i})$, estará asociado con el bandwidth: "tweeter", "midrange"  o "woofer" que minimice el error bajo la validación cruzada. Es posible a través de esta metodología obtener para cada vecindad en torno a $x_{i}$ diferentes bandwith y suavizados que proporcionan resultados óptimos, por tal razón, Friedman \cite{F} propone seleccionar la mejor amplitud de intervalo, suavizando los $h_{vc} (x_{i})$ contra $x_{i}$ utilizando el suavizador ``midrange", mientras que la curva estimada es obtenida interpolando entre los dos suavizadores con los bandwith estimados más parecidos.


Una suposición general establece que la curva subyacente que describe el comportamiento de los datos es suave, así que sería posible modificar el bandwidth en procura de un mayor suavizamiento, sacrificando exactitud numérica. Con este fin, Friedman \cite{F} propone un método de cálculo del bandwidth,

$$\displaystyle{h(x_{i}) = h_{vc}(x_{i}) + (h_{w} - h_{vc}(x_{i}))R_{i}^{10-\alpha}}$$

 con, 


$$\displaystyle{R_{i} = \left[\frac{\hat{e}(h_{vc}(x_{i})|x_{i})}{\hat{e}(h_{w}(x_{i})|x_{i})} \right] }$$


donde $0 < \alpha < 10$, $h_{w}$ es la amplitud calculada utilizando 
el suavizador "woofer" y $h_{vc}(.)$ la amplitud obtenida bajo la validación cruzada para cada observación. Sin importar el $\alpha$, cuando la contribución relativa de cada una de estas amplitudes no difiere significativamente, la amplitud de intervalo o bandwidth seleccionada es la determinada por el suavizador "woofer"; no obstante, si la eficiencia relativa está asociada con la amplitud bajo la validación cruzada, entonces, la ecuación en proporcionará esta amplitud. En otros casos, dependiendo del desempeño relativo y el parámetro $\alpha$ definido por el usuario, la ecuación proporcionará una amplitud, resultado de la combinación lineal entre el suavizador ``woofer" \hspace*{0.01 cm} y el obtenido bajo validación cruzada.

Una vez el bandwidth de suavización variable ha sido obtenido, los siguientes pasos son realizados sobre el conjunto de observaciones,


+ Suavice los datos con los bandwidth "tweeter", "midrange" y "woofer".
+ Suavice los residuales absolutos (12) obtenidos bajo cada bandwidth en el paso anterior, utilizando una amplitud de intervalo "midrange".
+ Seleccione el mejor bandwidth para cada observación, minimizando el error sobre la salida del paso (2).
+ Suavice los mejores bandwidth estimados en el paso (3) utilizando amplitud de intervalo "midrange".
+ Utilice los bandwidth suavizados para interpolar entre los valores suavizados obtenidos en el paso 1.



Las principales deficiencias atribuidas a esta técnica están asociadas con la pérdida de independencia entre los residuales $\epsilon_{i}$ relativo al orden de los predictores $x_{i}$, subestimando (sobre-estimando) cuando la correlación es positiva-alta (negativa-alta).

### Redes neuronales artificiales

Los recientes desarrollos investigativos han mostrado la capacidad de las redes neuronales para la detección de patrones, clasificación y predicción a través del aprendizaje por medio de la experiencia. Su importancia actual, sin lugar a dudas, es consecuencia del desarrollo computacional, punto de partida para su divulgación, desenvolvimiento teórico y práctico en diversos campos del conocimiento.

Una de las mayores áreas de aplicación de las redes neuronales es la predicción (Sharda \cite{SH}). Dentro de este contexto, las redes resultan ser una herramienta atractiva para los investigadores, comparada con las metodologías tradicionales basada en modelos de funciones.

Las redes neuronales artificiales intentan emular el comportamiento biológico del cerebro humano. Como sabemos, el cerebro humano es un conjunto complejo de interconexiones de elementos simples llamados nodos o neuronas. Cada nodo recibe una señal de entrada proveniente de otros nodos o a través de estímulos externos; localmente el nodo procesa la información recibida por medio de una función de transferencia o activación y produce una señal de salida transformada, que irá hacia otros nodos o como una respuesta, consecuencia de un estímulo. Aunque cada nodo individualmente no proporciona información realmente valiosa, en conjunto, pueden realizar un sorprendente número de tareas de forma eficiente. Esta característica hace de las redes neuronales un mecanismo poderoso computacionalmente para aprender a partir de ejemplos y después generalizar para casos nunca antes considerados.


Aunque diferentes arquitecturas de redes neuronales han sido propuestas (Haykin \cite{Ha}), la más utilizada es la red multilayer Perceptron (MLP). Una red MLP está compuesta de varias capas y nodos o neuronas. Los nodos de la primera capa son los encargados de recibir la información del exterior, mientras que la última capa es encargada de proporcionar la respuesta asociada a esta información. Entre estas dos capas puede haber innumerables capas y nodos. Adicionalmente, los nodos de capas adyacentes están completamente conectados.

Para un problema de pronóstico con redes neuronales, las entradas a la red son asociadas con variables independientes o explicativas. En este caso la relación funcional estimada establecida por la red neuronal será de la forma $y_{t}=f(x_{1}, x_{2},..., x_{p})$; donde $x_{1}, x_{2},..., x_{p}$ son p variables exógenas y y una variable endógena. En este sentido la red es equivalente a un modelo de regresión no lineal. Igualmente, en el contexto del pronóstico de series temporales, las entradas de la red son series rezagadas de la original y la salida representa su valor futuro. En este caso la red haría un mapeo de la siguiente forma $y_{t}= f (y{t-1}, y{t-2}, ..., y{t-p})$, donde $y_{t}$ es la observación en el tiempo t. Bajo estas características la red asemeja un modelo autorregresivo en el pronóstico de series temporales. Una discusión respecto a la relación existente entre las redes neuronales y la metodología de Box-Jenkins es dada por Suykens, Vandewalle y Moor \cite{SVM}.

Antes de que la red sea utilizada para realizar alguna tarea específica, debe ser entrenada. Básicamente, entrenar es el proceso de determinar los pesos (eje central de la red neuronal). El conocimiento aprendido por la red es almacenado en cada una de los arcos que representan las conexiones entre los nodos. Es a través de estas conexiones que las redes pueden realizar complejos mapeos no lineales desde los nodos de entrada hasta los nodos de salida. El entrenamiento de la MLP es supervisado, caso en el cual la respuesta deseada o valor objetivo para cada patrón de entrada o ejemplo está siempre disponible.

Los datos de entrenamiento son ingresados a la red en forma de vectores de variables o como patrones de entrada. Cada elemento en el vector de entrada es asociado con un nodo de la capa de entrada; de esta forma, el número de entradas a la red es igual a la dimensión del vector de entrada. Para el pronóstico de series temporales el número de variables de entrada es difícil de establecer, no obstante, una ventana de rezagos fija es constituida a lo largo de la serie. El total de datos disponible es usualmente dividido en un conjunto de entrenamiento y otro de prueba. El primero es utilizado para estimar los pesos de la red, mientras que el segundo es empleado para evaluar la capacidad de generalización de la red.


Para el proceso de entrenamiento, patrones de entrada son ingresados a la red. Los valores de activación de los nodos de entrada son multiplicados por su peso respectivo y acumulados en cada nodo sobre la primera capa. El total es evaluado en una función de activación y asumido como la salida del respectivo nodo. A esta salida algunos investigadores la identifican como la activación del nodo y es la entrada de otros nodos en capas siguientes de la red hasta que los valores de activación de la salida sean encontrados. El algoritmo de entrenamiento es utilizado para encontrar los pesos que minimicen una medida global de error tal como la suma de cuadrados del error (SSE).


En el pronóstico con series temporales, un patrón de entrenamiento consiste de un conjunto de valores fijos de variables en rezago de la serie. Suponga que tenemos N observaciones $y_{1}, y_{2}, ..., y_{N}$ para el proceso de entrenamiento y se requiere pronosticar un paso al frente, entonces con una red neuronal de n nodos de entrada, tenemos $N-n$ patrones de entrenamiento. El primer patrón de entrenamiento estará conformado por $y_{1}, y_{2}, ..., y_{n}$ como las entradas y $y_{n+1}$ como el valor objetivo. El segundo patrón de entrenamiento será $y_{2}$, $y_{3}$, ..., $y_{n+1}$ y el valor de salida deseado $y_{n+2}$. Finalmente, el último patrón de entrada será $y_{N-n}, y_{N-n+1}, ..., y_{N-1}$ y $y_{N}$ el valor objetivo. Frecuentemente, una función objetivo basada en la SSE es minimizada durante el proceso de entrenamiento,


$$\displaystyle{E = \frac{1}{2} \sum_{i=n+1}^{N}(y{i}-a{i})^2 }$$

donde $a_{i}$ es la salida actual de la red.

Una descripción más detallada sobre las diferentes arquitecturas de red existentes, el número óptimo de capas ocultas y neuronas, las funciones de activación más utilizadas, los algoritmos de entrenamiento, la normalización de los datos, como también de otros temas relacionados con sus ventajas y deficiencias (Haykin \cite{Ha}, Kaastra y Boyd \cite{KB}, Zhang, Patuwo y Hu \cite{ZPH}, Isasi y Galván \cite{IG}), entre otros.


### Metodología Splines Cúbicos de Suavizado

#### Teoría de interpolación

En el subcampo matemático del análisis numérico, se denomina interpolación a la obtención de nuevos puntos partiendo del conocimiento de un conjunto discreto de puntos. En ciertos casos el usuario conoce el valor de una función $f(x)$ en una serie de puntos $x_{1}, x_{2},..., x_{N}$, pero no se conoce una 
expresión analítica de $f(x)$ que permita calcular el valor de la función para un punto arbitrario. 

Un ejemplo claro son las mediciones de laboratorio, donde se mide cada minuto un valor, pero se requiere el valor en otro punto que no ha sido medido. Otro ejemplo son mediciones de temperatura en la superficie
de la Tierra, que se realizan en equipos o estaciones meteorológicas y se necesita calcular la temperatura en un punto cercano, pero distinto al punto de medida.

La idea de la interpolación es poder estimar $f(x)$ para un valor de x arbitrario, a partir de la construcción de una curva o superficie que une los puntos donde se han realizado las mediciones y cuyo valor si se conoce. Se asume que el punto arbitrario x se encuentra dentro de los límites de los puntos de medición, en caso contrario se llamaría extrapolación. Un proceso de interpolación se realiza en dos etapas:


+ Hacer un ajuste de los datos disponibles con una función interpolante.
+ Evaluar la función interpolante en el punto de interés x.


Este proceso en dos etapas no es necesariamente el más 
eficiente. La mayoría de algoritmos comienzan con un punto cercano $f(x_{i})$, y poco a poco van aplicando correcciones más pequenas a medida que la información de valores $f(x_{i})$ más distantes son incorporadas. El procedimiento toma aproximadamente $O(N^{2})$ operaciones. Si la función tiene un comportamiento suave, la última correción será la más pequeña y puede ser utilizada para estimar un límite a rango de error.

Dentro de las intepolaciones más usadas podemos destacar,


+ Interpolación lineal.
+ Interpolación de Hermite.
+ Interpolación polinómica.
+ Interpolación de Splines.


#### Interpolación lineal

La interpolación lineal es un procedimiento muy utilizado para estimar los valores que toma una función en un intervalo del cual conocemos sus valores en los extremos $(x_{1}, f(x_{1}))$ y $(x_{2},f(x_{2}))$. Para estimar este valor utilizamos la aproximación a la función $f(x)$ por medio de una recta $r(x)$ (de ahí el nombre de interpolación lineal, ya que también existe la interpolación cuadrática). 


La expresión de la interpolación lineal se obtiene del polinomio interpolador de Newton de grado uno. Aunque sólo existe un único polinomio que interpola una serie de puntos, existen diferentes formas de calcularlo. Este método es útil para situaciones que requieran un número bajo de puntos para interpolar, ya que a medida que crece el número de puntos, también lo hace el grado del polinomio.

Existen ciertas ventajas en el uso de este polinomio respecto al polinomio interpolador de Lagrange. Por ejemplo, si fuese necesario añadir algún nuevo punto o nodo a la función, tan sólo habría que calcular este último punto, dada la relación de recurrencia existente y demostrada anteriormente. El primer paso para hallar la fórmula de la interpolación es definir la pendiente de orden $n$ de manera recursiva, así,


+ $f_{0}(x_{i})$ es el término i-ésimo de la secuencia. 
+ $\displaystyle{f_{1}(x_{0},x_{1}) = \frac{f_{0}(x_{1})-f_{0}(x_{0})}{x_{1}-x_{0}}}$
  \item $\displaystyle{f_{2}(x_{0},x_{1},x_{2}) = \frac{f_{1}(x_{1},x_{2})-f_{1}(x_{0},x_{1})}{x_{2}-x_{0}}}$

En general,

$$\displaystyle{f_{i}(x_{0},x_{1},...,x_{i-1},x_{i}) = \frac{f_{i-1}(x_{1},...,x_{i-1},x_{i})-f_{i-1}(x_{0},x_{1},...,x_{i-1} )}{x_{i}-x_{0}}}$$

donde  $\displaystyle{x_{i}-x_{j}}$ representa la distancia entre dos elementos.

Puede apreciarse cómo en la definición general se usa la pendiente del paso anterior, $f_{i-1}(x_{1},...,x_{i-1},x_{i})$, a la cual se le resta la pendiente previa de mismo orden, es decir, el subíndice de los términos se decrementa en 1, como si se desplazara, para obtener $f_{i-1}(x_{0},x_{1},...,x_{i-1})$.

Nótese también que aunque el término inicial siempre es $x_{0}$ este puede ser en realidad cualquier otro, por ejemplo, se puede definir $f_{1}(x_{i-1},x_{i})$ de manera análoga al caso mostrado arriba. Una vez conocemos la pendiente, ya es posible definir el polinomio de grado $n$ de manera también recursiva, así,


+ $p_{0}(x) = f_{0} (x_{0}) =x_{0}$.  Se define así ya que este valor es el único que se ajusta a la secuencia original para el primer término. 
+ $\displaystyle{p_{1}(x) = p_{0}(x) +  f_{1} (x_{0},x_{1}) (x-x_{0})}$.
+ $\displaystyle{p_{2}(x) = p_{1}(x) +  f_{2} (x_{0},x_{1},x_{2}) (x-x_{0})(x-x_{1})}$.

En general,

$$\displaystyle{p_{i}(x) = p_{i-1}(x) +  f_{i} (x_{0},x_{1},...,x_{i-1},x_{i}) \prod_{j=0}^{i-1}(x-x_{j})}$$

#### Interpolación de Lagrange

Empezamos con un conjunto de $n+1$ puntos en el plano (que tengan diferentes coordenadas x), $(x_{0}, y_{0}), (x_{1}, y_{1}), (x_{2}, y_{2}),...,(x_{n}, y_{n})$. Así, queremos encontrar una función polinómica que pase por esos $n+1$ puntos y que tengan el menor grado posible. Un polinomio que pase por varios puntos determinados se llama un polinomio de interpolación.

Una posible solución viene dada por el  polinomio de interpolación de Lagrange. Lagrange publicó su fórmula en 1795 pero ya había sido publicada en 1779 por Waring y redescubierta por Euler en 1783.

Dado un conjunto de $k + 1$ puntos $(x_{0}.x_{1}),...,(x_{k}.x_{k+1})$, donde todos los $x_{j}$ se asumen distintos, el polinomio interpolador en la forma de Lagrange es la combinación lineal,

$$\displaystyle{L(x) = \sum_{j=0}^{k} y_{j} l_{j}(x)}$$
donde $ l_{j}(x)$ son las bases polinómicas de Lagrange,


$$\displaystyle{l_{j}(x) = \prod_{i=0,i \neq j}^{k} \frac{x-x_{i}}{x_{j}-x_{i}}  }$$

La función que estamos buscando es una función polinómica $L(x)$ de grado $k$ con el problema de interpolación puede tener tan solo una solución, pues la diferencia entre dos tales soluciones, sería otro polinomio de grado $k$ a lo sumo, con $k+1$ ceros. Por lo tanto, $L(x)$ es el único polinomio interpolador.


Si se aumenta el número de puntos a interpolar (o nodos) con la intención de mejorar la aproximación a una función, también lo hace el grado del polinomio interpolador así obtenido, por norma general. De este modo, aumenta la dificultad en el cálculo, haciéndolo poco operativo manualmente a partir del grado 4, dado que no existen métodos directos de resolución de ecuaciones de grado 4, salvo que se puedan tratar como ecuaciones bicuadradas, situación extremadamente rara.

La tecnología actual permite manejar polinomios de grados superiores sin grandes problemas, a costa de un elevado consumo de tiempo de computación. Pero, a medida que crece el grado, mayores son las oscilaciones entre puntos consecutivos o nodos. Se podría decir que a partir del grado 6 las oscilaciones son tal que el método deja de ser válido, aunque no para todos los casos.

Sin embargo, pocos estudios requieren la interpolación de tan sólo 6 puntos. Se suelen contar por decenas e incluso centenas. En estos casos, el grado de este polimonio sería tan alto que resultaría inoperable. Por lo tanto, en estos casos, se recurre a otra técnica de interpolación, como por ejemplo a la Interpolación polinómica de Hermite o a los splines cúbicos. Otra gran desventaja, respecto a otros métodos de interpolación, es la necesidad de recalcular todo el polinomio si se varía el número de nodos.

Aunque el polinomio interpolador de Lagrange se emplea mayormente para interpolar funciones e implementar esto fácilmente en una computadora, también tiene otras aplicaciones en el campo del álgebra exacta, lo que ha hecho más célebre a este polinomio, por ejemplo en el campo de los proyectores ortogonales.

#### Interpolación de Hermite

El objetivo de esta interpolación Hermite es minimizar el error producido en la interpolación de Lagrange de la función $f(x)$ sobre el intervalo $[a, b]$ sin aumentar el grado del polinomio interpolador.


Dados un entero no negativo N y  N + 1 puntos $(x_{0},..., x_{N})$ de la recta distintos dos a dos y los valores $f^{(j)} (x_{i})$, donde $0<i<N$ y $0<j<k_{i-1}$ de una función f y de sus derivadas, encontrar un polinomio de grado $m = (k_{0} +k_{1} +...+k_{n-1},)$ tal que,

$$\displaystyle{P^{(j)} (x_{i}) = f^{(j)} (x_{i}), para \hspace{0.2cm} 0<i<N \hspace{0.2cm}y \hspace{0.2cm} 0<j<k_{i-1}}$$
El problema de interpolación de Hermite tiene solución única, que se llama polinomio interpolador de Hermite. En lugar de interpolar sobre un soporte de puntos (de Tchebycheff) donde en general se desconoce el valor de la función, de hace de otra manera, imponiendo unas condiciones al polinomio,

+ Igualar el valor de la función en en los puntos del soporte, $P (x_{i}) = f^{(j)}$.
+ Igualar el valor de algunas derivadas de la función también en los puntos del soporte, $P^{(j)} (x_{i}) = f^{(j)} (x_{i})$.


Por lo que podemos dejar el polinomio de Hermite de grado (n-1) expresado de la siguiente manera,

$$\displaystyle{L(x) = \sum_{j=0}^{k} y_{j} l_{j} (x) }$$
donde $l_{j}(x)$ son los polinomios de la base de Lagrange.

#### Interpolación Polinómica

Asumamos que se tiene una tabla con $n$ puntos, $(x_{1},y_{1})...,(x_{n},y_{n})$, donde los valores $x_{i}$, para $i=1,...,n$ están ordenados de forma creciente y todos ellos son distintos. Supongamos que dichos puntos se representan en un plano cartesiano y se quiere determinar una curva suave que interpole dichos valores. Así, se desea determinar una curva que esté definida para todos los $x$ y tome los valores correspondientes de $y$, esto es, que interpole todos los datos de la tabla. Cabe destacar que los puntos considerados se les conoce como nodos.


La primera idea natural es usar una función polinomial que represente esta curva, la cual se puede representar como sigue,


$$\displaystyle{P_{n} = \sum_{i=0}^{n-1} a_{i}x^{i}}$$
tal curva se le conoce como función polinomial interpoladora de grado n. Nótese que en cada nodo se satisface que $P_{n}(x_{k})=y_{k}$, donde $k=1,...,n$.


Así, se tiene que si la tabla es representada mediante una función subyacente $f(x)$ tal que $f(x_{k})=y_{k}$, para todo $k$, entonces esta función puede ser aproximada mediante $P_{n}$, en los puntos intermedios.



No sería erróneo pensar que a medida que los nodos se incrementan, la aproximación sería cada vez mejor, lamentablemente esto no siempre es cierto, debido a que en el caso de tener una data con mucho ruido la interpolación no tendría mucho sentido ya que la varianza de los valores interpolados sería muy grande. En este caso, los polinomios interpoladores resultantes serían una mala representación de la función subyacente.

Para evitar este fenómeno, puede ser de utilidad relajar la condición de que $f(x)$ debería ser una función que interpole todos los valores dados y en su lugar usar un trozo de un polinomio local de interpolación. La función mediante la cual se logra esta interpolación se le conoce como spline.

#### Splines

Una función spline $S(x)$ es una función que consta de trozos de polinomios unidos por ciertas condiciones de suavizado. Un ejemplo simple, es una función poligonal (spline de primer grado), la cual se forma por polinomios lineales unidos, los cuales se definen entre cada par de nodos. Entre los nodos $x_{j}$ y $x_{j+1}$ se define un spline de primer grado como sigue, 

$$\displaystyle{S(x) = a_{j}x + b_{j} = S_{j}(x)}$$

este spline es lineal. Usualmente $S(x)$ se define como $S_{1}(x)$ para $x<x_{1}$ y como $S_{n-1}(x)$ para $x>x_{n}$, donde $x_{1}$ y $x_{n}$ son nodos frontera (Ver Figura \@ref(fig:spline1)).

```{r , echo=FALSE, fig.align='center',fig.cap="Spline Lineal",label=spline1}
knitr::include_graphics("images/spline_lineal.png")
```

Un spline de segundo grado es una unión de polinomios cuadráticos tal que $S(x)$ y su derivada $S^{(1)}(x)$ son continuas (Ver Figura \@ref(fig:spline2)). Los polinomios $P(x)$ a través de los que construimos el Spline tienen grado 2. Esto quiere decir, que va a tener la forma $P(x) = ax^2 + bx + c$.

```{r , echo=FALSE, fig.align='center',fig.cap="Spline Cuadrático",label=spline2}
knitr::include_graphics("images/spline_cuadratico.png")
```


Como en la interpolación segmentaria lineal, vamos a tener $N-1$ ecuaciones (donde N son los puntos sobre los que se define la función). La interpolación cuadrática nos va a asegurar que la función que nosotros generemos a trozos con los distintos $P(x)$ va a ser continua, ya que para sacar las condiciones que ajusten el polinomio, vamos a determinar como condiciones,

+ Que las partes de la función a trozos P(x) pasen por ese punto. Es decir, que las dos Pn(x) que rodean al f(x) que queremos aproximar, sean igual a f(x) en cada uno de estos puntos.
+ Que la derivada en un punto siempre coincida para ambos "lados" de la función definida a trozos que pasa por tal punto común.
+ Esto sin embargo no es suficiente, y necesitamos una condición más, la cual se obtiene a partir de una condición de borde.

Por su parte un spline cúbico, se representa mediante la unión de polinomios cúbicos con primera y segunda derivada continuas (Ver Figura \@ref(fig:spline3)). Este spline debido a su flexibilidad es el más usado en las aplicaciones.

```{r , echo=FALSE, fig.align='center',fig.cap="Spline Cúbico",label=spline3}
knitr::include_graphics("images/spline_cubico.png")
```

Formalmente un spline cúbico con nodos $x_{1},...x_{n}$ se define a partir de un conjunto de polinomios de la forma,\\


$$\displaystyle{S_{j}(x) = a_{j} + b_{j}x +c_{j}x^2 +d_{j}x^3}$$
con $x_{j}<x<x_{j+1}$, sujeto a las siguientes condiciones,


$$
\begin{array}{cc}
 \displaystyle{a_{j-1} + b_{j-1}x_{j} +c_{j-1}x_{j}^2 +d_{j-1}x_{j}^3 = a_{j} + b_{j}x_{j} +c_{j}x_{j}^2 +d_{j}x_{j}^3} \\
\displaystyle{ b_{j-1} +2c_{j-1}x_{j} +3d_{j-1}x_{j}^2 = b_{j} +2c_{j}x_{j} +3d_{j}x_{j}^2} \\
\displaystyle{ 2c_{j-1} +6d_{j-1}x_{j} = 2c_{j} +6d_{j}x_{j}}\\
\displaystyle{ c_{0} = d_{0} = c_{n} =d_{n}}
\end{array}
$$

Así para n nodos, existen $4(n-1)$ variables y $4(n-1)-2$ restricciones. Las mismas se deben a la necesidad de que el spline cúbico sea igual en los valores dados en cada nodo. Las primeras tres restricciones aseguran que la función resultante en su primera y segunda derivada sean continuas en los nodos. La restricción final significa que el spline cúbico es lineal en el punto inicial y final de la muestra. Sin embargo, es importante resaltar que el spline cúbico tiene tercera derivada discontinua en los nodos.

Debido a que hacen falta dos restricciones de borde, estas se deben añadir. Así  $S^{(2)}(x_{1}) = S^{(2)}(x_{n}) = 0$ son las restricciones faltantes, estan hacen referencia a que el spline sea un spline cúbico natural. Como se mencionó al inicio si se considera una interpolación polinomial global de un conjunto de datos con mucho ruido pueden surgir aproximaciones no deseables e inestables. En constrate, un spline cúbico de interpolación encaja perfectamente con la suavidad de la función subyacente.

```{r , echo=FALSE, fig.align='center',fig.cap="Comparativo Splines",label=spline4}
knitr::include_graphics("images/Comparativo_splines.png")
```


Otra característica de los splines es que con la adición de un parámetro sólo se aumenta la dimensionalidad del espacio de parámetros en una unidad, ya que tres de los cuatro parámetros están restringidos. De igual forma, al incrementar el número de nodos los splines toman formas funcionales más flexibles, lo cual muestra la relación entre el grado aproximación que se logra con el spline y el número de nodos que lo definen. Un comparativo para la interpolación lineal usando splines de primer, segundo y tercer grado, se muestra en la Figura \@ref(fig:spline4).


Mientras que las funciones spline son una herramienta interesante para interpolar funciones suaves, encontrarlas numéricamente no es tarea fácil. Una manera eficiente y muy estable para generar los splines necesarios para aproximar la función subyacente $f(x)$, es usando las bases de los B-splines cúbicos.

#### Bases de splines

En el subcampo matemático de análisis numérico, una B-spline o Basis spline (o traducido una línea polinómica suave básica), es una función spline que tiene el mínimo soporte con respecto a un determinado grado, suavidad y partición del dominio. Un teorema fundamental establece que cada función spline de un determinado grado, suavidad y partición del dominio, se puede representar como una combinación lineal de B-splines del mismo grado y suavidad, y sobre la misma partición. El término B-spline fue propuesto por Isaac Jacob Schoenberg y es la abreviatura de spline básica. Las B-splines pueden ser evaluadas de una manera numéricamente estable por el algoritmo de Boor. De un modo simplificado, se han creado variantes potencialmente más rápidas que el algoritmo de Boor, pero adolecen comparativamente de una menor estabilidad.


Una B-spline es simplemente una generalización de una curva de Bézier, que puede evitar el fenómeno Runge sin necesidad de aumentar el grado de la B-spline. Este fenómeno, se presenta al realizar interpolación lineal usando nodos equidistantes, básicamente es un problema que se presenta con el error de aproximaci\'n en los extremos del intervalo que se este considerando, así a medida que crece el número de nodos el error de aproximación se incrementa. 


Supongamos que tenemos un conjunto infinito de nodos $...<x_{-2}<x_{-1}<x_{0}<x_{1}<x_{2}<...$, entonces el j-ésimo B-spline de grado cero es igual a $B^{0}_{j}(x)=1$, si $x_{j} \leq x \leq x_{j+1}$ y $B^{0}_{j}(x)=0$ en otro caso. Con la función $B^{0}_{j}(x)$ como punto de partida se puede generar B-splines de grados mayores mediante la siguiente fórmula recursiva,\\


$$\displaystyle{B^{k}_{j}(x) = \frac{(x-x_{j})B^{k-1}_{j}(x)}{x_{j+k}-x_{j}} + \frac{(x_{j+k+1}-x)B^{k-1}_{j+1}(x)}{x_{j+k+1}-x_{j+1}}}$$
para $k\geq 1$. Así un B-spline de grado $k$ se define como,\\


$$\displaystyle{S^{k}(x) = \sum_{j=-\infty}^{\infty} \theta^{k}_{j} B^{k}_{j-k}(x)}$$
Los coeficientes $\theta^{k}_{j}$ se llaman puntos de control o puntos de Boor. Hay $m-(n+1)$ puntos de control que forman una envoltura convexa. Note que los B-splines de grado positivo no son ortogonales y por ende no poseen una expresión simple para sus coeficientes.

Sin embargo, los cálculos empleados para los B-splines interpoladores de grado cero y uno, son bastante sencillos,\\


$$\displaystyle{S^{0}(x) = \sum_{j=-\infty}^{\infty} y_{j} B^{0}_{j}(x),\hspace{0.4cm} S^{1}(x) = \sum_{j=-\infty}^{\infty} y_{j} B^{1}_{j-1}(x) }$$
Cuando los nodos son equidistantes, la B-spline se dice que es uniforme, de otro modo será no uniforme. Si dos nodos tj son idénticos, cualquiera de las posibles formas indeterminadas 0/0 se consideran 0.


#### B-spline uniforme

Cuando la B-spline es uniforme, las B-splines básicas para un determinado grado n son sólo copias cambiadas de una a otra. Una alternativa no recursiva de la definición de la B-splines $m-n+1$ básica es,


$$\displaystyle{B_{j}^{n}(t)= B_{n}(t-t_{j}), \hspace{0.2cm}para \hspace{0.2cm}j=0,...,m-n-1 }$$
con,

$$\displaystyle{B_{n}(t):= \frac{n+1}{n} \sum_{i=0}^{n+1}w_{i}^{n}(t - t_{i})_{+}^{n}   }$$
donde,

$$\displaystyle{w_{i}^{n} := \prod_{j=0, j\neq i}^{n+1} \frac{1}{t_{j}- t_{i}}}$$
nótese que $(t - t_{i})_{+}^{n}$ es la función potencia truncada definida como,


$$
(t - t_{i})_{+}^{n} :=  \left\{
\begin{array}{ll}
0 &  \text{ si }  t < t_{i} \\
(t - t_{i})^{n}   & \text{ si }  t \ge t_{i}
\end{array}
\right.
$$

#### B-spline cardinal

Si se define $B_{0}$ como la función característica de ${\displaystyle [-{\tfrac {1}{2}},{\tfrac {1}{2}}]}$, y $B_{k}$ recursivamente como el producto convolución,


$$\displaystyle{B_{k} := B_{k-1}*B_{0}, \hspace{0.2cm} k=1,2,... }$$

entonces $B_{k}$ se llaman B-splines cardinales (centradas). Esta definición se remonta a Schoenberg. $B_{k}$ tiene soporte compacto ${\displaystyle [-{\tfrac {k+1}{2}},{\tfrac {k+1}{2}}]}$ y es una función impar. Como ${\displaystyle k\rightarrow \infty }$ las B-splines cardinales normalizadas tienden a la función de Gauss.

Cuando el número de puntos de control de Boor es el mismo que el grado, la B-Spline degenera en una curva de Bézier. La forma de las funciones base es determinada por la posición de los nodos. Escalar o trasladar el vector de nodo no altera las funciones de base.

El spline está contenido en el casco convexo de sus puntos de control. Una B-spline básica de grado n $B_{i}^{n}(t)$ es distinta de cero sólo en el intervalo $[t_{i}, t_{i+n+1}]$ esto es,


$$
B_{i}^{n}(t)  = \left\{
\begin{array}{ll}
> 0 &  \text{ si }  t_{i}  \leq  t < t_{i+n+1} \\
0   & \text{ si }  resto
\end{array}
\right.
$$

En otras palabras si manipulamos un punto de control cambiamos sólo el comportamiento local de la curva y no el comportamiento global como con las curvas de Bézier. La función base se pueda obtener del polinomio de Bernstein. Algunos ejemplos de las bases B-splines se muestran a continuación,

#### B-spline constante

La B-spline constante es la spline más simple. Se define en un solo tramo de nodo y ni siquiera es continua en los nodos. Es sólo la función indicador de los diferentes tramos de nodo.

$$
B_{j}^{0}(t) = 1_{[t_{j},t_{j+1})}  = \left\{
\begin{array}{ll}
1 &  \text{ si }  t_{j}  \leq  t < t_{j+1}1 \\
0   & \text{ si }  resto
\end{array}
\right.
$$

#### B-spline lineal

La B-spline lineal se define en dos tramos de nodo consecutivos y es continua sobre los nodos, pero no diferenciable.

$$
B_{j}^{1}(t) = \left\{
\begin{array}{ll}
\frac{t-t_{j}}{t_{j+1}-t_{j}} &  \text{ si } t_{j}  \leq  t < t_{j+1} \\
\frac{t_{j+2}-t}{t_{j+2}-t_{j+1}}    & \text{ si }  t_{j+1}  \leq  t < t_{j+2} \\
0 &  \text{ si } resto
\end{array}
\right.
$$

#### B-spline cuadrática uniforme

Las B-splines cuadráticas con nodo-vector uniforme es una forma común de B-spline. La función base puede ser calculada fácilmente , y es igual para cada segmento, en este caso.

$$
B_{j}^{2}(t) = \left\{
\begin{array}{ll}
\frac{1}{2} (1-t)^2 \\
-t^2+t+\frac{1}{2} \\
\frac{1}{2} t^2
\end{array}
\right.
$$

Escrito en forma de matriz, esto es,


$$\displaystyle{S_{i}(t) = \begin{bmatrix} t^2 & t & 1  \end{bmatrix} \frac{1}{2} \begin{bmatrix} 1 & -2 & 1\\ -2 & 2 & 0  \\ 1 & 1 & 0 \end{bmatrix} \begin{bmatrix}  \theta_{i-1} \\ \theta_{i} \\ \theta_{i+1}  \end{bmatrix} , \hspace{0.2cm} para \hspace{0.2cm} t \in [0,1],\hspace{0.2cm}  i=1,2,...,m-1 }$$


#### B-spline cúbica

Una formulación B-spline para un solo segmento puede ser escrita como,

$$\displaystyle{S_{i}(t) = \sum_{k=0}^{3} \theta_{i-3+k} B_{i-3+k}^{3}(t) \hspace{0.2cm} con \hspace{0.2cm} t \in [0,1]  }$$

donde $S_{i}$ es el i-ésimo segmento B-spline, $\theta$ es el conjunto de puntos de control, el segmento i y k es el índice del punto de control local y $ B_{i-3+k}^{3}(t)$ representa la base de B-spline de grado 3. Un conjunto de puntos de control $\theta$ sería  ${\displaystyle \theta_{i}^{w}=(w_{i}x_{i},w_{i}y_{i},w_{i}z_{i},w_{i})}$ donde el ${\displaystyle w_{i}}$ es el peso, tirando de la curva hacia el punto de control ${\displaystyle \theta_{i}}$ mientras que aumenta o se desplazan fuera de la curva, a la vez que disminuye.

Toda una serie de segmentos se definiría como,

$$\displaystyle{S(t) = \sum_{i=0}^{m-1} \theta_{i} B_{i}^{3}(t)  }$$

donde i es el número de puntos de control y t es un parámetro global dados los valores de los nodos. Esta formulación expresa una curva B-spline como una combinación lineal de funciones B-spline básicas, de ahí el nombre.

Hay dos tipos de B-spline - uniforme y no uniforme. Una B-spline no uniforme es una curva donde los intervalos entre los puntos sucesivos de control no son, o no necesariamente son, iguales (el vector de nodos de espacios de nodo interiores no son iguales). Una forma común es donde los intervalos se reducen sucesivamente a cero, interpolando los puntos de control.

#### B-spline cúbica uniforme

La B-spline cúbica con vector-nodo uniforme es la forma más usual de B-spline. La función base puede ser fácilmente calculada, y es igual para cada segmento, en este caso. Puesto en forma de matriz, esto es,

$$\displaystyle{S_{i}(t) = \begin{bmatrix} t^3  & t^2 & t & 1  \end{bmatrix} \frac{1}{6} \begin{bmatrix} -1 & 3 & -3 & 1 \\ 3 & -6 & 3 & 0 \\ -3 & 0 & 3 & 0 \\ 1 & 4 & 1 & 0 \end{bmatrix} \begin{bmatrix}  \theta_{i-1} \\ \theta_{i} \\ \theta_{i+1} \\ \theta_{i+2}  \end{bmatrix} , \hspace{0.2cm} para \hspace{0.2cm} t \in [0,1] }$$

Para splines de grados más elevados, algunas arbitrariedades surgen al momento de calcular los coeficientes $\theta_{i}^{k}$. Por lo tanto, debido a que en las aplicaciones estadisticas existe un mayor interés por encontrar una aproximación que una interpolación, la técnica de minimos cuadros puede ser empleada para calcular estos valores.

Ahora bien, supongamos que se tiene un conjunto de $m$ funciones diferenciables $f(x)$, con soporte en el intervalo $[a,b]$, las cuales satifacen las siguientes condiciones,

+ $f(x_{i})=y_{i}$, para $i=1...,n$.
+ La m-1 derivada $f^{(m-1)}(x)$, es continua en x.

El problema es encontrar entre todas esas funciones, una función tal que tenga la mínima integral del cuadro de su segunda derivada, esto es, una función que tenga el valor más pequeño de $\int_{a}^{b} (f^{(m)}(x))^2 dx$. Dicha función será la elección más óptima al momento de hallar un balance entre suavidad y ajuste de los datos.

Se puede desmostrar que la solución de este problema es única y la función en cuestión es un spline polinomial que cumple la condición i), y además satisface que,


+ f es un pilinomio de grado no mayor que $m-1$ cuando $x \in [a,x_{1}]$ y $x \in [x_{n},b]$ .
+ F es un polinomio de grado no mayor a $2m-1$ para puntos interiores, $x \in [x_{i},x_{i+1}]$ con i=1,...,n.
+ f(x) tiene $2m-2$ derivadas continuas en el eje real.

En resumen, la función $f$ mínima es un spline el cual está conformado por trozos de polinomios unidos en los nodos $x_{i}$, donde dicha función tiene $2m-2$ derivadas continuas. Nótese que en muchas aplicaciones $m=2$ es un valor muy utilizado y cuya solución viene dada mediante el spline cúbico natural.

### Regresión no paramétrica

La teoría clásica de la regresión se basa, en gran parte, en el supuesto que las observaciones son independientes y se encuentran idéntica y normalmente distribuidas. Si bien existen muchos fenómenos del mundo real que pueden modelarse de esta manera, para el tratamiento de ciertos problemas, la normalidad de los datos es insostenible. En el intento de eliminar esa restricción se diseñaron métodos que hacen un número mínimo de supuestos sobre los modelos que describen las observaciones.

La teoría de los métodos no paramétricos trata, esencialmente, el desarrollo de procedimientos de inferencia estadística, que no realizan una suposición explícita con respecto a la forma funcional de la 
distribución de probabilidad de las observaciones de la muestra. Si bien en la Estadística no paramétrica también aparecen modelos y parámetros, ellos están definidos de una manera más general que en su contrapartida paramétrica.

La regresión no paramétrica es una colección de técnicas para el ajuste de funciones de regresión cuando existe poco conocimiento a priori acerca de su forma. Proporciona funciones suavizadas de la relación y el procedimiento se denomina suavizado.

Los fundamentos de los métodos de suavizado son antiguos pero sólo lograron el estado actual de desarrollo gracias a los avances de la computación y los estudios por simulación han permitido evaluar sus comportamientos.

La técnica más simple de suavizado, los promedios móviles, fue la primera en usarse, sin embargo han surgido nuevas técnicas como la estimación mediante núcleos (``kernel") o la regresión local ponderada. Estos estimadores de regresión no paramétrica son herramientas poderosas para el análisis de datos, tanto como una técnica de estimación para resumir una relación compleja que no puede ser aprendida por un modelo paramétrico, como para suplementar (o complementar) un análisis de regresión paramétrico.

En los análisis paramétricos se comienza haciendo supuestos rígidos sobre la estructura básica de los datos, luego se estiman de la forma más eficiente posible los parámetros que definen la estructura y por último se comprueba si los supuestos iniciales se cumplen.

La regresión no paramétrica, en cambio, desarrolla un ``modelo libre" \hspace{0.02cm} para predecir la respuesta sobre el rango de valores de los datos. Básicamente está constituida por métodos que proporcionan una estimación suavizada de la relación para un conjunto de valores (de- nominado ventana) de la variable explicativa. Estos valores son ponderados de modo que, por ejemplo, los vecinos más cercanos tengan mayor peso que los más alejados dentro de una ventana de datos. Se pueden utilizar diversas funciones de ponderación, que son los pesos en que se basan los estimadores. La combinación de la función de ponderación y el ancho de la ventana inciden sobre la bondad de la estimación resultante.


La mayor parte de las publicaciones sobre regresión no paramétrica consideran el caso de un solo regresor a pesar de que, a simple vista no pareciera de gran utilidad, ya que las aplicaciones más interesantes involucran varias variables explicativas. Sin embargo, la regresión no paramétrica simple es importante por dos motivos:


+ En etapas preliminares del análisis de datos o en pruebas de diagnóstico se utilizan gráficos de dispersión en los cuales puede ser muy útil ajustar una "curva suavizada". Por ejemplo, para explorar la forma de la función respuesta, para confirmar una función respuesta en particular que haya sido ajustada a los datos, para obtener estimaciones de la respuesta media sin especificar la forma de la función respuesta, para estudiar el cumplimientos de supuestos, etc.
+ Forma la base a partir de la cual se extienden los conceptos para regresión no paramétrica múltiple.

### Regresión no paramétrica mediante splines de suavizado

Consideremos el siguiente modelo de regresión homocedástico,


$$\displaystyle{Y_{i}=f(X_{i})+\epsilon_{i}}, \hspace{0.3cm} para \hspace{0.2cm} i=1,...,n$$
donde los $\epsilon_{i}$ son errores de media cero independientes e idénticamente distribuidos.

Uno de los posibles métodos para emplear splines es aproximar la función de regresión subyacente mediante las bases de splines, por ejemplo, la base de los B-splines cúbicos. Así, se escoge una secuencia fija de nodos $-\infty<t_{1}<t_{2}<...<t_{J}<\infty$, los cuales pueden diferir de los predictores. Luego, se calculan los elementos de la base cúbica de spline correspondiente.

Es posible mostrar que sólo son necesarios $J+4$ elementos de esta base. Denotemos a estos elementos por $B_{j}(x)$, así el spline polinomial lo podemos expresar como sigue,


$$\displaystyle{S(x)=\sum_{j=1}^{J+4} \theta_{j}B_{j}(x)}$$
Entonces los coeficientes $\theta_{j}$ pueden ser calculados al ser considerados como los parámetros que se obtienen al minimizar la suma de los errores al cuadrado,


$$\displaystyle{\sum_{i=1}^{n} \left[ Y_{i} - \sum_{j=1}^{J+4} \theta_{j}B_{j}(X_{j})\right]^2}$$
Denotamos por $\hat{\theta_{j}}$ al estimador de mínimos cuadrados y definimos el estimador del spline polinomial como sigue,

$$\displaystyle{ \hat{f}_{n}(x) = \sum_{j=1}^{J+4} \hat{\theta_{j}}B_{j}(x)}$$
Otro enfoque, se basa en la idea de encontrar un curva suave que minimize la suma penalizada de errores al cuadrado, es decir, que minimize la siguiente expresión,

(@ecu9)
$$
  n^{-1}\sum_{j=1}^{n}(Y_{j}-f(X_{j}))^2+\mu \int_{a}^{b} [f^{(m)} (x)]^2 dx
$$

para algún $\mu > 0$. Así como el enfoque de interpolación anterior, la solución de este problema de minimización es un spline, el cual recibe el nombre de estimador de spline de suavizado.


En particular, para el caso $m=2$ el minimizador de la ecuación @ecu9, es un spline cúbico natural. Note que $\mu$ juega el papel de parámetro de suavizado, este término se puede interpretar como una penalización por rugosidad de la función. Curvas que cambian lenta o suavemente presentan un valor pequeño de la integral, por ejemplo, en una función lineal la integral toma el valor de cero.


De hecho, la primera suma en la ecuación @ecu9 penaliza la falta de fidelidad de la aproximación de la data mediante el spline. El segundo término es el responsable de la suavidad de la aproximación obtenida mediante el spline. Para ver esto considérese los casos extremos, es decir, cuando $\mu =0$ y $\mu=\infty$. El primer caso conduce a una interpolación, esto es $\hat{f}(X_{i})=Y_{i}$ para $i=1,...,n$. El otro caso, conduce a una regresión lineal pues $f^{(2)}(x)\equiv 0$.


Por lo tanto $\mu$ es el parámetro de suavizado que controla la medida del estimador del spline polinomial, el cual puede variar desde el modelo más complicado e inestable hasta el modelo más simple. En otras palabras, la ecuación @ecu9 representa un balance entre la fidelidad o ajuste de los datos, representado mediante la suma de los residuos al cuadrado y la suavidad de la curva resultante, la cual se representa por la integral del cuadrado de la m-émisa derivada.

### Proceso de Optimización de Nelson y Siegel y Svensson


Para aplicar este proceso es necesario tener una función objetivo, sobre la cual se realizará el proceso de optimización, ya sea para maximizar ó minimizar dicha función. Dependiendo de la forma de dicha función el proceso de optimización será lineal o no lineal. En nuestro caso particular se llevará a cabo un proceso de optimización no lineal donde se buscará minimizar la función objetivo.


En el cálculo de nuestra función objetivo inteviene el concepto de la duración de un bono ó título, la cuál es una medida del vencimiento medio ponderado de todos los flujos que paga un bono. La misma viene dada mediante la siguiente expresión, 



$$\displaystyle{Duracion = \frac{1+r}{r} - \frac{n(c-r)+(1+r)}{c(1+r)^{n}-(c-r)}}$$

donde

+ r es el rendimiento al vencimiento del bono durante el período considerado.
+ n es el número de períodos que restan hasta la fecha de vencimiento del bono.
+ c es el cupón del bono.

Así nuestra función objetivo viene dada mediante la siguiente expresión,


$$
  f(x) = \sum_{i=1}^{n} (w_{i}\epsilon(x)_{i} )^2
$$


donde $w_{i}$ representan las ponderaciones, y se calculan mediante la siguiente expresión,


$$\displaystyle{w_{i} = \frac{\frac{1}{D_{i}}}{\sum_{j=1}^{N}\frac{1}{D_{j}}}}$$

 por su parte, $\epsilon_{i}(x)= \hat{Pr}_{i}(x)-Pr_{i}$, donde $Pr_{i}$ representan los precios promedios de los títulos a considerar, de entrada este es un parámetro ó valor con el que se cuenta. Por otra parte $\hat{Pr}_{i}(x)$ representa los precios estimados donde $x$ es el parámetro que va a variar y es el valor que se quiere optimizar.


Mediante la función objetivo descrita anteriormente se busca minimizar la diferencia que existe entre los precios promedios y los precios estimados, calculando un valor óptimo del parámetro $x$ mediante el proceso de optimización no lineal.


El proceso de optimización se realizó mediante el software estadístico R, mediante el paquete "nloptr". En este paquete, se encuentra el comando "aulag" el cual minimiza un función objetivo y devuelve entre otros valores el parámetro más óptimo, que hace que la función sea mínima. Un ejemplo del uso de este comando se presenta acontinuación,


$$ala2=auglag(1.22, fn=mifuncion, hin=res)$$
donde el primer argumento debe ser el valor inicial del parámetro a optimizar, el segundo argumento "fn" se refiere a la función que se desea optimizar, finalmente en el tercer parámetro "hin" se indican las restricciones sobre el parámetro a optimizar, en este caso la restricción establecida es que el parámetro sea mayor a cero.


Recordemos que la tasa cero cupón que se obtiene mediante la metodología de Svensson está dada por la siguiente expresión,


$$\displaystyle{s(m) = \beta_{0}+ \beta_{1}\frac{\left(1-e^\frac{-m}{\tau_{1}}\right)}{m/\tau_{1}} + \beta_{2} \left(\frac{\left(1-e^\frac{-m}{\tau_{1}}\right)}{m/\tau_{1}} -  e^\frac{-m}{\tau_{1}}\right) + \beta_{3} \left(\frac{\left(1-e^\frac{-m}{\tau_{2}}\right)}{m/\tau_{2}} -  e^\frac{-m}{\tau_{2}}\right)}$$

esta expresión está sujeta a las siguientes restricciones,

+ $\beta_{0} > 0$
+ $\beta_{0}+\beta_{1} > 0$
+ $\tau_{1} > 0$
+ $\tau_{2} > 0$

cada parámetro controla una sección de la curva. La fórmula anterior es de suma importancia ya que ella interviene en el cálculo del precio teórico de cada instrumento. El proceso de optimización actúa directamente sobre esta fórmula, ya que el mismo se centra en variar los parámetros de tal manera que la función objetivo sea minimizada.

Como se observó en las secciones anteriores el parámetro de suavizamiento fué elegido mediante el método de ensayo y error el cual no es para nada óptimo pues a priori este método no nos garantiza que el valor seleccionado sea el mejor, ya que se contarían con una gran cantidad de posibles valores a seleccionar, con el fin  de encontrar dicho parámetro el procedimiento anteriormente explicado puede ser implementado. 

Sin embargo, al realizar este proceso, se obtienen curvas que no son para nada suaves y en ocasiones no poseen ningunas de las formas usuales de la curva de rendimientos. Esto es debido a que en este caso este proceso, varía el parámetro de suavizamiento de tal manera que la diferencia entre el precio promedio y el precio teórico sea lo mas pequeña posible y en este proceso no existe un parámetro que controle la forma de la curva obtenida. Por lo tanto, su aplicación presenta algunos inconvenientes.  


### Elaboracion Base de datos para la metodología Splines y Diebold-Li

La fuente principal de información para calcular la curva de rendimientos para los títulos de la deuda pública nacional es el Banco Central de Venezuela (BCV), el cual diariamente publica las operaciones realizadas con estos instrumentos y los publica en el documento "resumersec" (Ver Figura \@ref(fig:bcv). Es importante destacar, que en este documento se encuentran por día dos pestañas, la $``0-22"$ y la $``0-23"$, en la primera pestaña se encuentran las operaciones interbancarias, por su parte la segunda pestaña muestra información sobre las operaciones realizadas por entes privados, en este caso el precio pautado en la operación no está disponible, razón por la cual esta pestaña no se toma en consideración. La información disponible en la pestaña $``0-22"$ es la siguiente,

+ Código del instrumento: código único que se asocia a cada instrumento.
+ Fecha de vencimiento: fecha de maduración de cada instrumento.
+ Plazo: cantidad de días que faltan para que el instrumento venza.
+ Cantidad de Operaciones: número de operaciones realizadas con cada instrumento.
+ Monto en Bolívares: monto total involucrado en la operación.
+ Precio mínimo: precio más bajo pautado en la operación.
+ Precio máximo: precio más alto pautado en la operación.
+ Precio promedio: precio promedio pautado en la la operación. Cabe destacar que si existe una sóla operación, los precios mínimo, máximo y promedio serán iguales.
+ Cupón: tasa de interés pagadera por cada instrumento.

```{r , echo=FALSE, fig.align='center',fig.cap="Pestaña 0-22",label=bcv}
knitr::include_graphics("images/Imagen022.png")
```

Es importante recordar que dentro de los títulos de la Deuda Pública Nacional se encuentran los títulos de interés fijo (TIF) y los títulos de tasa variable (VEBONO), los primeros se caracterizan por poseer una tasa de cupón que no varia, por su parte los VEBONO poseen una tasa de interés variable.

Esta información también es suministrada por el BCV, en su documento de las "Características de la Deuda Pública Nacional" (Ver Figura \@ref(fig:carac) ), por lo cual el mismo se debe revisar con cierta frecuencia, con el fin de actualizar la tasa de cupón de los VEBONO. En este documento se muestra información que caracteriza a cada instrumento, el mismo posee varias pestañas, en este trabajo sólo se considerará la pestaña $``DPN"$ en donde se encuentra información sobre los instrumentos emitidos en moneda nacional. La información disponible en este documento se muestra a continuación,

+ Número-Emisión-Decreto: información sobre emisión de cada instrumento.
+ Código: código único que se asocia a cada instrumento.
+ Fecha de emisión: fecha cuando se emitió cada instrumento.
+ Fecha de vencimiento: fecha de maduración de cada instrumento.
+ Monto en Circulación: monto total de cada instrumento en circulación.
+ Porcentaje de referecia: indica si el instrumento es de tasa fija o tasa variable.
+ Fecha de inicio: indica cada cuanto tiempo el instrumento paga cupón.
+ Período vigente: indica el período (fecha inicio y fecha fin) cuando cada instrumento paga cupón.
+ Tasa: cupón asociado a cada instrumento.

```{r , echo=FALSE, fig.align='center',fig.cap="Características",label=carac}
knitr::include_graphics("images/Imagencarac.png")
```

A partir de la pestaña "0-22" y del documento de las características, se creó la base de datos con la cual se va a trabajar, la misma contiene no sólo la información suministrada por la pestaña "0-22", sino alguna información adicional tomada del documento de las características. En dicha base de datos se contará con la siguiente información,

+ Tipo Instrumento: Indica el tipo de instrumento.
+ Nombre: Proporciona el nombre corto del título, usualmente este nombre se conforma por el tipo de título más su mes y año de vencimiento, por ejemplo, el título TIF032028, representa al título TIF con vencimiento en marzo del 2028.
+ Fecha de operación: Indica la fecha en que se efectuó dicha operación.
+ Fuente: Indica la fuente de donde se tomó la información, esta se puede tomar de dos fuentes, la primera mediante la pestaña 0-22 (mercado secundario) y  la otra mediante el documento de las subastas (mercado primario, información suministrada por el BCV).
+ Sicet: Proporciona el código asociado a cada título.
+ Fecha de vencimiento: Indica la fecha de maduración (vencimiento) del instrumento.
+ Plazo: Indica la cantidad de días que falta para que el instrumento se venza.
+ Cantidad de operaciones: Proporciona la cantidad de operaciones efectuadas con un insrumento en específico.
+ Monto: Indica el monto en Bolívares, por el cual se efectuó la operación u operaciones.
+ Precio mínimo: Indica el precio mínimo, por el cual se transó la operación.
+ Precio máximo: Indica el precio máximo, por el cual se transó la operación.
+ Precio promedio: Indica el precio promedio, por el cual se transó la operación, cabe destacar que en dado caso de existir una sola operación el valor del precio mínimo, máximo y promedio van a coincidir.
+ Cupón: Proporciona la tasa de cupón asociado a cada instrumento.
+ Frecuencia: Indica con que frecuencia el instrumento paga cupón, para los TIF y VEBONO, esta es 4, pues los mismos pagan cu\'pon trimestralmente, así se obtiene este valor pues existen 4 trimestres en el año.

<!-- Una vez obtenida la base de datos esta según sea el caso puede ser depurada mediante ciertos criterios, el primero es que aquellas operaciones con un monto menor a los 10 milllones no se consideran. El segundo es considerar la operación mas reciente, es decir, si en la base de datos se tiene que para un mismo instrumento existen diferentes operaciones en diferentes días, sólo se considerará la operación más reciente. -->


<!-- Para efectuar la depuración, a la base de datos anterior se le añadirán dos columnas nuevas una que indica el rendimiento al vencimiento de cada instrumento y la otra que indica la decisión que se tomó en base a los criterios descritos anteriormente (Ver Figura \@ref(fig:base)). Esta última será una variable dicotómica, es decir solo con dos valores (0 ó 1), en donde "0" me indica que no selecciono el título y "1" me indica que si lo tomo en cuenta para el estudio a realizar. -->

<!-- ```{r , echo=FALSE, fig.align='center',fig.cap="Base de datos",label=base} -->
<!-- knitr::include_graphics("images/Imagenbase.png") -->
<!-- ``` -->

<!-- Una vez calculados los precios estimados asociados a cada instrumento, se procederá a comparar los mismo con aquellos obtenidos por una metodología distinta. La metodología con la cual se va a comparar es la de Svensson, la cual es una metodología paramétrica. -->


<!-- Los instrumentos a considerar serán aquellos pertencientes al portafolio de inversiones de una institución financiera, de tal manera que para un día especifico sea posible conocer cuanta es la ganancia o pérdida que generan estos instrumentos. y por ende saber si es viable la venta o compra de determinado instrumento. -->

<!-- A partir de la data obtenida (Ver Figura \@ref(fig:base)), se procederá a añadir unas columnas nuevas con el fin de clasificar las observaciones para los distintos instrumentos en diferentes períodos de vencimiento. Los períodos de vencimiento son, -->


<!-- + Corto plazo: se refiere al vencimiento más cercano, los instrumentos que se encuentran aquí son aquellos que poseen un vencimiento menor a un año. -->
<!-- + Mediano plazo: en esta clasificación se encuentran los instrumentos cuyo vencimiento este entre uno y diez años. -->
<!-- + Largo plazo: hace referencia a aquellos instrumentos que tengan un vencimiento mayor a diez años. -->


<!-- Luego de separar la data por tipo de instrumento, la nueva data con la que se trabajará es la siguiente, -->

<!-- ```{r , echo=FALSE, fig.align='center',fig.cap="Base de datos TIF",label=baset} -->
<!-- knitr::include_graphics("images/data_nueva.png") -->
<!-- ``` -->

<!-- Con el fin de contar con la data más reciente a partir de la fecha de valoración, se creó la función "extrae" la cual selecciona de la data de la Figura (\@ref(fig:baset)) una determinada cantidad de observaciones, la cual es especificada por el usuario, esta función cuenta con los siguientes argumentos, -->

<!-- + fv: indica la fecha de valoración para la cual se está realizando el estudio. -->
<!-- + dias: indica la cantidad de días que el usuario desea, a partir de este valor se va a obtener la data con la que se va a trabajar. -->
<!-- + data: hace referencia a la data completa para cada tipo de instrumento, a partir de la misma se procedera a extraer parte de ella a partir del número de dias seleccionado. -->


<!-- Luego de selecionar la data, la misma se procede a depurar, es decir, se van a eliminar las observaciones duplicadas considerando sólo aquellas que sean más recientes.  -->


<!-- Así a partir de esta data sólo se consideraran las columnas plazo y rendimiento con el fin de tener una nube de puntos a partir de la cual se haga el ajuste de la función spline, y así obtener la curva de rendimientos. -->


<!-- La data obtenida a partir de la depuración anterior es, -->

<!-- ```{r , echo=FALSE, fig.align='center',fig.cap="Data depurada TIF",label=datadep} -->
<!-- knitr::include_graphics("images/cand.png") -->
<!-- ``` -->


<!-- Una vez obtenida la data para los Tif y Vebono se utilizó la función "smooth.spline" del programa estadístico R, para ajustar un spline cúbico a la data ingresada. Los argumentos requeridos por esta función son los siguientes, -->

<!-- + X: representa el vector de la variable predictiva. -->
<!-- + Y: representa el vector de la variable repuesta. -->
<!-- + cv: (TRUE/FALSE) variable del tipo lógico que representa si se va a utilizar la validación cruzada generalizada al momento de calcular el parámetro de suavizamiento. -->
<!-- + Spar: representa el parámetro de suavizamiento, típicamente (aunque no necesariamente) ubicado entre 0 y 1. Es el coeficiente lambda que acompaña a la integral del cuadrado de la segunda derivada de la función f. -->

<!-- De esta manera el siguiente comando ajusta un spline cubico a la data ingresada, -->


<!-- spline1=smooth.spline(X=datT1\$Plazo,Y=datT1\$Rendimiento,cv=TRUE, spar=1.35) -->

<!-- y lo guarda en la variable ``spline1". -->

<!-- Es importante señalar lo crucial de la escogencia del parámetro "spar", pues de él depende que tan suave sea la curva, la Figura (\@ref(fig:compspar) muestra como varía la curva cuando se cambia  el valor del ``spar", para esta comparación se usó tres valores, el primero fue 0.51 con el cual se obtiene una curva con ciertos picos la cual no es suave en lo absoluto.  -->

<!-- Usando el valor de 0.71 se obtiene la curva roja la cual presenta una mayor suavidad. Mientras que usando el valor de 0.81 se obtiene un mejor resultado aunque similar al anterior. De esta manera, se puede observar la importancia de la elección correcta de este parámetro, mientras este valor se aproxime a 1 se obtendrá una curva con mayor suvidad. -->

<!-- ```{r , echo=FALSE, fig.align='center',fig.cap="Curva de rendimiento Vebono para diferentes valores de suavizado",label=compspar} -->
<!-- knitr::include_graphics("images/curvavbv2.jpg") -->
<!-- ``` -->

<!-- Cabe destacar que para cada versión el parámetro usado en la variable "spar" cambió. Esto debido a la diferente cantidad de puntos que tiene cada versión. Así el valor del parámetro "spar" para los TIF se ubicó en el siguiente intervalo [0.4,0.6], por su parte para los VEBONOS la elección de dicho parámetro esta en [0.3,0.5]. Los mismos se obtuvieron mediante ensayo y error. Para los valores ubicados dentro de los intervalos mencionados siempre se obtuvo una curva suave. -->

<!-- Una vez que se obtiene la curva estimada y es guardada en una variable (en este caso, la variable es spline1), se procede a aplicar el comando "predict", para estimar el rendimiento de algún plazo que se ingrese. -->

<!-- Así con el fin de calcular el precio estimado de cada título, se creó la función "precio" mediante R, para determinar de forma automática dichos valores. Los imputs de dicha función son los siguientes, -->


<!-- + Tit: representa el nombre de cada título, al cual se le quiere estimar su precio, el mismo debe ser un carácter, ej: TIF102017 ó VEBONO112017. -->
<!-- + Spline1: representa la variable donde se guardo la curva ajustada mediante el spline. -->
<!-- + Fv: indica la fecha de valoración, para la cual se desea conocer el precio estimado. -->


<!-- Una vez ingresado los imputs, la función internamente busca el nombre del título en el documento de las características más reciente, y extrae del mismo la fecha de pago del próximo cupón y su fecha de vencimiento, con el fin de crear un vector de flujos. -->

<!-- Por ejemplo, si se quiere conocer el precio estimado del título "TIF032022"  al "01/03/2018", la función busca su fecha de vencimiento $(03/03/2022)$ y la fecha de pago del próximo cupón la cual es en este caso $08/03/2018$. Luego con dichos valores calcula la Tabla \ref{tabla1}, que representa los cupones que le quedan por pagar al título, -->

<!-- \renewcommand{\tablename}{Tabla} -->
<!-- \begin{table}[H] -->
<!-- \centering -->
<!-- %\begin{center} -->
<!-- {\begin{tabular}[t]{|l |c |c |c |c |c |r|} -->
<!-- \hline -->
<!-- Fecha & Plazo título & Plazo años & Rend estimado & Exp & Cupón & Producto \\ -->
<!-- \hline -->
<!-- 08/03/2018 & 7  & 0,0191780 & 0,45\% & 0,9999131 & 4& 3,999652\\ -->
<!-- \hline -->
<!-- 07/06/2018 & 98 & 0,2684931 & 1,05\% & 0,9971804 & 4& 3,988721\\ -->
<!-- \hline -->
<!-- 06/09/2018 & 189 & 0,5178082 & 1,64\% & 0,9915025 & 4& 3,966010\\ -->
<!-- \hline -->
<!-- 06/12/2018 & 280 & 0,7671232 & 2,23\% & 0,9829646 & 4& 3,931859\\ -->
<!-- \hline -->
<!-- 07/03/2019 & 317 & 1,0164383 & 2,82\% & 0,9717013 & 4& 3,886805\\ -->
<!-- \hline -->
<!-- 06/06/2019 & 462 & 1,2657534 & 3,39\% & 0,9578928 & 4& 3,831571\\ -->
<!-- \hline -->
<!-- 05/09/2019 & 553 & 1,5150684 & 3,96\% & 0,9417596 & 4& 3,767038\\ -->
<!-- \hline -->
<!-- 05/12/2019 & 644 & 1,7643835 & 4,50\% & 0,9235567 & 4& 3,694227\\ -->
<!-- \hline -->
<!-- 05/03/2020 & 735 & 2,0136986 & 5,03\% & 0,9035668 & 4& 3,614267\\ -->
<!-- \hline -->
<!-- 04/06/2020 & 826 & 2,2630137 & 5,54\% & 0,8820934 & 4& 3,528373\\ -->
<!-- \hline -->
<!-- 03/09/2020 & 917 & 2,5123287 & 6,02\% & 0,8594532 & 4& 3,437813\\ -->
<!-- \hline -->
<!-- 03/12/2020 & 1008 & 2,7616438 & 6,48\% & 0,8359698 & 4& 3,343879\\ -->
<!-- \hline -->
<!-- 04/03/2021 & 1099 & 3,0109589 & 6,91\% & 0,8119665 & 4& 3,247866\\ -->
<!-- \hline -->
<!-- 03/06/2021 & 1190 & 3,2602739 & 7,31\% & 0,7877226 & 4& 3,150890\\ -->
<!-- \hline -->
<!-- 02/09/2021 & 1281 & 3,5095890 & 7,69\% & 0,7634473 & 4& 3,053789\\ -->
<!-- \hline -->
<!-- 02/12/2021 & 1372 & 3,7589041 & 8,03\% & 0,7393192 & 4& 2,957277\\ -->
<!-- \hline -->
<!-- 03/03/2022 & 1463 & 4,0082191 & 8,35\% & 0,7154912 & 104 & 74,411084\\ -->
<!-- \hline -->
<!-- % Precio &  &  &  & & & 112,688809\\ -->
<!-- \multicolumn{6}{|c|}{Precio} & 131,8111 \\ -->
<!-- \hline -->
<!-- \end{tabular} -->
<!-- } -->
<!-- %\caption{Tabla} -->
<!-- %\end{center} -->
<!-- \caption{Cálculos función precio.} -->
<!-- \label{tabla1} -->
<!-- \end{table} -->

<!-- Así la primera columna (Fecha) se obtiene de sumarle a la fecha de pago del próximo cupón ($08/03/2018$) 91 días, que representa el tiempo cada cuando el título paga cupón, esto se realiza  hasta llegar a la fecha de vencimiento. -->

<!-- Luego la columna "Plazo título", se obtiene realizando la diferencia entre la columna 1 y la fecha de valoración (01/03/2018). Luego la columna 3 se obtiene dividiendo el valor de la columna 2 entre 365, para pasar dicho valor a años. Después evalúo los valores de la columna 2 en el spline obtenido, para así obtener los rendimientos estimados (columna 4). Posteriormente en la columna 5 (EXP) calculo la exponencial del producto de menos uno con el plazo en años (columna 3) y con el rendimiento estimado (columna 4). -->


<!-- La columna 6 (Cupón) la calculo dividiendo el valor del cupón del título entre 4, ya que cada cupón se paga cada tres meses, a diferencia del último al cual se le debe sumar el valor de 100. Finalmente en la última columna (Producto) calculo el producto del valor de la columna EXP con la columna Cupón, para luego realizar la sumatoria de todas sus filas y así obtener el precio estimado (131,8111 en este caso). -->


<!-- El mismo procedimiento se repite para cada título ya sea Tif o Vebono. Es importante señalar que los títulos considerados fueron aquellos que pertenecían al portafolio de inversiones del banco en un tiempo determinado. -->


