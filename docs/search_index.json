[
["index.html", "Riesgo de Mercado Ciencia de los Datos Financieros Prefacio ¿Por qué leer este libro? Estructura del libro Información sobre los programas y convenciones Prácticas interactivas con R Agradecimientos", " Riesgo de Mercado Ciencia de los Datos Financieros Synergy Vision 2019-06-11 Prefacio La versión en línea de este libro se comparte bajo la licencia Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. ¿Por qué leer este libro? Este libro es el resultado de enfocarnos en proveer la mayor cantidad de material sobre Probabilidad y Estadística Matemática con un desarrollo teórico lo más explícito posible, con el valor agregado de incorporar ejemplos de las finanzas y la programación en R. Finalmente tenemos un libro interactivo que ofrece una experiencia de aprendizaje distinta e innovadora. El un mundo abierto, ya no es tanto el acceso a la información, sino el acceso al conocimiento. Este libro es la base teórica para nuestro Diplomado en Probabilidades y Estadística Matemática aplicado a las Finanzas. Aunque es un material de corte general, hay ejemplos específicos traido de las finanzas. En el Diplomado nos enfocamos en el participante, el propósito es que el instructor ocupa a lo sumo el 20% del tiempo y el resto del tiempo los participantes se dedican a practicar y resolver ejercicios, tanto teóricos como de programación y modelaje en R al nivel de un curso de Postgrado. Ésta es la base de un programa en Ciencia de los Datos Financieros. Es mucha la literatura, pero son pocas las opciones donde se pueda navegar el libro de forma amigable y además contar con ejemplos en R y ejercicios interactivos, además del contenido multimedia. Esperamos que ésta sea un contribución sobre nuevas prácticas para publicar el contenido y darle vida, crear una experiencia distinta, una experiencia interactiva y visual. El reto es darle vida al contenido asistidos con las herramientas de Internet. Finalmente este es un intento de ofrecer otra visión sobre la enseñanza y la generación de material más accesible. Estamos en un mundo multidisciplinado, es por ello que ahora hay que generar contenido que conjugue en un mismo lugar las matemáticas, estadística, finanzas y la computación. Lo dejamos público ya que las herramientas que usamos para ensamblarlo son abiertas y públicas. Estructura del libro TODO: Describir la estructura Información sobre los programas y convenciones Este libro es posible gracias a una gran cantidad de desarrolladores que contribuyen en la construcción de herramientas para generar documentos enriquecidos e interactivos. En particular al autor de los paquetes Yihui Xie xie2015. Prácticas interactivas con R Vamos a utilizar el paquete Datacamp Tutorial que utiliza la librería en JavaScript Datacamp Light para crear ejercicios y prácticas con R. De esta forma el libro es completamente interactivo y con prácticas incluidas. De esta forma estamos creando una experiencia única de aprendizaje en línea. eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6ImIgPC0gNSIsInNhbXBsZSI6IiMgQ3JlYSB1bmEgdmFyaWFibGUgYSwgaWd1YWwgYSA1XG5cblxuIyBNdWVzdHJhIGVsIHZhbG9yIGRlIGEiLCJzb2x1dGlvbiI6IiMgQ3JlYSB1bmEgdmFyaWFibGUgYSwgaWd1YWwgYSA1XG5hIDwtIDVcblxuIyBNdWVzdHJhIGVsIHZhbG9yIGRlIGFcbmEiLCJzY3QiOiJ0ZXN0X29iamVjdChcImFcIilcbnRlc3Rfb3V0cHV0X2NvbnRhaW5zKFwiYVwiLCBpbmNvcnJlY3RfbXNnID0gXCJBc2VnJnVhY3V0ZTtyYXRlIGRlIG1vc3RyYXIgZWwgdmFsb3IgZGUgYGFgLlwiKVxuc3VjY2Vzc19tc2coXCJFeGNlbGVudGUhXCIpIn0= Agradecimientos A todo el equipo de Synergy Vision que no deja de soñar. Hay que hacer lo que pocos hacen, insistir, insistir hasta alcanzar. Lo más importante es concretar las ideas. La idea es sólo el inicio y solo vale cuando se concreta. Synergy Vision, Caracas, Venezuela "],
["acerca-del-autor.html", "Acerca del Autor", " Acerca del Autor Este material es un esfuerzo de equipo en Synergy Vision, (http://synergy.vision/nosotros/). El propósito de este material es ofrecer una experiencia de aprendizaje distinta y enfocada en el estudiante. El propósito es que realmente aprenda y practique con mucha intensidad. La idea es cambiar el modelo de clases magistrales y ofrecer una experiencia más centrada en el estudiante y menos centrado en el profesor. Para los temas más técnicos y avanzados es necesario trabajar de la mano con el estudiante y asistirlo en el proceso de aprendizaje con prácticas guiadas, material en línea e interactivo, videos, evaluación contínua de brechas y entendimiento, entre otros, para procurar el dominio de la materia. Nuestro foco es la Ciencia de los Datos Financieros y para ello se desarrollará material sobre: Probabilidad y Estadística Matemática en R, Programación Científica en R, Mercados, Inversiones y Trading, Datos y Modelos Financieros en R, Renta Fija, Inmunización de Carteras de Renta Fija, Teoría de Riesgo en R, Finanzas Cuantitativas, Ingeniería Financiera, Procesos Estocásticos en R, Series de Tiempo en R, Ciencia de los Datos, Ciencia de los Datos Financieros, Simulación en R, Desarrollo de Aplicaciones Interactivas en R, Minería de Datos, Aprendizaje Estadístico, Estadística Multivariante, Riesgo de Crédito, Riesgo de Liquidez, Riesgo de Mercado, Riesgo Operacional, Riesgo de Cambio, Análisis Técnico, Inversión Visual, Finanzas, Finanzas Corporativas, Valoración, Teoría de Portafolio, entre otros. Nuestra cuenta de Twitter es (https://twitter.com/bysynergyvision) y nuestros repositorios están en GitHub (https://github.com/synergyvision). Somos Científicos de Datos Financieros "],
["introduccion.html", "Capítulo 1 Introducción 1.1 Motivation", " Capítulo 1 Introducción Conocer en qué se invierte es un problema fundamental en la gestión del capital propio o de terceros, es el problema primario en la gestión de inversiones. La gestión de inversiones es un campo con muchos avances que, hoy en día, demanda métodos basados en las ciencias. En particular las matemáticas, probabilidades y la computación. Un primer avance en la solución de este problema es ofrecido por Harry Markowitz en 1952 e inaugura una nueva era en la aplicación de los métodos matemáticos para obtener carteras de inversión con características especiales, para conocer la relación entre el riesgo y el rendimiento. Este avance es reconocido con el precio Nobel en Economía en 1990 e inaugura la era de las Finanzas Cuantitativas, las Finanzas Computaciones, las Matemáticas Financieras, la Ingenería Financiera, entre otras prácticas. Markowitz desarrolla un modelo que considera los retornos de cada instrumento como una variable aleatoria y combina estas variables aleatorias para obtener el riesgo y el rendimiento de dicha combinación. Esta combinación de intrumentos es lo que denominamos portafolios y el objeto de Markowitz es encontrar los portafolios óptimos. Los portafolios óptimos son combinaciones de activos financieros que generan el mayor rendimiento posible con el menor riesgo. La combinación de activos nos lleva a la idea de la diversificación y su efecto en la gestión del riesgo, ya que ahora también es importante la relación que existe entre los instrumentos como un factor determinante del riesgo de la cartera. Si los instrumentos tienen una correlación positiva, el riesgo no disminuye, sin embargo si la correlación es negativa es posible, mediante la diversificación, disminuir el riesgo de la cartera. En la práctica es muy complejo obtener instrumentos no correlacionados, ya que los mercados tienen a seguir ciclos alcistas o bajistas y los instrumentos tienen a moverse de forma acompasada. Es decir, que la diversificación no necesariamente logra el objetivo de disminuir el riesgo de la cartera. Con estos avances ahora el rendimiento no se discute de forma aislada, el rendimiento siempre viene acompañado del riesgo y ahora la selección del inversionista debe considerar ambas variables para tomar su decisión. Este terreno es un campo con muchos avances ya que se aplican diversos métodos de optimización, de simulación o de medición del riesgo para conseguir las carteras que cumplen con la mejor relación para el inversionista. Este trabajo pretende abordar el campo de la distribución del capital en activos financieros que cumplen con ciertas condiciones partiendo de los datos disponibles. En este trabajo partiremos de la historia de los instrumentos para obtener sus características y conformar portafolios óptimos de acuerdo a la metodología de Markowitz. Adicionalmente se desarrollarán herramientas para facilitar el procesamiento de los datos y obtener dichos portafolios. 1.1 Motivation La historia reciente de los mercados, se caracteriza por la presencia de varias burbujas o grandes choques en los mercados. En el mercado americano, iniciemos desde 1999-2000 con la burbuja de las empresas tecnológicas que surgen con el auge de Internet y tienen como objetivo el desarrollo de la nueva economía virtual, hoy día es evidente que Amazon y Google son las grandes empresas que capitalizaron gran parte del potencial, el gran impacto generado por la caida de las Torres Gemelas en Septiembre del 2001 y luego la gran crisis inmobiliaria durante el 2007-2008 de la cual se ha recuperado apenas en 2015-2016. Hoy de nuevo, en el año 2017, estamos en presencia de un mercado alcista que alcanza nuevo territorio, ha alcanzado nuevos altos históricos y es inevitable la pregunta: ¿Estamos en presencia de una nueva burbuja?. Esta pregunta no tiene el objetivo de predecir que el mercado va a caer en algún momento, de hecho lo normal es que lo haga, el mercado cumple ciclos contínuamente y no tiene nada de innovador señalar que va a caer cuando hace altos históricos. El mercado se mueve en ondas. Lo que nos interesa de esta dinámica es cómo construir carteras de inversión que puedan sobrevivir estos choques, estos mercados bajistas y al mismo tiempo aprovechar al máximo los mercados alcistas. Desde 1952 y más notablemente desde 1990 que recibe el premio Nobel de Economía, Harry Markowitz es señalado como el padre de la Teoría Moderna de Portafolio. Su objetivo es definir una metodología que partiendo de un grupo de activos, se generan las carteras eficientes. Mucho ha ocurrido desde que la Economía reconoce como un área de desarrollo la aplicación de los modelos Matemáticos para ofrecer luz sobre la dedicación óptima del capital. Este reconocimiento al aporte de la ciencia en la Economía genera dos grandes líneas de trabajo que intenta responder dos preguntas fundamentales: ¿Dónde coloco el capital y en qué cantidad? y ¿Cuál es el precio de un activo financiero?. Este trabajo elabora sobre la primera pregunta. La segunda pregunta tiene que ver con el trabajo de Black, Scholes y Merton sobre la Valoración de opciones y derivados financieros, que también reciben el premio Nobel de Economía en 1997, apenas hace 20 años. Esta línea de trabajo donde se conjugan varias disciplinas como las Finanzas, Matemáticas, Estadística y más recientemente la Computación, genera un nuevo sector de Analistas Financieros Multidisciplinados que denominaremos Científicos de Datos Financieros. La demanda principal a estos analistas consiste en partir de los datos y generar inferencias sobre los mismos aplicando la ciencia y la tecnología. Gran parte del trabajo de Markowitz ha sido la aplicación de métodos de optimización de portafolios con el uso de las computadoras y asistido por sistemas. Las grandes burbujas y choques al mercado generan una gran preocupación sobre la gestión apropiada del riesgo y al mismo tiempo un entendimiento de la relación entre el riesgo que se asume y los beneficios posibles. El rendimiento es el resultado de una inversión después de un período de tiempo específico, podemos ganar o perder cuando realizamos una inversión, entonces el rendimiento es la variación (porcentual) del valor de un activo financiero en un período de tiempo. La variación del valor en términos absolutos es la ganancia o pérdida en valor monetario. Por otra parte el riesgo, en este caso, se puede definir como el grado de variabilidad de dicho rendimiento, esta es la medida propuesta por Markowitz y más recientemente han surgido nuevas medidas como el Valor en Riesgo (VaR) o el VaR Condicional (CVaR). Usualmente para rendimientos iguales, en teoría, y de acuerdo a la teoría de decisiones racionales bajo incertidumbre, un inversionista debería preferir la inversión con menor riesgo y por otra parte para riesgos iguales, en general, deberíamos preferir la inversión de mayor rendimiento. En este trabajo vamos a concentrarnos en la primera pregunta y nos basaremos en las aspiraciones del inversionista para conocer cuales instrumentos pueden formar parte de la cartera y en qué cantidad son requeridos para alcanzar un objetivo de inversión que combina la medida de rentabilidad y la medida de riesgo. Los gerentes e inversionistas se enfrentan a situaciones donde deben escoger entre distintas alternativas de inversión basándose en estas dos variables principales, a saber, el rendimiento y el riesgo. Las preguntas son diversas y típicas como por ejemplo: El problema fundamental es encontrar el balance adecuado entre un grupo de activos para alcanzar el máximo rendimiento con el mínimo riesgo. Debido a los ciclos y choques que reciben los mercados, el mundo de hoy está retado a trabajar en un ambiente donde la incertidumbre sobre el desempeño de las inversiones es un hecho palpable y sin embargo desde el punto de vista de los inversionistas o accionistas es necesario ofrecer proyecciones de desempeño que si no se cumplen generan un cuestionamiento sobre el profesionalismo de los gerentes del portafolio o de productos. Esto nos lleva a la valoración del riesgo debido a la incertidumbre inherente a los instrumentos de inversión, inclusive a nivel regulatorio la evaluación del riesgo con métodos más rigurosos ya empieza a ser necesario. El trabajo de tesis consiste en la investigación documental del estado del arte en la conformación de portafolios de inversión y la aplicación de sus métodos, partiendo del trabajo de Harry Markowitz como base, con el objetivo de entender el desempeño de estas carteras en cada burbuja o choque que han recibido los mercado y además comparar algunas combinaciones de medidas de riesgo y rendimiento. Adicionalmente vamos a continuar la construcción de una aplicación Web interactiva para aplicar los métodos y valernos lo más posible de las diversas librerías construidas en R para este fin. "],
["curva-de-rendimientos.html", "Capítulo 2 Curva de Rendimientos 2.1 Metodologías Paramétricas. 2.2 Metodologías no Paramétricas", " Capítulo 2 Curva de Rendimientos La curva de rendimientos es una representación grafica que muestra la relación que existe entre los rendimientos de una clase particular de títulos valores y el tiempo que falta para su vencimiento, lo cual es conocido como la estructura temporal de la tasa de interés (ETTI) para instrumentos con riesgo similar pero con diferentes plazos de maduración. La ETTI es un indicador de la evolución futura de los tipos de interés y de inflación, además, la mayoría de los activos financieros se valoran mediante este indicador, por lo cual también se considera básico en el diseño de estrategias de gestión de riesgos y en la toma de decisiones de inversión y financiación. Existen cuatro formas que puede adoptar una curva de rendimientos: Curva ascendente: generalmente, la curva de rendimientos tiene esta forma, lo que indica que los inversionistas requieren mayores rendimientos para vencimientos de más largo plazo, es decir, que los rendimientos varían directamente con los plazos. Curva descendente: indica que los rendimientos disminuyen a medida que aumentan los plazos. Curva horizontal: indica que independientemente del plazo de vencimient o, los rendimientos son los mismos; para períodos muy largos, todas las curvas de rendimientos tienden a aplanarse. Curva horizontal: indica que independientemente del plazo de vencimiento, los rendimientos son los mismos; para períodos muy largos, todas las curvas de rendimientos tienden a aplanarse. Curva creciente y decreciente: es el reflejo de una situación en la que los rendimientos de corto y largo plazo son los mismos y los rendimientos de mediano plazo son los que varían. Figura 2.1: Tipos de Curvas Es de esperar que una pendiente negativa de la curva de rendimientos (Ver Figura 2.1 ) o curva invertida (tasas de largo plazo menores a las de corto plazo) indique expectativas de una recesión futura y, por lo tanto, menores tasas de interés futuras; esto se puede explicar ya que los rendimientos esperados contienen información sobre los planes de consumo de los agentes. Entre las teorías que explican la pendiente de la curva de rendimientos, se encuentran: La teoría de la preferencia por la liquidez: consiste en que los inversionistas prefieren manejar títulos a corto plazo, pues éstos tienen una sensibilidad menor a los cambios en las tasas de interés y ofrecen una mayor flexibilidad en las inversiones si se compara con los títulos de largo plazo. Además, los prestatarios prefieren deuda a largo plazo, pues la de corto plazo los expone al riesgo de hacer una refinanciación de la deuda en condiciones adversas. Ambas situaciones, generan entonces, tasas de corto plazo relativamente bajas. En su conjunto, estos dos grupos de preferencias implican que en condiciones normales existe una Prima de Riesgo por Vencimiento (PRV) que aumenta en función de los años de vencimiento, haciendo que la curva de rendimientos posea una pendiente ascendente . La teoría de la segmentación del mercado: considera el mercado de renta fija como una serie de distintos mercados, los inversionistas y los emisores están restringidos por el sector específico de maduración. De acuerdo con esta teoría, la curva de rendimientos refleja una serie de condiciones de oferta y demanda que crean una secuencia de precios de equilibrio de mercado (tasas de interés) de los fondos . La teoría del Hábitat Preferido: plantea que los inversionistas intentarán liquidar sus inversiones en el menor plazo posible mientras que los prestamistas querrán tomar un plazo más largo; por lo tanto, dado que no se encuentran oferta y demanda de fondos para un mismo plazo, algunos inversionistas o prestatarios se verán motivados a cambiar el plazo de la inversión o el financiamiento pero, para lograrlo, deben ser compensados con un premio por el riesgo cuyo tamano reflejará la extesión de la aversión al riesgo. La Hipótesis de las Expectativas (HE): plantea que las tasas de interés de largo plazo deben reflejar por completo la información revelada por las futuras tasas de interés de corto plazo esperadas , o sea que los tipos de largo plazo no son más que una suma ponderada de los tipos de corto plazo esperados . Así, se puede afirmar entonces que la HE es una teoría que plantea que las tasas de interés exclusivamente representan las tasas previstas en el futuro. ##Metodologías de estimación de la Curva de Rendimientos. La curva de rendimientos presenta empíricamente una serie de dificultades, debido a que se construye a través de una serie de precios (tasas) de instrumentos financieros discontinuos en el tiempo que, por lo general, están lejos de ser una curva suave. Para su estimación existen diversas metodologías, las paramétricas y las no paramétricas. Las metodologías paramétricas se basan en modelos asociados a una familia funcional que obedece al comportamiento de alguna distribución de probabilidad, sobre la cual suponemos que las características de la población de interés pueden ser descritas. Es así como, los modelos diseñados en este contexto, basados en regresión, buscan describir el comportamiento de una variable de interés con otras llamadas exógenas, a través de funciones de vínculo lineales o no lineales. 2.1 Metodologías Paramétricas. Estadísticamente, un modelo paramétrico es una familia funcional que obedece al comportamiento de alguna distribución de probabilidad, sobre la cual suponemos que las características de la población de interés pueden ser descritas. Es así como, los modelos diseñados en este contexto, basados en regresión, buscan describir el comportamiento de una variable de interés con otras llamadas exógenas, a través de funciones de vínculo lineales o no lineales. 2.1.1 Metodología Nelson y Siegel Nelson y Siegel introducen un modelo paramétrico para el ajuste de los rendimientos hasta la madurez de los bonos del tesoro de Estados Unidos que se caracteriza por ser parsimonioso y flexible en modelar cualquier forma típica asociada con las curvas de rendimientos. La estructura paramétrica asociada a este modelo permite analizar el comportamiento a corto y a largo plazo de los rendimientos y ajustar -sin esfuerzos adicionales-, curvas monótonas, unimodales o del tipo S. Una clase de funciones que genera fácilmente las formas usuales de las curvas de rendimientos es la asociada con la solución de ecuaciones en diferencia. La teoría de expectativas sobre la estructura de las tasas de interés promueve la investigación en este sentido, dado que si las tasas spot son producidas por medio de una ecuación diferencial, entonces las tasas forward -siendo pronósticos-, serán la solución de las ecuaciones diferenciales. La expresión paramétrica propuesta por Nelson y Siegel que describe las tasas forward es exhibida a continuación: \\[\\displaystyle{f(m) = \\beta_{0} + \\beta_{1} e^{\\frac{-m}{\\tau}} +\\beta_{2} \\left(\\frac{-m}{\\tau}\\right)e^{\\frac{-m}{\\tau}}}\\] donde \\(m\\) denota la madurez del activo y \\(\\beta_{0}\\), \\(\\beta_{1}\\), \\(\\beta_{2}\\) y \\(\\tau\\) los parámetros a ser estimados. Puesto que las tasas spot pueden ser obtenidas a través de tasas forward por medio de la expresión: \\[\\displaystyle{s(m) = \\int_{0}^{m}f(x)dx}\\] la ecuación que determina las tasas spot \\(s(m)\\) de activos con madurez m es dada por: \\[\\displaystyle{s(m) = \\beta_{0}+ \\beta_{1}\\frac{\\left(1-e^\\frac{-m}{\\tau}\\right)}{m/\\tau} + \\beta_{2} \\left(\\frac{\\left(1-e^\\frac{-m}{\\tau}\\right)}{m/\\tau} - e^\\frac{-m}{\\tau}\\right)}\\] cuya ecuación es lineal si conocemos \\(\\tau\\) . El valor límite del rendimiento es \\(\\beta_{0}\\) cuando el plazo al vencimiento \\(m\\) es grande, mientras que, cuando el plazo al vencimiento \\(m\\) es pequeño el rendimiento en el límite es \\(\\beta_{0}+\\beta_{1}\\). Igualmente, los coeficientes del modelo de tasas forward pueden ser interpretados como medidas de fortaleza al corto, mediano y largo plazo. La contribución al largo plazo es determinada por \\(\\beta_{0}\\), \\(\\beta_{1}\\) lo hace al corto plazo ponderado por la función monótona creciente (decreciente) \\(e^{\\frac{-m}{\\tau}}\\) cuando \\(\\beta_{1}\\) es negativo (positivo) y \\(\\beta_{2}\\) lo hace al mediano plazo ponderado por la función monótona creciente (decreciente) \\((\\frac{-m}{\\tau}) e^{\\frac{-m}{\\tau}}\\) cuando \\(\\beta_{2}\\) es negativo (positivo). Una de las principales utilidades de la curva ha sido para propósitos de control de la política monetaria. Consecuentemente, \\(s(m)\\) será la ecuación utilizada para captar la relación subyacente entre los rendimientos y los plazos al vencimiento o madurez, sin recurrir a modelos más complejos que involucren un mayor número de parámetros. Adicionalmente, dado que la curva de Nelson-Siegel proporciona tasas spot compuestas continuas, estas deben transformarse en cantidades discretas, a través de la función de descuento. \\[\\displaystyle{s_{d}(m) = e^{\\frac{s(m)}{100}} - 1}\\] 2.1.2 Metodología Svensson En la curva de Nelson-Siegel se destaca que cada coeficiente del modelo contribuye en el comportamiento de las tasas forward en el corto, mediano y largo plazo; no obstante, Svensson propone una nueva versión de la curva de Nelson-Siegel donde un cuarto término es incluido para producir un efecto adicional y semejante al proporcionado por: \\(\\beta_{3}(\\frac{m}{\\tau_{2}})e^{\\frac{-m}{\\tau_{2}}}\\). En este caso, la función para describir la dinámica de las tasas forward es, \\[\\displaystyle{f(m) = \\beta_{0} + \\beta_{1} e^{\\frac{-m}{\\tau_{1}}} +\\beta_{2} \\left(\\frac{-m}{\\tau_{1}}\\right)e^{\\frac{-m}{\\tau_{1}}} + \\beta_{3}\\left(\\frac{-m}{\\tau_{2}}\\right)e^{\\frac{-m}{\\tau_{2}}} }\\] La curva spot de Svensson puede ser derivada a partir de la curva forward en forma semejante a la descrita para el modelo de Nelson- Siegel, obteniendo la siguiente expresión: \\[\\displaystyle{s(m) = \\beta_{0}+ \\beta_{1}\\frac{\\left(1-e^\\frac{-m}{\\tau_{1}}\\right)}{m/\\tau_{1}} + \\beta_{2} \\left(\\frac{\\left(1-e^\\frac{-m}{\\tau_{1}}\\right)}{m/\\tau_{1}} - e^\\frac{-m}{\\tau_{1}}\\right) + \\beta_{3} \\left(\\frac{\\left(1-e^\\frac{-m}{\\tau_{2}}\\right)}{m/\\tau_{2}} - e^\\frac{-m}{\\tau_{2}}\\right)}\\] La función de descuento tiene que ser utilizada con el fin de obtener las tasas estimadas para cada día de negociación o trading. Svensson propone estimar los parámetros de la curva cero cupón (curva spot), minimizando una medida de ajuste tal como la suma de cuadrados del error sobre los precios spot; sin embargo, enfatiza en que los precios pueden llegar a ser mal ajustados para los activos de madurez corta. En lugar de llevar el análisis por este camino, propone estimar los rendimientos fundamentado, principalmente, en que las decisiones de la política económica se basan en el comportamiento de las tasas y que obteniendo las tasas a través de la curva, los precios pueden ser calculados una vez la función de descuento es evaluada. De esta manera, los parámetros son escogidos minimizando la suma de cuadrados de la diferencia entre los rendimientos observados y estimados por la curva. La estimación es realizada por medio de máxima verosimilitud, mínimos cuadrados no lineales o el método de momentos generalizados. En muchos casos, como afirma Svensson , el modelo de Nelson- Siegel proporciona ajustes satisfactorios, aunque en algunos casos cuando la estructura de las tasas de interés es más compleja, el ajuste del modelo de Nelson-Siegel es poco satisfactorio y el modelo de Svensson logra desempeñarse mejor. 2.1.3 Polinomios de componentes principales. Hunt y Terry propone un ajuste de la curva de rendimientos utilizando polinomios. Si frecuentemente la curva es especificada como, \\[ y(\\tau) = \\beta_{0} + \\beta_{1}\\tau +\\beta_{2}\\tau^2 +\\beta_{3}\\tau^3 \\] La cual puede captar todas la formas que puede asumir la curva, su principal problema recae en el ajuste para aquellas tasas con períodos de vencimiento bastante largos. Aunque los autores conocen sobre las propiedades de parsimonia y de ajuste asociados con la curva de Nelson- Siegel, critican los problemas que acarrea la estimación de sus parámetros, proponiendo el ajuste de la curva de polinomios, bajo algunas modificaciones. Una transformación sobre el término de plazos (\\(\\tau\\)) que remueve la inestabilidad asociada con las tasas a largo plazo del polinomio de la ecuación 1 es sugerida. El modelo recomendado, siguiendo la notación de Hunt y Terry (1998) es: \\[ y(\\tau) = \\beta_{0} + \\sum_{i=1}^{p} \\beta_{i} \\frac{1}{(1+\\tau)^i} \\] donde, \\[\\displaystyle{y(0) = \\sum_{i=0}^{p}\\beta_{i}, \\text{ con } y(\\infty) = \\beta_{0} }\\] Investigaciones relacionadas con curvas de rendimientos, han llegado a la conclusión que modelos con tres o cuatro parámetros son suficientes para obtener un buen ajuste de los datos (Hunt ). Por tal motivo, Hunt y Terry proponen restringir \\(p\\) a tres o cuatro. Aunque este número de parámetros no necesariamente determina si realmente la bondad de ajuste pueda llegar a ser satisfactoria, los autores proponen utilizar componentes principales sobre los primeros \\(p\\) términos polinomiales \\(1/(1 + \\tau)\\), con el fin de seleccionar \\(k&lt;p\\) variables, a ser incluidas en la ecuación 2. Utilizar las componentes principales proporcionará un menor error de ajuste en comparación con la ecuación 1, debido a su capacidad para captar variabilidad. Una descripción detallada respecto al cálculo de las componentes principales en el esquema polinomial es dada por Hunt y Terry . 2.1.4 Polinomios trigonométricos. Las funciones trigonométricas pueden ser utilizadas para capturar de forma satisfactoria las distintas configuraciones que pueden asumir las curvas de rendimientos. En este caso, el modelo puede ser descrito como \\(y(\\tau) = \\beta_{0} + \\beta_{1}cos(\\gamma_{1}\\tau) + \\beta_{2}sen(\\gamma_{2}\\tau)\\); donde \\(\\tau\\) representa la duración o la madurez del papel, en tanto que \\(\\beta_{0}\\), \\(\\beta_{1}\\), \\(\\beta_{2}\\), \\(\\gamma_{1}\\) y \\(\\gamma_{2}\\) son los parámetros objeto de interés. Cualquier metodología de optimización no lineal puede ser utilizada para estimar los parámetros del modelo (Nocedal y Wright ). Aunque podría asumirse un parámetro de fase en el modelo, este no es considerado por motivos de parsimonia. 2.1.5 Metodología Diebold-Li 2.1.5.1 Modelos de factores de la curva de rendimientos Estos modelos son conocidos porque utilizan factores latentes asociados directamente a los componentes de la curva; generalmente el nivel, la pendiente y la curvatura de la misma. El modelo más simple y conocido es el de Nelson y Siegel, pues genera aproximaciones muy acertadas en cuanto a la simulación de la curva, además de representar de manera adecuada sus características: como lo mencionan en (Christensen, Diebold, &amp; Rudebusch, 2009) se pueden encontrar las diferentes formas de la curvatura, es más volátil la parte corta en comparación con el mediano y largo plazo y está acotado con límites superiores e inferiores para las tasas. \\[\\displaystyle{s(m) = \\beta_{0}+ \\beta_{1}\\frac{\\left(1-e^\\frac{-m}{\\tau}\\right)}{m/\\tau} + \\beta_{2} \\left(\\frac{\\left(1-e^\\frac{-m}{\\tau}\\right)}{m/\\tau} - e^\\frac{-m}{\\tau}\\right)}\\] La ecuación anterior corresponde al modelo de Nelson – Siegel, donde \\(s(m)\\) es el rendimiento de los títulos con vencimiento \\(m\\). Este modelo propone interpretar los parámetros \\(\\beta_0\\) como el factor de largo plazo o nivel, \\(\\beta_2-\\beta_1\\) como el factor de corto plazo o pendiente y \\(\\beta_2\\) como el factor de mediano plazo o curvatura. Siendo el anterior modelo la base para muchos desarrollos, se han encontrado diferentes estudios que buscan utilizar las características favorables de éste y un mejor ajuste a través de modificaciones, tal y como lo hacen en (Christensen et al., 2009) con el modelo sin arbitraje de Nelson y Siegel el cual es una generalización a través de cinco factores. \\[\\displaystyle{s(m) = L_t + S_{t}^{1}(\\frac{1-e^{-\\lambda_1 m}}{\\lambda_1 m})+S_{t}^{2}(\\frac{1-e^{-\\lambda_2 m}}{\\lambda_2 m})+C_{t}^{1}(\\frac{1-e^{-\\lambda_1 m}}{\\lambda_1 m}-e^{-\\lambda_1 m})+C_{t}^{2}(\\frac{1-e^{-\\lambda_2 m}}{\\lambda_2 m}-e^{-\\lambda_2 m})}\\] Donde, \\(L_t\\) corresponde al nivel. \\(S_{t}^{1}\\) y \\(S_{t}^{2}\\) corresponden a dos factores de pendiente. \\(C_{t}^{1}\\) y \\(C_{t2}^{2}\\) hacen referencia a dos factores asociados a la curvatura. Dentro de sus conclusiones, los autores resaltan de este modelo que además de ser libre de arbitraje, tiene un aumento en la cantidad de factores y un muy buen desempeño en el ajuste de la curva. Dentro del pool de autores que se ha dedicado a encontrar las mejores aproximaciones para el modelado de la estructura a plazo se encuentre Diebold, quien en compañía de Li y Francis, en su artículo (Francis Diebold &amp; Li, 2005) proponen un ajuste a través de un modelo con variaciones del modelo de Nelson y Siegel para modelar la curva, periodo a periodo, con factores. Básicamente realizan una reinterpretación de la curva de Nelson y Siegel con un modelo dinámico en el tiempo que utiliza tres factores (nivel, pendiente y curvatura) teniendo muy buenos resultados para plazos superiores a un año, y aunque no es libre de arbitraje, resaltan su buen desempeño por su sencillez y parsimonia. \\[\\displaystyle{s_t(m) = \\beta_{0t}+ \\beta_{1t}\\frac{\\left(1-e^\\frac{-m}{\\tau_t}\\right)}{m/\\tau_t} + \\beta_{2t} \\left(\\frac{\\left(1-e^\\frac{-m}{\\tau_t}\\right)}{m/\\tau_t} - e^\\frac{-m}{\\tau_t}\\right)}\\] Donde \\(\\beta_{0t}\\), \\(\\beta_{1t}\\) y \\(\\beta_{2t}\\) son interpretados como los factores latentes. De manera que \\(\\beta_{0t}\\), al igual que en el modelo de Nelson y Siegel corresponde al factor de largo plazo nivel, \\(\\beta_{1t}\\) es el factor negativo asociado a la pendiente de la curva o factor de corto plazo y \\(\\beta_{2t}\\) se asocia a la curvatura. Con respecto al modelo de Nelson y Siegel, la interpretación es más sencilla pues los factores tienen una interpretación directa. 2.2 Metodologías no Paramétricas La regresión no paramétrica se ha convertido en los últimos años en un área de excesivo estudio, debido a sus ventajas relativas respecto a los modelos de regresión basado en funciones. Entre las características más importantes de estos modelos tenemos, la flexibilidad en los supuestos y el ajuste dirigido específicamente a través de los datos. Dentro de un marco estadístico supondremos que tenemos un conjunto de n observaciones \\((x_{i}, y_{i})\\), \\(i= 1, 2,., n\\), independientes, donde se intenta establecer las relaciones existentes entre una respuesta y un conjunto de variables explicativas de forma semejante a los modelos de regresión clásica. El modelo que relaciona este conjunto de variables es dado por: \\[\\displaystyle{y_{i} = m(x_{i}) + \\epsilon_{i}}\\] donde la función \\(m(.)\\) no específica una relación paramétrica, sino permitir que los datos determinen la relación funcional apropiada. Bajo estas condiciones la idea es que la media m(.) sea suave, suavidad que puede controlarse acotando la segunda derivada, \\(|m&#39;&#39;(x)| \\leq M\\), para todo \\(x\\) y \\(M\\) una constante. 2.2.1 Regresión Kernel El método más simple de suavizamiento es el suavizador Kernel. Un punto x se fija en el soporte de la función \\(m(.)\\) y una ventana de suavizamiento es definida alrededor de x. Frecuentemente, la ventana de suavizamiento es simplemente un intervalo de la forma \\((x - h, x + h)\\), donde h es un parámetro conocido como bandwidth. La estimación Kernel es un promedio ponderado de las observaciones dentro de la ventana de suavizamiento, \\[ \\hat{m}(x) = \\frac{\\sum_{i=1}^{n} K(\\frac{x_{i}-x}{h}) y_{i}}{\\sum_{i=1}^{n} K(\\frac{x_{i}-x}{h})} \\] donde \\(K(.)\\) es la función Kernel de ponderación. La función Kernel es escogida de tal forma que las observaciones más próximas a x reciben mayor peso. Una función frecuentemente utilizada es la bicuadrática: \\[ K(x) = \\left\\{ \\begin{array}{ll} (1-x^2)^2 &amp; \\text{ si } -1 \\leq x \\leq 1 \\\\ 0 &amp; \\text{ si } x \\ge 1, x&lt;-1 \\end{array} \\right. \\] Sin embargo, otro tipo de funciones de peso son utilizadas, tal como la gaussiana, \\(K(x) = (2 \\sqrt{\\pi})^{-1} e^{\\frac{-x^2}{2}}\\) y la familia beta simétrica \\(K(x) = \\frac{(1-x^2)_{+}^{\\gamma}}{Beta(0.5,\\gamma+1)}, \\text{ con } \\gamma = 0,1,...\\) Note que cuando escogemos \\(\\gamma = 0, 1, 2\\) y \\(3\\) obtenemos las funciones Kernel uniforme (Box), de Epanechnikov, la bipeso y la tripeso, respectivamente. El suavizador Kernel puede ser representado como, \\[ \\hat{m}(x) = \\sum_{i=1}^{n} l_{i}(x) y_{i} \\] donde, \\[\\displaystyle{l_{i} = \\frac{K(\\frac{x_{i}-x}{h})}{\\sum_{j=1}^{n} K(\\frac{x_{j}-x}{h})} }\\] La estimación Kernel en la ecuación 3 es llamada la estimación de Nadaray- Watson, en honor a sus creadores. Su simplicidad lo hace de fácil comprensión e implementación; no obstante, se sabe que los ajustes en los extremos son sesgados. Una referencia ideal para un desarrollo más completo sobre este tema puede encontrarse en Fan y Gijbels . 2.2.2 Polinomios locales Conocida también como regresión local, la idea es aproximar la función suave m(.) por medio de un polinomio de bajo orden en una vecindad entorno de un punto x. Por ejemplo, una aproximación lineal local es \\(m(x_{i}) \\approx a_{0} + a_{1}(x_{i}-x)\\) para \\(x - h \\leq x_{i} \\leq x+h\\). Una aproximación local cuadrática es: \\[\\displaystyle{m(x_{i}) \\approx a_{0} + a_{1}(x_{i}-x) + \\frac{a_{2}}{2}(x_{i}-x)^2}\\] La aproximación local puede ser ajustada a través de mínimos cuadrados ponderados localmente. Una función Kernel y un bandwidth son definidos como en la regresión Kernel. Los coeficientes \\(\\hat{a}_{0}\\) y \\(\\hat{a}_{1}\\), son escogidos de tal forma que se pueda minimizar la expresión, \\[ \\sum_{i=1}^{n} K(\\frac{x_{i}-x}{h}) (y_{i}-a_{0}-a_{1}(x_{i}-x))^2 \\] Reescribiendo la ecuación 5 en términos matriciales obtenemos, \\(X^{T}W(\\tilde{Y}-X \\tilde{a})\\). Donde X es la matriz diseño para cada regresión lineal, \\(\\tilde{a}\\) el vector de parámetros, W la matriz diagonal de pesos \\(K(\\frac{x_{i}-x}{h})\\) y \\(\\tilde{Y}\\) el vector de observaciones de orden n. El vector de parámetros estimado está dado por $ = (X{T}WX)X{T}W $ y en forma semejante con la ecuación 4, tenemos que: \\(l(x)_{nx1} = e_{1}^{T} (X^{T}WX)X^{T} \\tilde{Y}\\) donde \\(e_{1}^{T}\\) es un vector de ceros de tamaño n, exceptuando la primera entrada cuyo valor es 1. Finalmente, la selección del h está basado en procedimientos de bondad de ajuste que permite obtener el mejor modelo. Entre los más utilizados sobresalen los métodos de validación cruzada generalizada y plug-in, los cuales son descritos detalladamente en Fan y Gijbels . 2.2.3 Splines suavizados Las funciones polinomiales se caracterizan por tener todas las derivadas en cualquier punto de su soporte; no obstante, cuando ciertas funciones no poseen un alto grado de suavidad en determinados puntos, el ajuste debido a estas funciones de polinomios no siempre será satisfactoria en estos tramos. Para sobrellevar esta desventaja, el ajuste de polinomios de bajo orden localmente, con discontinuidades en ciertos puntos (knots), resulta en el conocido método de splines. 2.2.4 Splines de polinomios Suponga que queremos aproximar la función \\(m(.)\\) por una función spline. Frecuentemente, el spline cúbico es utilizado para esta aproximación, sin embargo, otro tipo de splines pueden ser definidos. Siguiendo la notación de Fan y Gijbels , sea \\(t_{1}, t_{2}, t_{3},...,t_{J}\\) el conjunto de nodos o knots en orden creciente, tal que en cada intervalo (\\(-\\infty\\), \\(t_{1}\\)], \\([t_{1}, t_{2}],..., [t_{J-1}, t_{J}]\\), [\\(t_{J}, \\infty\\)), funciones cúbicas continuas diferenciables son ajustadas. En este caso el espacio parámetrico es (J+4)-dimensional. Un conjunto de splines cúbicos son ampliamente utilizados en la obtención de la función de splines, estas son las bases de potencias. Las mismas se definen como sigue, \\((x- t_{j})_{+}^{3}, j= 1,2,...,J,1,x,x^2,x^3\\) donde \\(x_{+}\\) es la parte positiva de x. Así por ejemplo, la función de suavizamiento puede ser expresada como, \\[ m(x) = \\sum_{j=1}^{J+4} \\theta_{j}B_{j}(x) \\] siendo \\(B_{j}(x), j = 1, 2,... , J+4\\), la base polinomial descrita anteriormente. Los regresores definidos de esta forma pueden ocasionar problemas de estimación (multicolinealidad), motivo por el cual, los \\(B_{j}(x)\\) son redefinidos como, \\[\\displaystyle{ B_{j}(x) = \\frac{x-x_{j}}{x_{j+k-1}-x_{j}} B_{j,k-1}(x) + \\frac{x_{j+k}-x}{x_{j+k}-x_{j}} B_{j+1,k-1}(x) }\\] suponiendo que \\(B_{j,1}=1\\) para \\(x_{j} \\leq x \\leq x_{j+1}\\) y cero en caso contrario. El proceso de estimación de la ecuación 6 es realizado a través de mínimos cuadrados penalizados. Adicionalmente, una desventaja del método, es su sensibilidad al número y ubicación de los nodos, motivo por el cual han sido propuestos diferentes procedimientos para su selección. 2.2.5 Splines suavizados El proceso de suavizamiento a través de este método está basado en la minimización de la funcíon, \\[\\displaystyle{ \\sum_{i=1}^{n} (y_{i}-m(x_{i})^2 + \\lambda \\int m&#39;&#39;(x)^2dx }\\] donde \\(\\lambda\\) es una constante especificada de suavizamiento. El mecanismo de optimización intenta crear un balance entre el sesgo de estimación y la suavidad de la curva ajustada. El parámetro \\(\\lambda\\) puede asumirse variable (Abramovich y Steinberg ) y estimado a través de validación cruzada generalizada. 2.2.6 Supersuavizador de Friedmann Las metodologías usuales de suavizamiento asumen que el parámetro suavizador es constante, factor que sumado a la forma de la curva subyacente puede hacer que surjan problemas, tal como el aumento en la varianza de la componente del error y/o a variaciones incontrolables de la segunda derivada de la función subyacente sobre el conjunto predictor. El suavizador propuesto por Friedman intenta corregir estos problemas, asumiendo que el bandwidth es variable sobre el conjunto de predictores. Formalmente, se puede estimar un bandwidth para cada \\(x\\), al igual que el correspondiente valor óptimo de suavizamiento, minimizando la expresión \\(e^{2}(m,h) = E(Y - m(X|h(X)))^2\\) con respecto a las funciones \\(m(x)\\) y \\(h(x)\\). La expresión anterior puede reescribirse como, \\[ e^{2}(m,h) = E(E(Y - m(X|h(X)))^2|X) \\] de tal forma que podemos minimizar el error con respecto a \\(m\\) y \\(h\\) para cada valor de \\(x\\). Como en el caso del bandwidth constante, se comienza aplicando un suavizador lineal local muchas veces sobre diferentes valores discretos de \\(h\\), \\(0 &lt; h &lt; n\\). Friedman propone utilizar tres conjuntos de valores, \\(h=0,05n\\), \\(h=0,2n\\) y \\(h=0,5n\\), los cuales llama suavizadores tweeter&quot;,midrange&quot; y ``woofer&quot;, respectivamente. Para estimar la ecuación 7 se utiliza el residual de la validación cruzada de la ecuación 8 cuya descripción completa puede encontrarse en Friedman , \\[ r_{i}(h)= \\frac{|y_{i}-m(x_{i}|h)|}{\\left(1-\\frac{1}{h}-\\frac{(x_{i}-\\bar{x}_{h})^2}{V_{h}}\\right)} \\] siendo \\(\\bar{x}_{h}\\) y \\(V_{h}\\), la media y varianza de los \\(x\\), bajo un \\(h\\) preeestablecido. Igualmente, Friedman aconseja suavizar \\(|r_{i}(h)|\\) contra \\(x_{i}\\), utilizando los \\(\\hat{e}(m, h|x_{i})\\), en procura de seleccionar la mejor amplitud de intervalo o bandwith, \\(\\hat{e}(m, h_{vc}(x_{i})|x_{i})=min_{h} \\hat{e}(m, h|x_{i})\\), donde \\(h_{vc} (x_{i})\\) es el mejor bandwidth bajo la validación cruzada respecto a cada \\(x_{i}\\) , mientras que \\(h\\) toma los valores de los suavizadores antes definidos. De esta manera, el mejor valor suavizado dado \\(x_{i}\\), siguiendo la notación de Friedman , \\(s^{*}(x_{i})\\), estará asociado con el bandwidth: “tweeter”, “midrange” o “woofer” que minimice el error bajo la validación cruzada. Es posible a través de esta metodología obtener para cada vecindad en torno a \\(x_{i}\\) diferentes bandwith y suavizados que proporcionan resultados óptimos, por tal razón, Friedman propone seleccionar la mejor amplitud de intervalo, suavizando los \\(h_{vc} (x_{i})\\) contra \\(x_{i}\\) utilizando el suavizador ``midrange&quot;, mientras que la curva estimada es obtenida interpolando entre los dos suavizadores con los bandwith estimados más parecidos. Una suposición general establece que la curva subyacente que describe el comportamiento de los datos es suave, así que sería posible modificar el bandwidth en procura de un mayor suavizamiento, sacrificando exactitud numérica. Con este fin, Friedman propone un método de cálculo del bandwidth, \\[\\displaystyle{h(x_{i}) = h_{vc}(x_{i}) + (h_{w} - h_{vc}(x_{i}))R_{i}^{10-\\alpha}}\\] con, \\[\\displaystyle{R_{i} = \\left[\\frac{\\hat{e}(h_{vc}(x_{i})|x_{i})}{\\hat{e}(h_{w}(x_{i})|x_{i})} \\right] }\\] donde \\(0 &lt; \\alpha &lt; 10\\), \\(h_{w}\\) es la amplitud calculada utilizando el suavizador “woofer” y \\(h_{vc}(.)\\) la amplitud obtenida bajo la validación cruzada para cada observación. Sin importar el \\(\\alpha\\), cuando la contribución relativa de cada una de estas amplitudes no difiere significativamente, la amplitud de intervalo o bandwidth seleccionada es la determinada por el suavizador “woofer”; no obstante, si la eficiencia relativa está asociada con la amplitud bajo la validación cruzada, entonces, la ecuación en proporcionará esta amplitud. En otros casos, dependiendo del desempeño relativo y el parámetro \\(\\alpha\\) definido por el usuario, la ecuación proporcionará una amplitud, resultado de la combinación lineal entre el suavizador ``woofer&quot; y el obtenido bajo validación cruzada. Una vez el bandwidth de suavización variable ha sido obtenido, los siguientes pasos son realizados sobre el conjunto de observaciones, Suavice los datos con los bandwidth “tweeter”, “midrange” y “woofer”. Suavice los residuales absolutos (12) obtenidos bajo cada bandwidth en el paso anterior, utilizando una amplitud de intervalo “midrange”. Seleccione el mejor bandwidth para cada observación, minimizando el error sobre la salida del paso (2). Suavice los mejores bandwidth estimados en el paso (3) utilizando amplitud de intervalo “midrange”. Utilice los bandwidth suavizados para interpolar entre los valores suavizados obtenidos en el paso 1. Las principales deficiencias atribuidas a esta técnica están asociadas con la pérdida de independencia entre los residuales \\(\\epsilon_{i}\\) relativo al orden de los predictores \\(x_{i}\\), subestimando (sobre-estimando) cuando la correlación es positiva-alta (negativa-alta). 2.2.7 Redes neuronales artificiales Los recientes desarrollos investigativos han mostrado la capacidad de las redes neuronales para la detección de patrones, clasificación y predicción a través del aprendizaje por medio de la experiencia. Su importancia actual, sin lugar a dudas, es consecuencia del desarrollo computacional, punto de partida para su divulgación, desenvolvimiento teórico y práctico en diversos campos del conocimiento. Una de las mayores áreas de aplicación de las redes neuronales es la predicción (Sharda ). Dentro de este contexto, las redes resultan ser una herramienta atractiva para los investigadores, comparada con las metodologías tradicionales basada en modelos de funciones. Las redes neuronales artificiales intentan emular el comportamiento biológico del cerebro humano. Como sabemos, el cerebro humano es un conjunto complejo de interconexiones de elementos simples llamados nodos o neuronas. Cada nodo recibe una señal de entrada proveniente de otros nodos o a través de estímulos externos; localmente el nodo procesa la información recibida por medio de una función de transferencia o activación y produce una señal de salida transformada, que irá hacia otros nodos o como una respuesta, consecuencia de un estímulo. Aunque cada nodo individualmente no proporciona información realmente valiosa, en conjunto, pueden realizar un sorprendente número de tareas de forma eficiente. Esta característica hace de las redes neuronales un mecanismo poderoso computacionalmente para aprender a partir de ejemplos y después generalizar para casos nunca antes considerados. Aunque diferentes arquitecturas de redes neuronales han sido propuestas (Haykin ), la más utilizada es la red multilayer Perceptron (MLP). Una red MLP está compuesta de varias capas y nodos o neuronas. Los nodos de la primera capa son los encargados de recibir la información del exterior, mientras que la última capa es encargada de proporcionar la respuesta asociada a esta información. Entre estas dos capas puede haber innumerables capas y nodos. Adicionalmente, los nodos de capas adyacentes están completamente conectados. Para un problema de pronóstico con redes neuronales, las entradas a la red son asociadas con variables independientes o explicativas. En este caso la relación funcional estimada establecida por la red neuronal será de la forma \\(y_{t}=f(x_{1}, x_{2},..., x_{p})\\); donde \\(x_{1}, x_{2},..., x_{p}\\) son p variables exógenas y y una variable endógena. En este sentido la red es equivalente a un modelo de regresión no lineal. Igualmente, en el contexto del pronóstico de series temporales, las entradas de la red son series rezagadas de la original y la salida representa su valor futuro. En este caso la red haría un mapeo de la siguiente forma \\(y_{t}= f (y{t-1}, y{t-2}, ..., y{t-p})\\), donde \\(y_{t}\\) es la observación en el tiempo t. Bajo estas características la red asemeja un modelo autorregresivo en el pronóstico de series temporales. Una discusión respecto a la relación existente entre las redes neuronales y la metodología de Box-Jenkins es dada por Suykens, Vandewalle y Moor . Antes de que la red sea utilizada para realizar alguna tarea específica, debe ser entrenada. Básicamente, entrenar es el proceso de determinar los pesos (eje central de la red neuronal). El conocimiento aprendido por la red es almacenado en cada una de los arcos que representan las conexiones entre los nodos. Es a través de estas conexiones que las redes pueden realizar complejos mapeos no lineales desde los nodos de entrada hasta los nodos de salida. El entrenamiento de la MLP es supervisado, caso en el cual la respuesta deseada o valor objetivo para cada patrón de entrada o ejemplo está siempre disponible. Los datos de entrenamiento son ingresados a la red en forma de vectores de variables o como patrones de entrada. Cada elemento en el vector de entrada es asociado con un nodo de la capa de entrada; de esta forma, el número de entradas a la red es igual a la dimensión del vector de entrada. Para el pronóstico de series temporales el número de variables de entrada es difícil de establecer, no obstante, una ventana de rezagos fija es constituida a lo largo de la serie. El total de datos disponible es usualmente dividido en un conjunto de entrenamiento y otro de prueba. El primero es utilizado para estimar los pesos de la red, mientras que el segundo es empleado para evaluar la capacidad de generalización de la red. Para el proceso de entrenamiento, patrones de entrada son ingresados a la red. Los valores de activación de los nodos de entrada son multiplicados por su peso respectivo y acumulados en cada nodo sobre la primera capa. El total es evaluado en una función de activación y asumido como la salida del respectivo nodo. A esta salida algunos investigadores la identifican como la activación del nodo y es la entrada de otros nodos en capas siguientes de la red hasta que los valores de activación de la salida sean encontrados. El algoritmo de entrenamiento es utilizado para encontrar los pesos que minimicen una medida global de error tal como la suma de cuadrados del error (SSE). En el pronóstico con series temporales, un patrón de entrenamiento consiste de un conjunto de valores fijos de variables en rezago de la serie. Suponga que tenemos N observaciones \\(y_{1}, y_{2}, ..., y_{N}\\) para el proceso de entrenamiento y se requiere pronosticar un paso al frente, entonces con una red neuronal de n nodos de entrada, tenemos \\(N-n\\) patrones de entrenamiento. El primer patrón de entrenamiento estará conformado por \\(y_{1}, y_{2}, ..., y_{n}\\) como las entradas y \\(y_{n+1}\\) como el valor objetivo. El segundo patrón de entrenamiento será \\(y_{2}\\), \\(y_{3}\\), …, \\(y_{n+1}\\) y el valor de salida deseado \\(y_{n+2}\\). Finalmente, el último patrón de entrada será \\(y_{N-n}, y_{N-n+1}, ..., y_{N-1}\\) y \\(y_{N}\\) el valor objetivo. Frecuentemente, una función objetivo basada en la SSE es minimizada durante el proceso de entrenamiento, \\[\\displaystyle{E = \\frac{1}{2} \\sum_{i=n+1}^{N}(y{i}-a{i})^2 }\\] donde \\(a_{i}\\) es la salida actual de la red. Una descripción más detallada sobre las diferentes arquitecturas de red existentes, el número óptimo de capas ocultas y neuronas, las funciones de activación más utilizadas, los algoritmos de entrenamiento, la normalización de los datos, como también de otros temas relacionados con sus ventajas y deficiencias (Haykin , Kaastra y Boyd , Zhang, Patuwo y Hu , Isasi y Galván ), entre otros. 2.2.8 Metodología Splines Cúbicos de Suavizado 2.2.8.1 Teoría de interpolación En el subcampo matemático del análisis numérico, se denomina interpolación a la obtención de nuevos puntos partiendo del conocimiento de un conjunto discreto de puntos. En ciertos casos el usuario conoce el valor de una función \\(f(x)\\) en una serie de puntos \\(x_{1}, x_{2},..., x_{N}\\), pero no se conoce una expresión analítica de \\(f(x)\\) que permita calcular el valor de la función para un punto arbitrario. Un ejemplo claro son las mediciones de laboratorio, donde se mide cada minuto un valor, pero se requiere el valor en otro punto que no ha sido medido. Otro ejemplo son mediciones de temperatura en la superficie de la Tierra, que se realizan en equipos o estaciones meteorológicas y se necesita calcular la temperatura en un punto cercano, pero distinto al punto de medida. La idea de la interpolación es poder estimar \\(f(x)\\) para un valor de x arbitrario, a partir de la construcción de una curva o superficie que une los puntos donde se han realizado las mediciones y cuyo valor si se conoce. Se asume que el punto arbitrario x se encuentra dentro de los límites de los puntos de medición, en caso contrario se llamaría extrapolación. Un proceso de interpolación se realiza en dos etapas: Hacer un ajuste de los datos disponibles con una función interpolante. Evaluar la función interpolante en el punto de interés x. Este proceso en dos etapas no es necesariamente el más eficiente. La mayoría de algoritmos comienzan con un punto cercano \\(f(x_{i})\\), y poco a poco van aplicando correcciones más pequenas a medida que la información de valores \\(f(x_{i})\\) más distantes son incorporadas. El procedimiento toma aproximadamente \\(O(N^{2})\\) operaciones. Si la función tiene un comportamiento suave, la última correción será la más pequeña y puede ser utilizada para estimar un límite a rango de error. Dentro de las intepolaciones más usadas podemos destacar, Interpolación lineal. Interpolación de Hermite. Interpolación polinómica. Interpolación de Splines. 2.2.8.2 Interpolación lineal La interpolación lineal es un procedimiento muy utilizado para estimar los valores que toma una función en un intervalo del cual conocemos sus valores en los extremos \\((x_{1}, f(x_{1}))\\) y \\((x_{2},f(x_{2}))\\). Para estimar este valor utilizamos la aproximación a la función \\(f(x)\\) por medio de una recta \\(r(x)\\) (de ahí el nombre de interpolación lineal, ya que también existe la interpolación cuadrática). La expresión de la interpolación lineal se obtiene del polinomio interpolador de Newton de grado uno. Aunque sólo existe un único polinomio que interpola una serie de puntos, existen diferentes formas de calcularlo. Este método es útil para situaciones que requieran un número bajo de puntos para interpolar, ya que a medida que crece el número de puntos, también lo hace el grado del polinomio. Existen ciertas ventajas en el uso de este polinomio respecto al polinomio interpolador de Lagrange. Por ejemplo, si fuese necesario añadir algún nuevo punto o nodo a la función, tan sólo habría que calcular este último punto, dada la relación de recurrencia existente y demostrada anteriormente. El primer paso para hallar la fórmula de la interpolación es definir la pendiente de orden \\(n\\) de manera recursiva, así, \\(f_{0}(x_{i})\\) es el término i-ésimo de la secuencia. \\(\\displaystyle{f_{1}(x_{0},x_{1}) = \\frac{f_{0}(x_{1})-f_{0}(x_{0})}{x_{1}-x_{0}}}\\) \\(\\displaystyle{f_{2}(x_{0},x_{1},x_{2}) = \\frac{f_{1}(x_{1},x_{2})-f_{1}(x_{0},x_{1})}{x_{2}-x_{0}}}\\) En general, \\[\\displaystyle{f_{i}(x_{0},x_{1},...,x_{i-1},x_{i}) = \\frac{f_{i-1}(x_{1},...,x_{i-1},x_{i})-f_{i-1}(x_{0},x_{1},...,x_{i-1} )}{x_{i}-x_{0}}}\\] donde \\(\\displaystyle{x_{i}-x_{j}}\\) representa la distancia entre dos elementos. Puede apreciarse cómo en la definición general se usa la pendiente del paso anterior, \\(f_{i-1}(x_{1},...,x_{i-1},x_{i})\\), a la cual se le resta la pendiente previa de mismo orden, es decir, el subíndice de los términos se decrementa en 1, como si se desplazara, para obtener \\(f_{i-1}(x_{0},x_{1},...,x_{i-1})\\). Nótese también que aunque el término inicial siempre es \\(x_{0}\\) este puede ser en realidad cualquier otro, por ejemplo, se puede definir \\(f_{1}(x_{i-1},x_{i})\\) de manera análoga al caso mostrado arriba. Una vez conocemos la pendiente, ya es posible definir el polinomio de grado \\(n\\) de manera también recursiva, así, \\(p_{0}(x) = f_{0} (x_{0}) =x_{0}\\). Se define así ya que este valor es el único que se ajusta a la secuencia original para el primer término. \\(\\displaystyle{p_{1}(x) = p_{0}(x) + f_{1} (x_{0},x_{1}) (x-x_{0})}\\). \\(\\displaystyle{p_{2}(x) = p_{1}(x) + f_{2} (x_{0},x_{1},x_{2}) (x-x_{0})(x-x_{1})}\\). En general, \\[\\displaystyle{p_{i}(x) = p_{i-1}(x) + f_{i} (x_{0},x_{1},...,x_{i-1},x_{i}) \\prod_{j=0}^{i-1}(x-x_{j})}\\] 2.2.8.3 Interpolación de Lagrange Empezamos con un conjunto de \\(n+1\\) puntos en el plano (que tengan diferentes coordenadas x), \\((x_{0}, y_{0}), (x_{1}, y_{1}), (x_{2}, y_{2}),...,(x_{n}, y_{n})\\). Así, queremos encontrar una función polinómica que pase por esos \\(n+1\\) puntos y que tengan el menor grado posible. Un polinomio que pase por varios puntos determinados se llama un polinomio de interpolación. Una posible solución viene dada por el polinomio de interpolación de Lagrange. Lagrange publicó su fórmula en 1795 pero ya había sido publicada en 1779 por Waring y redescubierta por Euler en 1783. Dado un conjunto de \\(k + 1\\) puntos \\((x_{0}.x_{1}),...,(x_{k}.x_{k+1})\\), donde todos los \\(x_{j}\\) se asumen distintos, el polinomio interpolador en la forma de Lagrange es la combinación lineal, \\[\\displaystyle{L(x) = \\sum_{j=0}^{k} y_{j} l_{j}(x)}\\] donde $ l_{j}(x)$ son las bases polinómicas de Lagrange, \\[\\displaystyle{l_{j}(x) = \\prod_{i=0,i \\neq j}^{k} \\frac{x-x_{i}}{x_{j}-x_{i}} }\\] La función que estamos buscando es una función polinómica \\(L(x)\\) de grado \\(k\\) con el problema de interpolación puede tener tan solo una solución, pues la diferencia entre dos tales soluciones, sería otro polinomio de grado \\(k\\) a lo sumo, con \\(k+1\\) ceros. Por lo tanto, \\(L(x)\\) es el único polinomio interpolador. Si se aumenta el número de puntos a interpolar (o nodos) con la intención de mejorar la aproximación a una función, también lo hace el grado del polinomio interpolador así obtenido, por norma general. De este modo, aumenta la dificultad en el cálculo, haciéndolo poco operativo manualmente a partir del grado 4, dado que no existen métodos directos de resolución de ecuaciones de grado 4, salvo que se puedan tratar como ecuaciones bicuadradas, situación extremadamente rara. La tecnología actual permite manejar polinomios de grados superiores sin grandes problemas, a costa de un elevado consumo de tiempo de computación. Pero, a medida que crece el grado, mayores son las oscilaciones entre puntos consecutivos o nodos. Se podría decir que a partir del grado 6 las oscilaciones son tal que el método deja de ser válido, aunque no para todos los casos. Sin embargo, pocos estudios requieren la interpolación de tan sólo 6 puntos. Se suelen contar por decenas e incluso centenas. En estos casos, el grado de este polimonio sería tan alto que resultaría inoperable. Por lo tanto, en estos casos, se recurre a otra técnica de interpolación, como por ejemplo a la Interpolación polinómica de Hermite o a los splines cúbicos. Otra gran desventaja, respecto a otros métodos de interpolación, es la necesidad de recalcular todo el polinomio si se varía el número de nodos. Aunque el polinomio interpolador de Lagrange se emplea mayormente para interpolar funciones e implementar esto fácilmente en una computadora, también tiene otras aplicaciones en el campo del álgebra exacta, lo que ha hecho más célebre a este polinomio, por ejemplo en el campo de los proyectores ortogonales. 2.2.8.4 Interpolación de Hermite El objetivo de esta interpolación Hermite es minimizar el error producido en la interpolación de Lagrange de la función \\(f(x)\\) sobre el intervalo \\([a, b]\\) sin aumentar el grado del polinomio interpolador. Dados un entero no negativo N y N + 1 puntos \\((x_{0},..., x_{N})\\) de la recta distintos dos a dos y los valores \\(f^{(j)} (x_{i})\\), donde \\(0&lt;i&lt;N\\) y \\(0&lt;j&lt;k_{i-1}\\) de una función f y de sus derivadas, encontrar un polinomio de grado \\(m = (k_{0} +k_{1} +...+k_{n-1},)\\) tal que, \\[\\displaystyle{P^{(j)} (x_{i}) = f^{(j)} (x_{i}), para \\hspace{0.2cm} 0&lt;i&lt;N \\hspace{0.2cm}y \\hspace{0.2cm} 0&lt;j&lt;k_{i-1}}\\] El problema de interpolación de Hermite tiene solución única, que se llama polinomio interpolador de Hermite. En lugar de interpolar sobre un soporte de puntos (de Tchebycheff) donde en general se desconoce el valor de la función, de hace de otra manera, imponiendo unas condiciones al polinomio, Igualar el valor de la función en en los puntos del soporte, \\(P (x_{i}) = f^{(j)}\\). Igualar el valor de algunas derivadas de la función también en los puntos del soporte, \\(P^{(j)} (x_{i}) = f^{(j)} (x_{i})\\). Por lo que podemos dejar el polinomio de Hermite de grado (n-1) expresado de la siguiente manera, \\[\\displaystyle{L(x) = \\sum_{j=0}^{k} y_{j} l_{j} (x) }\\] donde \\(l_{j}(x)\\) son los polinomios de la base de Lagrange. 2.2.8.5 Interpolación Polinómica Asumamos que se tiene una tabla con \\(n\\) puntos, \\((x_{1},y_{1})...,(x_{n},y_{n})\\), donde los valores \\(x_{i}\\), para \\(i=1,...,n\\) están ordenados de forma creciente y todos ellos son distintos. Supongamos que dichos puntos se representan en un plano cartesiano y se quiere determinar una curva suave que interpole dichos valores. Así, se desea determinar una curva que esté definida para todos los \\(x\\) y tome los valores correspondientes de \\(y\\), esto es, que interpole todos los datos de la tabla. Cabe destacar que los puntos considerados se les conoce como nodos. La primera idea natural es usar una función polinomial que represente esta curva, la cual se puede representar como sigue, \\[\\displaystyle{P_{n} = \\sum_{i=0}^{n-1} a_{i}x^{i}}\\] tal curva se le conoce como función polinomial interpoladora de grado n. Nótese que en cada nodo se satisface que \\(P_{n}(x_{k})=y_{k}\\), donde \\(k=1,...,n\\). Así, se tiene que si la tabla es representada mediante una función subyacente \\(f(x)\\) tal que \\(f(x_{k})=y_{k}\\), para todo \\(k\\), entonces esta función puede ser aproximada mediante \\(P_{n}\\), en los puntos intermedios. No sería erróneo pensar que a medida que los nodos se incrementan, la aproximación sería cada vez mejor, lamentablemente esto no siempre es cierto, debido a que en el caso de tener una data con mucho ruido la interpolación no tendría mucho sentido ya que la varianza de los valores interpolados sería muy grande. En este caso, los polinomios interpoladores resultantes serían una mala representación de la función subyacente. Para evitar este fenómeno, puede ser de utilidad relajar la condición de que \\(f(x)\\) debería ser una función que interpole todos los valores dados y en su lugar usar un trozo de un polinomio local de interpolación. La función mediante la cual se logra esta interpolación se le conoce como spline. 2.2.8.6 Splines Una función spline \\(S(x)\\) es una función que consta de trozos de polinomios unidos por ciertas condiciones de suavizado. Un ejemplo simple, es una función poligonal (spline de primer grado), la cual se forma por polinomios lineales unidos, los cuales se definen entre cada par de nodos. Entre los nodos \\(x_{j}\\) y \\(x_{j+1}\\) se define un spline de primer grado como sigue, \\[\\displaystyle{S(x) = a_{j}x + b_{j} = S_{j}(x)}\\] este spline es lineal. Usualmente \\(S(x)\\) se define como \\(S_{1}(x)\\) para \\(x&lt;x_{1}\\) y como \\(S_{n-1}(x)\\) para \\(x&gt;x_{n}\\), donde \\(x_{1}\\) y \\(x_{n}\\) son nodos frontera (Ver Figura 2.2). Figura 2.2: Spline Lineal Un spline de segundo grado es una unión de polinomios cuadráticos tal que \\(S(x)\\) y su derivada \\(S^{(1)}(x)\\) son continuas (Ver Figura 2.3). Los polinomios \\(P(x)\\) a través de los que construimos el Spline tienen grado 2. Esto quiere decir, que va a tener la forma \\(P(x) = ax^2 + bx + c\\). Figura 2.3: Spline Cuadrático Como en la interpolación segmentaria lineal, vamos a tener \\(N-1\\) ecuaciones (donde N son los puntos sobre los que se define la función). La interpolación cuadrática nos va a asegurar que la función que nosotros generemos a trozos con los distintos \\(P(x)\\) va a ser continua, ya que para sacar las condiciones que ajusten el polinomio, vamos a determinar como condiciones, Que las partes de la función a trozos P(x) pasen por ese punto. Es decir, que las dos Pn(x) que rodean al f(x) que queremos aproximar, sean igual a f(x) en cada uno de estos puntos. Que la derivada en un punto siempre coincida para ambos “lados” de la función definida a trozos que pasa por tal punto común. Esto sin embargo no es suficiente, y necesitamos una condición más, la cual se obtiene a partir de una condición de borde. Por su parte un spline cúbico, se representa mediante la unión de polinomios cúbicos con primera y segunda derivada continuas (Ver Figura 2.4). Este spline debido a su flexibilidad es el más usado en las aplicaciones. Figura 2.4: Spline Cúbico Formalmente un spline cúbico con nodos \\(x_{1},...x_{n}\\) se define a partir de un conjunto de polinomios de la forma,\\ \\[\\displaystyle{S_{j}(x) = a_{j} + b_{j}x +c_{j}x^2 +d_{j}x^3}\\] con \\(x_{j}&lt;x&lt;x_{j+1}\\), sujeto a las siguientes condiciones, \\[ \\begin{array}{cc} \\displaystyle{a_{j-1} + b_{j-1}x_{j} +c_{j-1}x_{j}^2 +d_{j-1}x_{j}^3 = a_{j} + b_{j}x_{j} +c_{j}x_{j}^2 +d_{j}x_{j}^3} \\\\ \\displaystyle{ b_{j-1} +2c_{j-1}x_{j} +3d_{j-1}x_{j}^2 = b_{j} +2c_{j}x_{j} +3d_{j}x_{j}^2} \\\\ \\displaystyle{ 2c_{j-1} +6d_{j-1}x_{j} = 2c_{j} +6d_{j}x_{j}}\\\\ \\displaystyle{ c_{0} = d_{0} = c_{n} =d_{n}} \\end{array} \\] Así para n nodos, existen \\(4(n-1)\\) variables y \\(4(n-1)-2\\) restricciones. Las mismas se deben a la necesidad de que el spline cúbico sea igual en los valores dados en cada nodo. Las primeras tres restricciones aseguran que la función resultante en su primera y segunda derivada sean continuas en los nodos. La restricción final significa que el spline cúbico es lineal en el punto inicial y final de la muestra. Sin embargo, es importante resaltar que el spline cúbico tiene tercera derivada discontinua en los nodos. Debido a que hacen falta dos restricciones de borde, estas se deben añadir. Así \\(S^{(2)}(x_{1}) = S^{(2)}(x_{n}) = 0\\) son las restricciones faltantes, estan hacen referencia a que el spline sea un spline cúbico natural. Como se mencionó al inicio si se considera una interpolación polinomial global de un conjunto de datos con mucho ruido pueden surgir aproximaciones no deseables e inestables. En constrate, un spline cúbico de interpolación encaja perfectamente con la suavidad de la función subyacente. Figura 2.5: Comparativo Splines Otra característica de los splines es que con la adición de un parámetro sólo se aumenta la dimensionalidad del espacio de parámetros en una unidad, ya que tres de los cuatro parámetros están restringidos. De igual forma, al incrementar el número de nodos los splines toman formas funcionales más flexibles, lo cual muestra la relación entre el grado aproximación que se logra con el spline y el número de nodos que lo definen. Un comparativo para la interpolación lineal usando splines de primer, segundo y tercer grado, se muestra en la Figura 2.5. Mientras que las funciones spline son una herramienta interesante para interpolar funciones suaves, encontrarlas numéricamente no es tarea fácil. Una manera eficiente y muy estable para generar los splines necesarios para aproximar la función subyacente \\(f(x)\\), es usando las bases de los B-splines cúbicos. 2.2.8.7 Bases de splines En el subcampo matemático de análisis numérico, una B-spline o Basis spline (o traducido una línea polinómica suave básica), es una función spline que tiene el mínimo soporte con respecto a un determinado grado, suavidad y partición del dominio. Un teorema fundamental establece que cada función spline de un determinado grado, suavidad y partición del dominio, se puede representar como una combinación lineal de B-splines del mismo grado y suavidad, y sobre la misma partición. El término B-spline fue propuesto por Isaac Jacob Schoenberg y es la abreviatura de spline básica. Las B-splines pueden ser evaluadas de una manera numéricamente estable por el algoritmo de Boor. De un modo simplificado, se han creado variantes potencialmente más rápidas que el algoritmo de Boor, pero adolecen comparativamente de una menor estabilidad. Una B-spline es simplemente una generalización de una curva de Bézier, que puede evitar el fenómeno Runge sin necesidad de aumentar el grado de la B-spline. Este fenómeno, se presenta al realizar interpolación lineal usando nodos equidistantes, básicamente es un problema que se presenta con el error de aproximaci'n en los extremos del intervalo que se este considerando, así a medida que crece el número de nodos el error de aproximación se incrementa. Supongamos que tenemos un conjunto infinito de nodos \\(...&lt;x_{-2}&lt;x_{-1}&lt;x_{0}&lt;x_{1}&lt;x_{2}&lt;...\\), entonces el j-ésimo B-spline de grado cero es igual a \\(B^{0}_{j}(x)=1\\), si \\(x_{j} \\leq x \\leq x_{j+1}\\) y \\(B^{0}_{j}(x)=0\\) en otro caso. Con la función \\(B^{0}_{j}(x)\\) como punto de partida se puede generar B-splines de grados mayores mediante la siguiente fórmula recursiva,\\ \\[\\displaystyle{B^{k}_{j}(x) = \\frac{(x-x_{j})B^{k-1}_{j}(x)}{x_{j+k}-x_{j}} + \\frac{(x_{j+k+1}-x)B^{k-1}_{j+1}(x)}{x_{j+k+1}-x_{j+1}}}\\] para \\(k\\geq 1\\). Así un B-spline de grado \\(k\\) se define como,\\ \\[\\displaystyle{S^{k}(x) = \\sum_{j=-\\infty}^{\\infty} \\theta^{k}_{j} B^{k}_{j-k}(x)}\\] Los coeficientes \\(\\theta^{k}_{j}\\) se llaman puntos de control o puntos de Boor. Hay \\(m-(n+1)\\) puntos de control que forman una envoltura convexa. Note que los B-splines de grado positivo no son ortogonales y por ende no poseen una expresión simple para sus coeficientes. Sin embargo, los cálculos empleados para los B-splines interpoladores de grado cero y uno, son bastante sencillos,\\ \\[\\displaystyle{S^{0}(x) = \\sum_{j=-\\infty}^{\\infty} y_{j} B^{0}_{j}(x),\\hspace{0.4cm} S^{1}(x) = \\sum_{j=-\\infty}^{\\infty} y_{j} B^{1}_{j-1}(x) }\\] Cuando los nodos son equidistantes, la B-spline se dice que es uniforme, de otro modo será no uniforme. Si dos nodos tj son idénticos, cualquiera de las posibles formas indeterminadas 0/0 se consideran 0. 2.2.8.8 B-spline uniforme Cuando la B-spline es uniforme, las B-splines básicas para un determinado grado n son sólo copias cambiadas de una a otra. Una alternativa no recursiva de la definición de la B-splines \\(m-n+1\\) básica es, \\[\\displaystyle{B_{j}^{n}(t)= B_{n}(t-t_{j}), \\hspace{0.2cm}para \\hspace{0.2cm}j=0,...,m-n-1 }\\] con, \\[\\displaystyle{B_{n}(t):= \\frac{n+1}{n} \\sum_{i=0}^{n+1}w_{i}^{n}(t - t_{i})_{+}^{n} }\\] donde, \\[\\displaystyle{w_{i}^{n} := \\prod_{j=0, j\\neq i}^{n+1} \\frac{1}{t_{j}- t_{i}}}\\] nótese que \\((t - t_{i})_{+}^{n}\\) es la función potencia truncada definida como, \\[ (t - t_{i})_{+}^{n} := \\left\\{ \\begin{array}{ll} 0 &amp; \\text{ si } t &lt; t_{i} \\\\ (t - t_{i})^{n} &amp; \\text{ si } t \\ge t_{i} \\end{array} \\right. \\] 2.2.8.9 B-spline cardinal Si se define \\(B_{0}\\) como la función característica de \\({\\displaystyle [-{\\tfrac {1}{2}},{\\tfrac {1}{2}}]}\\), y \\(B_{k}\\) recursivamente como el producto convolución, \\[\\displaystyle{B_{k} := B_{k-1}*B_{0}, \\hspace{0.2cm} k=1,2,... }\\] entonces \\(B_{k}\\) se llaman B-splines cardinales (centradas). Esta definición se remonta a Schoenberg. \\(B_{k}\\) tiene soporte compacto \\({\\displaystyle [-{\\tfrac {k+1}{2}},{\\tfrac {k+1}{2}}]}\\) y es una función impar. Como \\({\\displaystyle k\\rightarrow \\infty }\\) las B-splines cardinales normalizadas tienden a la función de Gauss. Cuando el número de puntos de control de Boor es el mismo que el grado, la B-Spline degenera en una curva de Bézier. La forma de las funciones base es determinada por la posición de los nodos. Escalar o trasladar el vector de nodo no altera las funciones de base. El spline está contenido en el casco convexo de sus puntos de control. Una B-spline básica de grado n \\(B_{i}^{n}(t)\\) es distinta de cero sólo en el intervalo \\([t_{i}, t_{i+n+1}]\\) esto es, \\[ B_{i}^{n}(t) = \\left\\{ \\begin{array}{ll} &gt; 0 &amp; \\text{ si } t_{i} \\leq t &lt; t_{i+n+1} \\\\ 0 &amp; \\text{ si } resto \\end{array} \\right. \\] En otras palabras si manipulamos un punto de control cambiamos sólo el comportamiento local de la curva y no el comportamiento global como con las curvas de Bézier. La función base se pueda obtener del polinomio de Bernstein. Algunos ejemplos de las bases B-splines se muestran a continuación, 2.2.8.10 B-spline constante La B-spline constante es la spline más simple. Se define en un solo tramo de nodo y ni siquiera es continua en los nodos. Es sólo la función indicador de los diferentes tramos de nodo. \\[ B_{j}^{0}(t) = 1_{[t_{j},t_{j+1})} = \\left\\{ \\begin{array}{ll} 1 &amp; \\text{ si } t_{j} \\leq t &lt; t_{j+1}1 \\\\ 0 &amp; \\text{ si } resto \\end{array} \\right. \\] 2.2.8.11 B-spline lineal La B-spline lineal se define en dos tramos de nodo consecutivos y es continua sobre los nodos, pero no diferenciable. \\[ B_{j}^{1}(t) = \\left\\{ \\begin{array}{ll} \\frac{t-t_{j}}{t_{j+1}-t_{j}} &amp; \\text{ si } t_{j} \\leq t &lt; t_{j+1} \\\\ \\frac{t_{j+2}-t}{t_{j+2}-t_{j+1}} &amp; \\text{ si } t_{j+1} \\leq t &lt; t_{j+2} \\\\ 0 &amp; \\text{ si } resto \\end{array} \\right. \\] 2.2.8.12 B-spline cuadrática uniforme Las B-splines cuadráticas con nodo-vector uniforme es una forma común de B-spline. La función base puede ser calculada fácilmente , y es igual para cada segmento, en este caso. \\[ B_{j}^{2}(t) = \\left\\{ \\begin{array}{ll} \\frac{1}{2} (1-t)^2 \\\\ -t^2+t+\\frac{1}{2} \\\\ \\frac{1}{2} t^2 \\end{array} \\right. \\] Escrito en forma de matriz, esto es, \\[\\displaystyle{S_{i}(t) = \\begin{bmatrix} t^2 &amp; t &amp; 1 \\end{bmatrix} \\frac{1}{2} \\begin{bmatrix} 1 &amp; -2 &amp; 1\\\\ -2 &amp; 2 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 \\end{bmatrix} \\begin{bmatrix} \\theta_{i-1} \\\\ \\theta_{i} \\\\ \\theta_{i+1} \\end{bmatrix} , \\hspace{0.2cm} para \\hspace{0.2cm} t \\in [0,1],\\hspace{0.2cm} i=1,2,...,m-1 }\\] 2.2.8.13 B-spline cúbica Una formulación B-spline para un solo segmento puede ser escrita como, \\[\\displaystyle{S_{i}(t) = \\sum_{k=0}^{3} \\theta_{i-3+k} B_{i-3+k}^{3}(t) \\hspace{0.2cm} con \\hspace{0.2cm} t \\in [0,1] }\\] donde \\(S_{i}\\) es el i-ésimo segmento B-spline, \\(\\theta\\) es el conjunto de puntos de control, el segmento i y k es el índice del punto de control local y $ B_{i-3+k}^{3}(t)$ representa la base de B-spline de grado 3. Un conjunto de puntos de control \\(\\theta\\) sería \\({\\displaystyle \\theta_{i}^{w}=(w_{i}x_{i},w_{i}y_{i},w_{i}z_{i},w_{i})}\\) donde el \\({\\displaystyle w_{i}}\\) es el peso, tirando de la curva hacia el punto de control \\({\\displaystyle \\theta_{i}}\\) mientras que aumenta o se desplazan fuera de la curva, a la vez que disminuye. Toda una serie de segmentos se definiría como, \\[\\displaystyle{S(t) = \\sum_{i=0}^{m-1} \\theta_{i} B_{i}^{3}(t) }\\] donde i es el número de puntos de control y t es un parámetro global dados los valores de los nodos. Esta formulación expresa una curva B-spline como una combinación lineal de funciones B-spline básicas, de ahí el nombre. Hay dos tipos de B-spline - uniforme y no uniforme. Una B-spline no uniforme es una curva donde los intervalos entre los puntos sucesivos de control no son, o no necesariamente son, iguales (el vector de nodos de espacios de nodo interiores no son iguales). Una forma común es donde los intervalos se reducen sucesivamente a cero, interpolando los puntos de control. 2.2.8.14 B-spline cúbica uniforme La B-spline cúbica con vector-nodo uniforme es la forma más usual de B-spline. La función base puede ser fácilmente calculada, y es igual para cada segmento, en este caso. Puesto en forma de matriz, esto es, \\[\\displaystyle{S_{i}(t) = \\begin{bmatrix} t^3 &amp; t^2 &amp; t &amp; 1 \\end{bmatrix} \\frac{1}{6} \\begin{bmatrix} -1 &amp; 3 &amp; -3 &amp; 1 \\\\ 3 &amp; -6 &amp; 3 &amp; 0 \\\\ -3 &amp; 0 &amp; 3 &amp; 0 \\\\ 1 &amp; 4 &amp; 1 &amp; 0 \\end{bmatrix} \\begin{bmatrix} \\theta_{i-1} \\\\ \\theta_{i} \\\\ \\theta_{i+1} \\\\ \\theta_{i+2} \\end{bmatrix} , \\hspace{0.2cm} para \\hspace{0.2cm} t \\in [0,1] }\\] Para splines de grados más elevados, algunas arbitrariedades surgen al momento de calcular los coeficientes \\(\\theta_{i}^{k}\\). Por lo tanto, debido a que en las aplicaciones estadisticas existe un mayor interés por encontrar una aproximación que una interpolación, la técnica de minimos cuadros puede ser empleada para calcular estos valores. Ahora bien, supongamos que se tiene un conjunto de \\(m\\) funciones diferenciables \\(f(x)\\), con soporte en el intervalo \\([a,b]\\), las cuales satifacen las siguientes condiciones, \\(f(x_{i})=y_{i}\\), para \\(i=1...,n\\). La m-1 derivada \\(f^{(m-1)}(x)\\), es continua en x. El problema es encontrar entre todas esas funciones, una función tal que tenga la mínima integral del cuadro de su segunda derivada, esto es, una función que tenga el valor más pequeño de \\(\\int_{a}^{b} (f^{(m)}(x))^2 dx\\). Dicha función será la elección más óptima al momento de hallar un balance entre suavidad y ajuste de los datos. Se puede desmostrar que la solución de este problema es única y la función en cuestión es un spline polinomial que cumple la condición i), y además satisface que, f es un pilinomio de grado no mayor que \\(m-1\\) cuando \\(x \\in [a,x_{1}]\\) y \\(x \\in [x_{n},b]\\) . F es un polinomio de grado no mayor a \\(2m-1\\) para puntos interiores, \\(x \\in [x_{i},x_{i+1}]\\) con i=1,…,n. f(x) tiene \\(2m-2\\) derivadas continuas en el eje real. En resumen, la función \\(f\\) mínima es un spline el cual está conformado por trozos de polinomios unidos en los nodos \\(x_{i}\\), donde dicha función tiene \\(2m-2\\) derivadas continuas. Nótese que en muchas aplicaciones \\(m=2\\) es un valor muy utilizado y cuya solución viene dada mediante el spline cúbico natural. 2.2.9 Regresión no paramétrica La teoría clásica de la regresión se basa, en gran parte, en el supuesto que las observaciones son independientes y se encuentran idéntica y normalmente distribuidas. Si bien existen muchos fenómenos del mundo real que pueden modelarse de esta manera, para el tratamiento de ciertos problemas, la normalidad de los datos es insostenible. En el intento de eliminar esa restricción se diseñaron métodos que hacen un número mínimo de supuestos sobre los modelos que describen las observaciones. La teoría de los métodos no paramétricos trata, esencialmente, el desarrollo de procedimientos de inferencia estadística, que no realizan una suposición explícita con respecto a la forma funcional de la distribución de probabilidad de las observaciones de la muestra. Si bien en la Estadística no paramétrica también aparecen modelos y parámetros, ellos están definidos de una manera más general que en su contrapartida paramétrica. La regresión no paramétrica es una colección de técnicas para el ajuste de funciones de regresión cuando existe poco conocimiento a priori acerca de su forma. Proporciona funciones suavizadas de la relación y el procedimiento se denomina suavizado. Los fundamentos de los métodos de suavizado son antiguos pero sólo lograron el estado actual de desarrollo gracias a los avances de la computación y los estudios por simulación han permitido evaluar sus comportamientos. La técnica más simple de suavizado, los promedios móviles, fue la primera en usarse, sin embargo han surgido nuevas técnicas como la estimación mediante núcleos (``kernel&quot;) o la regresión local ponderada. Estos estimadores de regresión no paramétrica son herramientas poderosas para el análisis de datos, tanto como una técnica de estimación para resumir una relación compleja que no puede ser aprendida por un modelo paramétrico, como para suplementar (o complementar) un análisis de regresión paramétrico. En los análisis paramétricos se comienza haciendo supuestos rígidos sobre la estructura básica de los datos, luego se estiman de la forma más eficiente posible los parámetros que definen la estructura y por último se comprueba si los supuestos iniciales se cumplen. La regresión no paramétrica, en cambio, desarrolla un ``modelo libre&quot; para predecir la respuesta sobre el rango de valores de los datos. Básicamente está constituida por métodos que proporcionan una estimación suavizada de la relación para un conjunto de valores (de- nominado ventana) de la variable explicativa. Estos valores son ponderados de modo que, por ejemplo, los vecinos más cercanos tengan mayor peso que los más alejados dentro de una ventana de datos. Se pueden utilizar diversas funciones de ponderación, que son los pesos en que se basan los estimadores. La combinación de la función de ponderación y el ancho de la ventana inciden sobre la bondad de la estimación resultante. La mayor parte de las publicaciones sobre regresión no paramétrica consideran el caso de un solo regresor a pesar de que, a simple vista no pareciera de gran utilidad, ya que las aplicaciones más interesantes involucran varias variables explicativas. Sin embargo, la regresión no paramétrica simple es importante por dos motivos: En etapas preliminares del análisis de datos o en pruebas de diagnóstico se utilizan gráficos de dispersión en los cuales puede ser muy útil ajustar una “curva suavizada”. Por ejemplo, para explorar la forma de la función respuesta, para confirmar una función respuesta en particular que haya sido ajustada a los datos, para obtener estimaciones de la respuesta media sin especificar la forma de la función respuesta, para estudiar el cumplimientos de supuestos, etc. Forma la base a partir de la cual se extienden los conceptos para regresión no paramétrica múltiple. 2.2.10 Regresión no paramétrica mediante splines de suavizado Consideremos el siguiente modelo de regresión homocedástico, \\[\\displaystyle{Y_{i}=f(X_{i})+\\epsilon_{i}}, \\hspace{0.3cm} para \\hspace{0.2cm} i=1,...,n\\] donde los \\(\\epsilon_{i}\\) son errores de media cero independientes e idénticamente distribuidos. Uno de los posibles métodos para emplear splines es aproximar la función de regresión subyacente mediante las bases de splines, por ejemplo, la base de los B-splines cúbicos. Así, se escoge una secuencia fija de nodos \\(-\\infty&lt;t_{1}&lt;t_{2}&lt;...&lt;t_{J}&lt;\\infty\\), los cuales pueden diferir de los predictores. Luego, se calculan los elementos de la base cúbica de spline correspondiente. Es posible mostrar que sólo son necesarios \\(J+4\\) elementos de esta base. Denotemos a estos elementos por \\(B_{j}(x)\\), así el spline polinomial lo podemos expresar como sigue, \\[\\displaystyle{S(x)=\\sum_{j=1}^{J+4} \\theta_{j}B_{j}(x)}\\] Entonces los coeficientes \\(\\theta_{j}\\) pueden ser calculados al ser considerados como los parámetros que se obtienen al minimizar la suma de los errores al cuadrado, \\[\\displaystyle{\\sum_{i=1}^{n} \\left[ Y_{i} - \\sum_{j=1}^{J+4} \\theta_{j}B_{j}(X_{j})\\right]^2}\\] Denotamos por \\(\\hat{\\theta_{j}}\\) al estimador de mínimos cuadrados y definimos el estimador del spline polinomial como sigue, \\[\\displaystyle{ \\hat{f}_{n}(x) = \\sum_{j=1}^{J+4} \\hat{\\theta_{j}}B_{j}(x)}\\] Otro enfoque, se basa en la idea de encontrar un curva suave que minimize la suma penalizada de errores al cuadrado, es decir, que minimize la siguiente expresión, \\[ n^{-1}\\sum_{j=1}^{n}(Y_{j}-f(X_{j}))^2+\\mu \\int_{a}^{b} [f^{(m)} (x)]^2 dx \\] para algún \\(\\mu &gt; 0\\). Así como el enfoque de interpolación anterior, la solución de este problema de minimización es un spline, el cual recibe el nombre de estimador de spline de suavizado. En particular, para el caso \\(m=2\\) el minimizador de la ecuación 9, es un spline cúbico natural. Note que \\(\\mu\\) juega el papel de parámetro de suavizado, este término se puede interpretar como una penalización por rugosidad de la función. Curvas que cambian lenta o suavemente presentan un valor pequeño de la integral, por ejemplo, en una función lineal la integral toma el valor de cero. De hecho, la primera suma en la ecuación 9 penaliza la falta de fidelidad de la aproximación de la data mediante el spline. El segundo término es el responsable de la suavidad de la aproximación obtenida mediante el spline. Para ver esto considérese los casos extremos, es decir, cuando \\(\\mu =0\\) y \\(\\mu=\\infty\\). El primer caso conduce a una interpolación, esto es \\(\\hat{f}(X_{i})=Y_{i}\\) para \\(i=1,...,n\\). El otro caso, conduce a una regresión lineal pues \\(f^{(2)}(x)\\equiv 0\\). Por lo tanto \\(\\mu\\) es el parámetro de suavizado que controla la medida del estimador del spline polinomial, el cual puede variar desde el modelo más complicado e inestable hasta el modelo más simple. En otras palabras, la ecuación 9 representa un balance entre la fidelidad o ajuste de los datos, representado mediante la suma de los residuos al cuadrado y la suavidad de la curva resultante, la cual se representa por la integral del cuadrado de la m-émisa derivada. 2.2.11 Proceso de Optimización de Nelson y Siegel y Svensson Para aplicar este proceso es necesario tener una función objetivo, sobre la cual se realizará el proceso de optimización, ya sea para maximizar ó minimizar dicha función. Dependiendo de la forma de dicha función el proceso de optimización será lineal o no lineal. En nuestro caso particular se llevará a cabo un proceso de optimización no lineal donde se buscará minimizar la función objetivo. En el cálculo de nuestra función objetivo inteviene el concepto de la duración de un bono ó título, la cuál es una medida del vencimiento medio ponderado de todos los flujos que paga un bono. La misma viene dada mediante la siguiente expresión, \\[\\displaystyle{Duracion = \\frac{1+r}{r} - \\frac{n(c-r)+(1+r)}{c(1+r)^{n}-(c-r)}}\\] donde r es el rendimiento al vencimiento del bono durante el período considerado. n es el número de períodos que restan hasta la fecha de vencimiento del bono. c es el cupón del bono. Así nuestra función objetivo viene dada mediante la siguiente expresión, \\[ f(x) = \\sum_{i=1}^{n} (w_{i}\\epsilon(x)_{i} )^2 \\] donde \\(w_{i}\\) representan las ponderaciones, y se calculan mediante la siguiente expresión, \\[\\displaystyle{w_{i} = \\frac{\\frac{1}{D_{i}}}{\\sum_{j=1}^{N}\\frac{1}{D_{j}}}}\\] por su parte, \\(\\epsilon_{i}(x)= \\hat{Pr}_{i}(x)-Pr_{i}\\), donde \\(Pr_{i}\\) representan los precios promedios de los títulos a considerar, de entrada este es un parámetro ó valor con el que se cuenta. Por otra parte \\(\\hat{Pr}_{i}(x)\\) representa los precios estimados donde \\(x\\) es el parámetro que va a variar y es el valor que se quiere optimizar. Mediante la función objetivo descrita anteriormente se busca minimizar la diferencia que existe entre los precios promedios y los precios estimados, calculando un valor óptimo del parámetro \\(x\\) mediante el proceso de optimización no lineal. El proceso de optimización se realizó mediante el software estadístico R, mediante el paquete “nloptr”. En este paquete, se encuentra el comando “aulag” el cual minimiza un función objetivo y devuelve entre otros valores el parámetro más óptimo, que hace que la función sea mínima. Un ejemplo del uso de este comando se presenta acontinuación, \\[ala2=auglag(1.22, fn=mifuncion, hin=res)\\] donde el primer argumento debe ser el valor inicial del parámetro a optimizar, el segundo argumento “fn” se refiere a la función que se desea optimizar, finalmente en el tercer parámetro “hin” se indican las restricciones sobre el parámetro a optimizar, en este caso la restricción establecida es que el parámetro sea mayor a cero. Recordemos que la tasa cero cupón que se obtiene mediante la metodología de Svensson está dada por la siguiente expresión, \\[\\displaystyle{s(m) = \\beta_{0}+ \\beta_{1}\\frac{\\left(1-e^\\frac{-m}{\\tau_{1}}\\right)}{m/\\tau_{1}} + \\beta_{2} \\left(\\frac{\\left(1-e^\\frac{-m}{\\tau_{1}}\\right)}{m/\\tau_{1}} - e^\\frac{-m}{\\tau_{1}}\\right) + \\beta_{3} \\left(\\frac{\\left(1-e^\\frac{-m}{\\tau_{2}}\\right)}{m/\\tau_{2}} - e^\\frac{-m}{\\tau_{2}}\\right)}\\] esta expresión está sujeta a las siguientes restricciones, \\(\\beta_{0} &gt; 0\\) \\(\\beta_{0}+\\beta_{1} &gt; 0\\) \\(\\tau_{1} &gt; 0\\) \\(\\tau_{2} &gt; 0\\) cada parámetro controla una sección de la curva. La fórmula anterior es de suma importancia ya que ella interviene en el cálculo del precio teórico de cada instrumento. El proceso de optimización actúa directamente sobre esta fórmula, ya que el mismo se centra en variar los parámetros de tal manera que la función objetivo sea minimizada. Como se observó en las secciones anteriores el parámetro de suavizamiento fué elegido mediante el método de ensayo y error el cual no es para nada óptimo pues a priori este método no nos garantiza que el valor seleccionado sea el mejor, ya que se contarían con una gran cantidad de posibles valores a seleccionar, con el fin de encontrar dicho parámetro el procedimiento anteriormente explicado puede ser implementado. Sin embargo, al realizar este proceso, se obtienen curvas que no son para nada suaves y en ocasiones no poseen ningunas de las formas usuales de la curva de rendimientos. Esto es debido a que en este caso este proceso, varía el parámetro de suavizamiento de tal manera que la diferencia entre el precio promedio y el precio teórico sea lo mas pequeña posible y en este proceso no existe un parámetro que controle la forma de la curva obtenida. Por lo tanto, su aplicación presenta algunos inconvenientes. 2.2.12 Elaboracion Base de datos para la metodología Splines y Diebold-Li La fuente principal de información para calcular la curva de rendimientos para los títulos de la deuda pública nacional es el Banco Central de Venezuela (BCV), el cual diariamente publica las operaciones realizadas con estos instrumentos y los publica en el documento “resumersec” (Ver Figura 2.6. Es importante destacar, que en este documento se encuentran por día dos pestañas, la \\(``0-22&quot;\\) y la \\(``0-23&quot;\\), en la primera pestaña se encuentran las operaciones interbancarias, por su parte la segunda pestaña muestra información sobre las operaciones realizadas por entes privados, en este caso el precio pautado en la operación no está disponible, razón por la cual esta pestaña no se toma en consideración. La información disponible en la pestaña \\(``0-22&quot;\\) es la siguiente, Código del instrumento: código único que se asocia a cada instrumento. Fecha de vencimiento: fecha de maduración de cada instrumento. Plazo: cantidad de días que faltan para que el instrumento venza. Cantidad de Operaciones: número de operaciones realizadas con cada instrumento. Monto en Bolívares: monto total involucrado en la operación. Precio mínimo: precio más bajo pautado en la operación. Precio máximo: precio más alto pautado en la operación. Precio promedio: precio promedio pautado en la la operación. Cabe destacar que si existe una sóla operación, los precios mínimo, máximo y promedio serán iguales. Cupón: tasa de interés pagadera por cada instrumento. Figura 2.6: Pestaña 0-22 Es importante recordar que dentro de los títulos de la Deuda Pública Nacional se encuentran los títulos de interés fijo (TIF) y los títulos de tasa variable (VEBONO), los primeros se caracterizan por poseer una tasa de cupón que no varia, por su parte los VEBONO poseen una tasa de interés variable. Esta información también es suministrada por el BCV, en su documento de las “Características de la Deuda Pública Nacional” (Ver Figura 2.7 ), por lo cual el mismo se debe revisar con cierta frecuencia, con el fin de actualizar la tasa de cupón de los VEBONO. En este documento se muestra información que caracteriza a cada instrumento, el mismo posee varias pestañas, en este trabajo sólo se considerará la pestaña \\(``DPN&quot;\\) en donde se encuentra información sobre los instrumentos emitidos en moneda nacional. La información disponible en este documento se muestra a continuación, Número-Emisión-Decreto: información sobre emisión de cada instrumento. Código: código único que se asocia a cada instrumento. Fecha de emisión: fecha cuando se emitió cada instrumento. Fecha de vencimiento: fecha de maduración de cada instrumento. Monto en Circulación: monto total de cada instrumento en circulación. Porcentaje de referecia: indica si el instrumento es de tasa fija o tasa variable. Fecha de inicio: indica cada cuanto tiempo el instrumento paga cupón. Período vigente: indica el período (fecha inicio y fecha fin) cuando cada instrumento paga cupón. Tasa: cupón asociado a cada instrumento. Figura 2.7: Características A partir de la pestaña “0-22” y del documento de las características, se creó la base de datos con la cual se va a trabajar, la misma contiene no sólo la información suministrada por la pestaña “0-22”, sino alguna información adicional tomada del documento de las características. En dicha base de datos se contará con la siguiente información, Tipo Instrumento: Indica el tipo de instrumento. Nombre: Proporciona el nombre corto del título, usualmente este nombre se conforma por el tipo de título más su mes y año de vencimiento, por ejemplo, el título TIF032028, representa al título TIF con vencimiento en marzo del 2028. Fecha de operación: Indica la fecha en que se efectuó dicha operación. Fuente: Indica la fuente de donde se tomó la información, esta se puede tomar de dos fuentes, la primera mediante la pestaña 0-22 (mercado secundario) y la otra mediante el documento de las subastas (mercado primario, información suministrada por el BCV). Sicet: Proporciona el código asociado a cada título. Fecha de vencimiento: Indica la fecha de maduración (vencimiento) del instrumento. Plazo: Indica la cantidad de días que falta para que el instrumento se venza. Cantidad de operaciones: Proporciona la cantidad de operaciones efectuadas con un insrumento en específico. Monto: Indica el monto en Bolívares, por el cual se efectuó la operación u operaciones. Precio mínimo: Indica el precio mínimo, por el cual se transó la operación. Precio máximo: Indica el precio máximo, por el cual se transó la operación. Precio promedio: Indica el precio promedio, por el cual se transó la operación, cabe destacar que en dado caso de existir una sola operación el valor del precio mínimo, máximo y promedio van a coincidir. Cupón: Proporciona la tasa de cupón asociado a cada instrumento. Frecuencia: Indica con que frecuencia el instrumento paga cupón, para los TIF y VEBONO, esta es 4, pues los mismos pagan cu'pon trimestralmente, así se obtiene este valor pues existen 4 trimestres en el año. "],
["valor-en-riesgo.html", "Capítulo 3 Valor en Riesgo 3.1 Metodologías para calcular el VaR 3.2 VaR Simulación Histórica 3.3 VaR Simulación MonteCarlo 3.4 Limitaciones del VaR", " Capítulo 3 Valor en Riesgo El valor en riesgo, conocido comúnmente como VaR (Value at Risk), es una técnica estadística para medir el riesgo financiero de una inversión. Indica la probabilidad (normalmente 1% o 5%) de sufrir una determinada pérdida durante un periodo de tiempo (normalmente 1 día, 1 semana o 1 mes). Dicho de otro modo, el VaR establece la pérdida máxima que puede experimentar una inversión dentro de un horizonte temporal, dado un nivel de confianza (1- α), normalmente 95% o 99%. Por ejemplo, la pérdida máxima será durante un mes con un 95% de probabilidad igual o menor que 5 millones de euros. O lo que es lo mismo, existe una probabilidad del 5% de que la pérdida sea como mínimo de 5 millones de euros en un mes. Por lo tanto, también mide la pérdida mínima que sufrirá una inversión para un nivel de significación (α). El VaR mide el riesgo financiero de una inversión, por lo que tiene una amplia aplicación en el mundo de las finanzas. Se puede calcular la pérdida máxima tanto para un solo activo financiero como para una cartera de activos financieros. Es muy utilizado en análisis de riesgos para medir y controlar el nivel de riesgo que una empresa es capaz de soportar. El gestor de riesgos es asegurarse de que no se incurre en riesgos mayores de los que la empresa podría afrontar. Se trata de un método para cuantificar la exposición al riesgo de mercado, utilizando técnicas estadísticas tradicionales. Partamos de la base de que los agentes económicos de hoy enfrentan riesgos de diferente naturaleza, como por ejemplo de crédito, de mercado, de liquidez, operacional, legal, etc. El Valor en Riesgo vendría a medir la pérdida que se podría sufrir en condiciones normales de mercado en un intervalo de tiempo y con un cierto nivel de probabilidad o de confianza. Este método fue desarrollado por matemáticos y estadísticos de JP Morgan a principios de los 90, y fue adaptado rápidamente por el resto de las firmas financieras de Wall Street gracias al gran éxito inicial y a su simplicidad de concepto. Sin embargo, no nos olvidemos de que el VaR mide probabilidades. Es decir, solo proporciona una medida resumida del riesgo del mercado. Se puede distinguir entre el VaR absoluto y el VaR relativo. En función del tipo, se pueden utilizar distintos tipos de métodos para hallarlo. Durante años, el VaR fue la panacea de los gurúes de Wall Street. Pero tantas noticias buenas les nubló la razón, y no se percataron de que lo único que mide el VaR son probabilidades. Es decir, hay un 99% de probabilidades de no perder. Pero nadie miró a ese 1% restante hasta que finalmente sucedió. Ese 1% se materializó y las pérdidas se acumularon en miles de millones. Se podría decir que el VaR hundió a Wall Street, ya que no tomaron en cuenta las probabilidades matemáticas negativas. Lo cierto es que la matemática no falla. Y las probabilidades, por mínimas que parezcan, pueden volverse en nuestra contra. La metodología VaR constituye el desarrollo natural de la Teoría de Carteras de Markowitz de los años cincuenta. Su principal impulso en el mundo financiero lo recibió de J.P. Morgan, cuando un alto ejecutivo preguntó por la máxima pérdida probable en las siguientes veinticuatro horas, surgiendo así el informe posterior al cierre, denominado 4:15, que hoy día da nombre a la aplicación de la metodología desarrolla por esta entidad financiera (Risk MetricsTM). El VaR puede entenderse de diversas maneras, cada una de las cuáles constituye una definición: Cuantía máxima de dinero que puede perderse en un período para un nivel específico de confianza. Procedimiento numérico, estadístico o matemático que genera un VaR monetario. Metodologías o conjunto de procedimientos que permiten estimar el valor monetario del riesgo. Técnica de gestión del riesgo, que conlleva una reestructuración de la entidad y de sus posiciones, en función del binomio rendimiento-riesgo, medido este último en términos VaR. En cuanto la propuesta Risk Metrics, destacar, en primer lugar, que se trata de un estudio sobre el riesgo de mercado y su medición a través de la metodología VaR, y dentro del mismo, aparecen las características técnicas de la propuesta de JP Morgan, así como la posibilidad de que esta entidad suministre la base de datos necesaria. Su aplicación está fundamentada en una contabilidad mark-to-market, realizándose las estimaciones no sobre precios, sino sobre rentabilidades. En esta propuesta pueden diferenciarse dos partes, una destinada al análisis de la metodología VaR, y otra que presenta el estudio de la propuesta concreta. A esta última parte le acompaña un completo estudio estadístico, sobre el que se fundamenta, en lo referente al comportamiento de la volatilidad y de las correlaciones entre factores de riesgo; destaca en este apartado estadístico la aplicación de la descomposición de Cholesky, empleada por JP Morgan en la estimación de la matriz de covarianzas tanto para la propuesta delta-normal como para la de simulación de Monte Carlo. En definitiva, según cuáles sean los parámetros iniciales y en base al modelo empleado, existen infinidad de posibilidades de estimación del riesgo según la metodología VaR, lo que hace difícil elegir una aplicación práctica concreta, de manera que cada entidad deberá buscarla para cubrir sus necesidades y objetivos. De todo ello se deduce que el VaR no es un único valor, sino que fluctuará en función de las decisiones iniciales que se adopten sobre el modelo, por tanto el análisis del riesgo a través de esta metodología quedaría incompleto si no se acompañase de otros análisis complementarios. Se pretende pues, establecer más que la bondad del modelo VaR empleado, lo cual será un objetivo estadístico, completar la información suministrada, con la intención de facilitar la toma de decisiones. 3.0.1 Importancia del VaR Tras el estallido de la crisis en 2008, el VaR cobra especial importancia, sobre todo en las salas de tesorería de los bancos. La creciente exigencia de capital (Basilea III) hacia el sector bancario y en consecuencia un mayor control de riesgos, hacen que los departamentos de riesgos asignen (menores cantidades de consumo de VaR), un VaR diario, semanal y mensual a las diferentes mesas de tipo de interés, bonos, trading, volatilidad u otros instrumentos negociables en los mercados. No obstante, también tiene especial importancia en el mundo de la gestión de activos, en la gestión de carteras o en otros sectores en contacto con los mercados financieros. 3.1 Metodologías para calcular el VaR Existen tres principales métodos para calcular el VaR: VaR paramétrico: utiliza datos de rentabilidad estimados y asume una distribución normal de la rentabilidad. VaR histórico: utiliza datos históricos. VaR por Monte Carlo: utiliza un software informático para generar cientos o miles de posibles resultados según unos datos iniciales introducidos por el usuario. ##VaR Paramétrico El VaR paramétrico es un método para estimar el VaR (Valor en Riesgo) utilizando datos de rentabilidad estimados y asumiendo una distribución normal de la rentabilidad. También se conoce como método varianza-covarianza o método analítico. Cuando tenemos los datos de rentabilidad esperados y el riesgo histórico (medido por la desviación típica) utilizamos la siguiente fórmula: \\[VaR = |R – z·δ|· V\\] Siendo R, la rentabilidad esperada, z, el valor correspondiente para un nivel de significancia (por ejemplo 1,645 para 5%), δ, la desviación típica de la rentabilidad y V, el valor de la inversión. El método del VaR paramétrico es el método más fácil de calcular, ya que el VaR histórico aunque puede ser más fácil de entender es bastante más laborioso de calcular y aunque es menos preciso que el VaR por Montecarlo es más sencillo de aplicar. Este método se basa en la hipótesis de la distribución conjunta normal de los rendimientos de la cartera, y en la hipótesis de la relación lineal (a lo más cuadrática) entre los factores de riesgo del mercado y el valor de la empresa. Ventajas: La estimación de VaR a través de éste método es simple y rápida, lo que lo hace atractivo cuando se trabaja en tiempo real. Desventajas: Sobreestima el VaR para niveles bajos de confianza, y lo subestima para niveles altos de confianza (inconveniente derivado de la suposición de normalidad en los retornos de la cartera y no capturar el fenómeno de las colas gruesas). Otra desventaja es que la hipótesis de linealidad condiciona a este método a ser aplicable a carteras lineales, en un mundo donde los activos no lineales toman cada día mas fuerza. Por último, incluso ampliando la aproximación del valor de la cartera a una cuadrática, no se logra una buena precisión en la estimación del VaR de carteras no lineales. Teniendo en cuenta que la aplicación complica el cálculo de VaR, disminuyendo de esa forma una de las ventajas más claras que tiene el método. 3.2 VaR Simulación Histórica El VaR histórico o VaR por simulación histórica es un método para estimar el VaR (Valor en Riesgo) que utiliza datos históricos.Una de las maneras de calcular el VaR por el método histórico es acumulando las rentabilidades pasadas y ordenarlas desde la más alta hasta la más baja. Después, identificamos el 5% de datos con rentabilidades más bajas y el mayor de ese 5% de rentabilidades más bajas será el VaR. Los datos para calcular el VaR histórico son los precios históricos de los títulos. Por tanto, una serie histórica más grande (por ejemplo 5 o 10 años) dará resultado a mayores resultados simulados y por tanto será más precisa que una serie histórica de 3 meses. La principal desventaja del modelo histórico para calcular el VaR es que se asume que las rentabilidades obtenidas en el pasado se van a repetir en el futuro. El VAR por simulación histórica es una de las formas del cálculo del VaR, siempre un poco más laboriosa que el VaR paramétrico y menos precisa que el VaR por simulación de montecarlo. Se trata de aplicar a la cartera de activos financieros, variaciones históricas del precio de los títulos para generar escenarios contrastables con la posición inicial (conocida como spot en inglés), generando diferentes posibles resultados simulados a partir de los cuales se obtendrá el VAR. Es un método aplicable tanto a carteras lineales como no lineales, debido a que es un método no paramétrico, que no depende de ninguna de las hipótesis sobre distribuciones de probabilidad subyacente y, por lo tanto, permite capturar el fenómeno de las colas gruesas al mismo tiempo que elimina la necesidad de estimar y trabajar con volatilidades y correlaciones, evitando en gran medida el riesgo de modelización. Ventajas: Las ventajas fueron mencionadas en su definición, y hace que este método sea preferible al de matriz varianza-covarianza, especialmente cuando se trata del cálculo del VaR para carteras de instrumentos no lineales. Desventajas: La principal desventaja del método de simulación histórica viene dada por las características de los datos utilizados, que supone que ningún evento que no haya ocurrido en el pasado podrá ocurrir en el futuro. 3.3 VaR Simulación MonteCarlo 3.3.1 Origenes La invención del método de MonteCarlo se asigna a Stanislaw Ulam y a John von Neumann. Ulam ha explicado cómo se le ocurrió la idea mientras jugaba un solitario durante una enfermedad en 1946. Advirtió que resulta mucho más simple tener una idea del resultado general del solitario haciendo pruebas múltiples con las cartas y contando las proporciones de los resultados que computar todas las posibilidades de combinación formalmente. Se le ocurrió que esta misma observación debía aplicarse a su trabajo de Los Álamos sobre difusión de neutrones, para la cual resulta prácticamente imposible solucionar las ecuaciones íntegro-diferenciales que gobiernan la dispersión, la absorción y la fisión. “La idea consistía en probar con experimentos mentales las miles de posibilidades, y en cada etapa, determinar por casualidad, por un número aleatorio distribuido según las probabilidades, qué sucedería y totalizar todas las posibilidades y tener una idea de la conducta del proceso físico”. Podían utilizarse máquinas de computación, que comenzaban a estar disponibles, para efectuar las pruebas numéricas y en efecto reemplazar el aparato experimental del físico. Durante una de las visitas de von Neumann a Los Álamos en 1946, Ulam le mencionó el método. Después de cierto escepticismo inicial, von Neumann se entusiasmó con la idea y pronto comenzó a desarrollar sus posibilidades en un procedimiento sistemático. Ulam expresó que MonteCarlo “comenzó a tener forma concreta y empezó a desarrollarse con todas sus fallas de teoría rudimentaria después de que se lo propuse a Johnny”. A principios de 1947 Von Neumann envió una carta a Richtmyer a Los Álamos en la que expuso de modo influyente tal vez el primer informe por escrito del método de MonteCarlo. Su carta fue encuadernada junto con la respuesta de Richtmyer como un informe de Los Álamos y distribuida entre los miembros del laboratorio. Von Neumann sugería aplicar el método para rastrear la generación isótropa de neutrones desde una composición variable de material activo a lo largo del radio de una esfera. Sostenía que el problema era adecuado para el ENIAC y estimaba que llevaría 5 horas calcular la acción de 100 neutrones a través de un curso de 100 colisiones cada uno. Ulam estaba particularmente interesado en el método MonteCarlo para evaluar integrales múltiples. Una de las primeras aplicaciones de este método a un problema determinista fue llevada a cabo en 1948 por Enrico Fermi, Ulam y von Neumann cuando consideraron los valores singulares de la ecuación de Schrödinger. 3.3.2 Definición El VaR por MonteCarlo es un método para estimar el VaR (Valor en Riesgo) que utiliza un software informático para generar cientos o miles de posibles resultados según unos datos iniciales introducidos por el usuario. Los resultados obtenidos se ordenan desde la rentabilidad más alta a la más pequeña como en el cálculo del VaR por el método histórico. Después, identificamos el 5% de datos con rentabilidades más bajas y el mayor de ese 5% de rentabilidades más bajas será el VaR. Los datos se suelen presentar de forma gráfica para tener una mejor visualización de los resultados y su frecuencia. La principal ventaja de estimar el VaR por el método de Monte Carlo es a su vez su principal desventaja, ya que dependiendo de los datos iniciales introducidos se generarán una serie de presunciones que guiarán los resultados (path dependency o dependiente del camino escogido). Dada la complejidad de Monte Carlo, se puede tener una falsa sensación de fiabilidad, pero si los datos introducidos (inputs) no son correctos la información no será fiable. A pesar de ello suele ser más preciso que el método del VaR paramétrico. El método se llamó así en referencia al Casino de Montecarlo (Mónaco) por ser “la capital del juego de azar”, al ser la ruleta un generador simple de números aleatorios. El nombre y el desarrollo sistemático de los métodos de Montecarlo datan aproximadamente de 1944 y se mejoraron enormemente con el desarrollo de la computadora. El método de Montecarlo proporciona soluciones aproximadas a una gran variedad de problemas matemáticos posibilitando la realización de experimentos con muestreos de números pseudoaleatorios en una computadora. El método es aplicable a cualquier tipo de problema, ya sea estocástico o determinista. A diferencia de los métodos numéricos que se basan en evaluaciones en N puntos en un espacio M-dimensional para producir una solución aproximada, el método de Montecarlo tiene un error absoluto de la estimación que decrece como \\({\\displaystyle {\\frac {1}{\\sqrt {N}}}}\\) en virtud del teorema del límite central. Ventajas: Las ventajas fueron mencionadas anteriormente, y ubican a este método de simulación por sobre los otros dos métodos analizados anteriormente. Desventajas: La mayor desventaja de este método es su lentitud, pero este inconveniente se va solucionando rápidamente a través del tiempo debido al desarrollo informático que se esta teniendo. 3.4 Limitaciones del VaR Principalmente las limitaciones del VaR vienen dadas por: La suposición de normalidad es inadecuada para la medición de riesgo en las colas de la distribución. En un mercado heterocedástico, la varianza no es un múltiplo del horizonte temporal y, por ejemplo, la varianza semanal no tiene relación con la varianza diaria. El VaR no tiene en cuenta que el riesgo de liquidez puede ser el mayor riesgo en algunos mercados. En algunos instrumentos que son nuevos o ilíquidos, y que no están muy anidados al mercado los costes liquidez son casi indistinguibles de los riesgos de mercado, por lo que la venta de un paquete grande de éstos activos, sobre todo si es una venta forzada, podría alterar fuertemente los precios de mercado de éstos. Tanto la diversificación, como la correlación entre activos falla en los momentos de tensión. La evidencia empírica lo muestra en épocas de crisis, como son por ejemplo la crisis de la bolsa en 1987, la de tipos de cambio en 1992, bonos en 1994, entre otras, en donde las correlaciones desaparecen y la diversificación se ve afectada por éste hecho. La volatilidad no es observable, por lo que siempre se hace necesario estimarla. Para esta estimación no existe un acuerdo sobre que método es mejor, e incluso si es mejor o no utilizar estimaciones implícitas. Los derivados presentan una no linealidad que dificulta el cálculo del VaR. No existe ningún tipo de convención que establezca de que forma se debe calcular el VaR en carteras con estas posiciones. El VaR es tan útil como buenos sean los resultados que se han utilizado para calcularlo. Si los datos incluidos no son correctos, el VaR no será útil. El VaR no considera todos los peor escenarios posibles. Para solventar esto, el VaR se complementa con los test de estrés, que consideran extremos escenarios no contemplados por el VaR. Algunos métodos para calcularlo son costosos y difíciles de aplicar (MonteCarlo). Los resultados obtenidos por diferentes métodos pueden ser diferentes. Genera una falsa sensación de seguridad, cuando es tan solo una probabilidad. + No tiene porque darse por sentado. No calcula la cuantía de la pérdida esperada que se queda en el porcentaje de probabilidad. A veces la diversificación que otorga el VaR no es intuitiva. Podemos pensar que es mejor invertir solo en los sectores que tienen mayores rendimientos por cada unidad de riesgo, pero de esta manera no diversificamos el riesgo. "],
["backtesting.html", "Capítulo 4 Backtesting 4.1 Definición 4.2 Prueba de Kupiec 4.3 Test de Prueba Mixta de Kupiec (Haas) 4.4 Marco de referencia Comité de Basilea (1996).", " Capítulo 4 Backtesting 4.1 Definición El Valor en Riesgo (VaR) se ha convertido en un modelo ampliamente empleado por las instituciones financieras y administradores de portafolios para medir el riesgo de mercado. Sin embargo, los modelos de VaR son útiles sólo si se predicen con exactitud los riesgos futuros. Con la finalidad de evaluar la calidad de las estimaciones de los modelos de Valor en Riesgo, se debe emplear el Backtesting que es un procedimiento estadístico donde las ganancias y las pérdidas reales se comparan sistemáticamente con el modelo de medición de riesgos. El rol que juega el VaR en la actualidad como una medida de riesgo incrementa la necesidad de evaluar la calidad de sus estimaciones. El Backtesting es un proceso donde los beneficios o pérdidas actuales son comparados con la estimación del modelo de Valor en Riesgo, los métodos de Backtesting pueden ser incondicionales o condicionales. Los métodos incondicionales cuentan el número de excepciones y las comparan con el nivel de confianza, si las excepciones están dentro de los límites estadísticos definidos el modelo es aceptado, y en caso contrario rechazado. Por el contrario, los métodos condicionales evalúan la independencia de las excepciones. Dentro de los métodos incondicionales se encuentran el Test de kupiec y el semáforo propuesto por Basilea, por su parte el Test de Haas y la Prueba Mixta de Kupiec son ejemplos de los métodos condicionales. El concepto de Backtesting es esencial en el proceso de evaluar y calibrar los modelos de medición de riesgos. Para realizar un backtesting es necesario comparar el valor en riesgo observado con las pérdidas y ganancias reales. En esta prueba lo que se mide es la eficiencia en el modelo, contando las observaciones de pérdidas y ganancias que fueron mayor al VaR. 4.2 Prueba de Kupiec Uno de los principales métodos desarrollados para calibrar y validar los modelos de riesgo es el test de Kupiec. El planteamiento propuesto por Paul H. Kupiec en 1995, permite determinar si la proporción de excepciones observadas de un modelo es consistente con la proporción de excepciones esperada, si se tiene en cuenta tanto el modelo VaR elegido como el nivel de confianza del mismo. Dado que la realización del VaR no es observable, se tienen que realizar varias consideraciones para evaluar las diferentes aproximaciones para estimar el VaR. La manera más intuitiva para comprobar la bondad del modelo propuesto será comprobar cuál es la proporción de períodos de la muestra en que se observa una pérdida superior a la predicción del modelo (es decir, superior al VaR). Dicha proporción debería ser en promedio igual al nivel de significancia. En otras palabras, el modelo debe proveer la cobertura no condicionada esperada por el diseño. La prueba consiste en contar las veces que las pérdidas y ganancias exceden el VaR durante un período (porcentaje de excepciones). Se asume que T es el número de observaciones que exceden la pérdida o ganancia, y para un nivel de confianza dado (1-P), se prueba si la N observada es estadísticamente diferente a la probabilidad de error p que se considera para el cálculo del VaR. La probabilidad de observar N excesos durante un período de T observaciones en total, se explica con una distribución binomial dada por: \\[f(x) = \\left(\\frac{T}{x}\\right) p^{x}(1-p)^{T-x}\\] Kupiec desarrolló unas regiones de confianza con base en una distribución chi-cuadrado con un grado de libertad, considerando la hipótesis nula de que p es estadísticamente igual a la probabilidad utilizada para el VaR, contra la hipótesis alternativa de que p sea diferente a dicha probabilidad. Las regiones fueron determinadas de los extremos de máxima verosimilitud dada por la siguiente ecuación: \\[Lrpof = -2* Ln(\\frac{(1-p)^{T-x}p^{x}}{\\left[1-\\left(\\frac{T}{x}\\right)\\right]^{T-x}\\left(\\frac{T}{x}\\right)^{x}})\\] La máxima probabilidad del resultado observado bajo la hipótesis nula se define en el numerador y la máxima probabilidad del resultado observado bajo la hipótesis alternativa se define en el denominador. La decisión se basará en el valor del ratio. Mientras más pequeño es el ratio, mayor será el estadístico L, si el valor del estadístico es mayor que el valor crítico de la distribución chi-cuadrado, la hipótesis nula será rechazada. 4.3 Test de Prueba Mixta de Kupiec (Haas) El Test de Haas plantea como hipótesis nula que las excepciones son independientes y por ende también lo son sus estadísticos, al ser los estadísticos independientes los mismos pueden sumarse, en vista de que el Test de Haas establece que el estadístico se comporta como una distribución chi cuadrado con n grados de libertad igual al número de las excepciones, la distribución chi cuadrado también será aditiva de esta manera podemos sumar los valores críticos, como resultado se tiene un Test de Independencia donde se plantea como Hipotesis Nula \\(H_0\\): que las Excepciones son Independientes, al tener n excepciones, el correspondiente estadístico es \\[L_{ind} = \\sum_{i=2}^{n} \\left[- 2Ln\\left( \\frac{p(1-p)^{v_i-1}}{\\left(\\frac{1}{v_i}\\right) \\left( 1- \\frac{1}{v_i}\\right)^{v_i-1}}\\right)\\right] - 2Ln\\left( \\frac{p(1-p)^{v-1}}{\\left(\\frac{1}{v}\\right) \\left( 1- \\frac{1}{v}\\right)^{v-1}}\\right)\\] Donde el valor \\(v_i\\) ubicado en el primer término de la ecuación corresponde al tiempo entre la excepción \\(i\\) y la excepción \\(i-1\\) y el segundo término corresponde al Test de Kupiec: (Tiempo Hasta la Primera Excepción), por su parte \\(v\\) es el tiempo donde ocurre la primera excepción. Una vez calculado del Test de Haas podemos realizar la prueba conjunta de condicionalidad e independencia, la cual consiste en sumar el Test de Kupiec (\\(L_{rpof}\\)) y el Test de Haas (\\(L_{ind}\\)). \\[L_{mix} = L_{rpof}+L_{rind}\\] El valor crítico será obtenido de una distribución chi cuadrado con n+1 grados de libertad, donde n es el número de excepciones del período en estudio. 4.4 Marco de referencia Comité de Basilea (1996). El comité de Basilea de Supervisión Bancaria elaboró un marco para la incorporación de backtesting en los modelos de valoración de riesgo, con el objetivo de diferenciar claramente modelos precisos de modelos imprecisos, ya que en la actualidad se siguen desarrollando nuevos enfoques que permiten perfeccionar los diferentes métodos. El objetivo del marco de referencia es promover enfoques más rigurosos en el desarrollo del backtesting y en la interpretación de los resultados (Basel Committee on Banking Supervision 1996). El marco abarca la comparación periódica del valor en riesgo diario con la pérdida y/o ganancia diaria real, y permite comparar si el porcentaje real de los resultados cubiertos por la medida de riesgo es consistente con un nivel de 99% de confianza; adicionalmente, se encuentra propuesto para el uso de medidas de riesgo calibradas para un período de tenencia de un día. El Comité de Basilea prevé que el requisito mínimo de capital (RMC) para cubrir el riesgo de mercado, sea calculado como el máximo valor entre el VaR del día anterior y el producto entre el promedio de los últimos 60 VaR diarios y un factor multiplicativo, calculado como la suma entre 3 y un factor aditivo !! que se encuentra relacionado con la calidad del modelo y que se fija dependiendo del número de pérdidas que exceden al VaR (excepciones). La expresión para hallar el RMC se encuentra a continuación: \\[ RMC_{t+1} = max[VaR_t , (3+s_t)*\\frac{1}{60}\\sum_{i=1}^{60} VaR_{t-1} ]+SR_t \\] Para evaluar la precisión del modelo VaR, se emplea el backtesting interpretando los resultados de acuerdo a la clasificación en tres zonas que se distinguen por colores y que jerarquizan los resultados. Las zonas, que se clasifican en verde, amarilla y roja, se encuentran determinadas de forma que se equilibren los tipos de errores estadísticos en los que es posible caer: (error tipo I) clasificar un modelo preciso siendo impreciso y (error tipo II) clasificar un modelo impreciso como un modelo preciso. Zona verde Los resultados que se encuentran en este rango requieren de poca información adicional. En esta región el modelo no necesita ser calibrado, ya que no presenta problemas de calidad. Para una muestra de 250 observaciones y con un método calculado con el 99% de confianza, se espera que un máximo de cuatro excepciones. La expresión que permite calcular el umbral de esta zona, para una muestra de \\(n\\) observaciones, se describe a continuación: \\[ \\binom{x}{n} * 0,01*(1-0.01)^{n-x} &lt; 0.95 \\] al despejar \\(x\\) de la desigualdad resulta el valor máximo de la zona verde, lo que quiere decir que esta zona está comprendida entre 0 y \\(x\\) excepeciones. Zona amarilla En esta zona, la interpretación de resultados es válida tanto para modelos precisos como para modelos imprecisos; sin embargo, para este rango la probabilidad de concluir que el modelo es inexacto cuando realmente no lo es, aumenta con el aumento del número de excepciones. Para un modelo de 250 observaciones, con el 99% de confianza, el umbral de la zona amarilla corresponde a nueve excepciones. La expresión que permite calcular el umbral de esta zona, para una muestra de ! observaciones, se describe a continuación: \\[ \\binom{y}{n} * 0,01*(1-0.01)^{n-y} &lt; 0.99 \\] al despejar \\(y\\) de esta desigualdad, se obtiene el límite superior de la zona amarilla, lo que quiere decir que esta zona está comprendida entre \\(x+1\\) y \\(y\\) excepeciones. Zona roja Los resultados que se encuentran en la zona roja indican automáticamente un problema en calidad del modelo y en la confiabilidad del mismo, debido a que no se considera la posibilidad de que un modelo exacto genere tantas excepciones independientes. Con esta conclusión, se pone en evidencia la necesidad de modificar la estimación. Para un modelo de 250 observaciones la zona roja corresponde a mínimo 10 excepciones. La construcción de este marco incentiva la utilización de modelos precisos, de forma que premian con un factor multiplicativo más pequeño los modelos bien calibrados, y penalizan un factor multiplicativo mayor a aquellos modelos que tengan excepciones que caigan en el límite de la zona amarilla y en la zona roja. "],
["pruebas-de-estres.html", "Capítulo 5 Pruebas de Estrés 5.1 Definición 5.2 Utilidad", " Capítulo 5 Pruebas de Estrés 5.1 Definición Las Pruebas de Estrés aplicables al Sistema Financiero son una medida de la exposición al riesgo de un grupo de instituciones financieras a un escenario macroeconómico o microfinanciero particular, ya sea histórico o hipotético, por lo que constituyen una herramienta útil para analizar la vulnerabilidad actual y futura de las entidades de intermediación financiera ante choques adversos. Desde el punto de vista regulador, los ejercicios de estrés resultan relevantes para monitorear los distintos riesgos a que está expuesto el sistema, en la medida en que proporcionan alertas sobre potenciales resultados adversos e informan sobre el capital necesario para enfrentar las pérdidas que pudieran materializarse. De hecho, las pruebas de tensión son un requisito presente en los estándares internacionales de buenas prácticas de gestión de riesgos tales como Basilea II, donde se requiere el uso de pruebas de tensión en el cálculo directo del capital mínimo regulatorio por riesgo de crédito y mercado (Pilar I), así como en la evaluación interna de la suficiencia de capital de cada entidad de intermediación financiera. Mediante la realización de Pruebas de Estrés es posible identificar las áreas de vulnerabilidad y/o el grado de exposición al riesgo del Sistema Financiero ante variaciones en las variables macroeconómicas y variables especificas de las entidades. Asimismo, es posible analizar el impacto de cambios en aspectos regulatorios, además de determinar el requerimiento de capital adicional necesario para mantener la estabilidad financiera ante choques de índole macroeconómica o microprudencial. La realización de Pruebas de Estrés involucra un conjunto de técnicas estadísticas, econométricas y de análisis, y su uso está ampliamente arraigado en las Superintendencias de Bancos, Bancos Centrales, Organismos Multilaterales, así como en Entidades de Intermediación Financiera, en razón de su evidente utilidad para el análisis de estabilidad financiera, el cual requiere una vigilancia sistemática de todas las fuentes y vulnerabilidades de riesgo, considerando también la medida en que las perturbaciones pueden ser absorbidas por el sistema financiero. 5.2 Utilidad Las pruebas de estrés son realmente necesarias después de largos períodos donde las condiciones económicas y financieras han sido favorables, debido a que la buena marcha de la economía durante tanto tiempo puede conducir a la complacencia y a la infravaloración del riesgo. También se emplea como una herramienta clave en la gestión de riesgos durante los períodos de expansión, cuando la innovación conduce a nuevos productos con alta demanda, de los cuales no se tiene ninguna experiencia histórica. En definitiva, lo que tratan de medir y analizar las pruebas de estrés es la solvencia financiera de una institución ante escenarios extremadamente adversos y muy volátiles. Estas pruebas deben ser realizadas por las empresas para mejorar en su toma de decisiones, estableciendo límites de exposición al riesgo, y que sirven para la evaluación de las opciones estratégicas en la planificación empresarial a largo plazo. Las pruebas de estrés deberían servir para los siguientes fines: Identificación del riesgo Los tests de estrés deben estar incluidos en las actividades de gestión de riesgos de las instituciones en los distintos niveles. En particular, se debe utilizar para abordar e identificar los riesgos en toda la institución. Complemento a otras herramientas de gestión de riesgos Las pruebas de estrés deben complementar a otras metodologías de cuantificación del riesgo que se basan en complejos modelos cuantitativos que analizan datos históricos y realizan proyecciones a futuro para tratar de conocer cuáles serán los escenarios a los que se van a enfrentar. En particular, los resultados de las pruebas de estrés para una cartera pueden proporcionar información acerca de la validez de los modelos estadísticos, por ejemplo los que se utilizan para determinar el VaR (Value at Risk). Apoyo a la gestión del capital Estas pruebas también deben formar parte de la gestión del capital interno de las entidades, donde con visión de futuro, tratarán de identificar problemas graves, incluyendo una serie de eventos de capitalización o cambios en las condiciones de mercado que podrían afectar negativamente a la institución. Mejora en la gestión de la liquidez Los tests de estrés deben ser la herramienta central en la identificación, medición y control de los riesgos de liquidez de financiación. En particular debe servir para identificar el perfil de liquidez de la institución y la adecuación de sus reservas en caso de que tanto la institución como el mercado estén en riesgo. Las pruebas de estrés tienen que cubrir una serie de riesgos y áreas de negocio, así como a nivel de toda la institución. Una empresa debe ser capaz de integrar de manera efectiva y significativa, en toda la gama de sus actividades, las pruebas de estrés para proporcionar una imagen completa de los riesgos en toda la institución. Antes de cualquier prueba de estrés debe definirse el nivel de granularidad adecuado para el propósito de las pruebas, así como qué efectos se van a evaluar a través de todos los factores de riesgo que se agreguen al test. Además también será necesario tener en cuenta las interrelaciones entre todos los factores. Los riesgos típicos que se suelen tener en cuenta en estas pruebas son los siguientes: Riesgo de crédito, incluyendo la contraparte y el riesgo de reaseguro. Riesgo de mercado Riesgo de seguro Riesgo de liquidez Riesgo operacional y legal Riesgo de concentración Riesgo de contagio Riesgo de reputación Riesgo regulatorio Riesgo de inflación Las pruebas de estrés deben llevarse a cabo con flexibilidad e imaginación, con el fin de mejorar la probabilidad de identificar las vulnerabilidades ocultas de la institución. La omisión de factores de riesgo puede dar lugar a una subestimación de la severidad de los efectos que puedan producir eventos extremos. Estos tests deben ofrecer una amplia gama de niveles de gravedad, incluyendo eventos capaces de generar el mayor daño a la institución, ya sea daño financiero o cualquier otro daño que provoque una gran caída de la reputación de la empresa frente a terceros. "],
["manual-de-usuario.html", "Capítulo 6 Manual de Usuario 6.1 Curva de Rendimientos 6.2 Valor en Riesgo 6.3 Backtesting 6.4 Valoración", " Capítulo 6 Manual de Usuario 6.1 Curva de Rendimientos 6.1.1 Curva de rendimientos individual En esta sección el usuario puede calcular los precios teóricos de los instrumentos de la Deuda Pública Nacional (TIF ó VEBONOS) para una fecha considerada y para una metodología en específico. Las metodologías disponibles son Nelson y Siegel, Svensson, Diebold-Li y Splines cúbicos de suavizado (Ver Figura 6.1). Para el cálculo de las metodología de Nelson y Siegel, así como la de Svensson y Diebold-Li es importante contar con los precios promedios de los instrumentos a considerar puesto que a partir de los mismos se realizará un proceso de optimización el cual busca encontrar los parámetros más adecuados de esta metodología de manera que los precios teóricos obtenidos se asemejen a los precios promedio. Esta sección permite calcular los precios teóricos de los instrumentos de la deuda pública nacional (Ver Figura 6.2). Figura 6.1: Sección individual curva de rendimientos Figura 6.2: Precios promedio 6.1.2 Datos Esta sección nos permite obtener de manera automática los archivos necesarios para el cálculo de los precios teóricos de los instrumentos a considerar (Ver Figura 6.3). Los archivos necesarios son obtenidos de la página del Banco Central de Venezuela, ellos son, Documento de operaciones del mercado secundario (resumersec) . Documento de las características de los instrumentos de la Deuda Pública Nacional. Figura 6.3: Sección Datos 6.1.3 Metodología Nelson y Siegel Con el fin de proceder a realizar los cálculos mediante esta metodología el usuario deberá seguir los siguientes pasos, Seleccionar una fecha para la cual se calcularán los precios estimados (Ver Figura 6.4). Figura 6.4: Sección Nelson y Siegel individual Selecionar los instrumentos a considerar ya sean TIF ó VEBONO (Ver Figura 6.5), para este fin el usuario podrá elegir los mismos selecionando su nombre corto o ingresando un archivo plano (Ver Figura 6.6). Cabe destacar que este archivo debera tener las siguientes características, Debe ser un archivo plano (txt). Debe contener dos columnas. En la primera columna debe contener el ISIN ó coódigo del instrumento. En la segunda columna debe contener el nombre corto del instrumento. Ejemplo estructura fila: Para el TIF de ISIN “DPBS04686-0040”, cuyo vencimiento es el “02/08/2019”. En este caso el nombre corto de este instrumento es “TIF082019”. Así para cada fila debe existir la siguiente información “DPBS04686-0040” “TIF082019”. El archivo debe contener tantas filas como instrumentos desee considerar el usuario. Posteriormente se mostraran los títulos seleccionados por el usuario así como su respectivo precio promedio es caso de existir, de lo contrario el precio asignado será de 0. Se mostrará también el documento de las características de los instrumentos financieros venezolanos, con el fin de que el usuario pueda observar en detalle cada instrumento. Figura 6.5: Selecionar instrumentos Figura 6.6: Selecionar instrumentos desde un archivo plano El usuario debera seleccionar un método para calcular los precios teóricos, los cuales se ubican en estas secciones (Ver Figura 6.7), Parámetros iniciales: en esta sección los parámetros a considerar serán tomados por defecto y los cálculos del los precios y curva de rendimientos (Ver Figura 6.8) se realizarán con los mismos. Figura 6.7: Opción parámetros iniciales Figura 6.8: Curva de rendimientos inicial Elegir parámetros: en esta sección el usuario podrá elegir los parámetros de está metodología siempre y cuando los mismos cumplan con ciertas restricciones (Ver Figura 6.9). El usuario deberá seleccionar un valor para las siguientes variables, \\(\\beta_{0}\\). \\(\\beta_{1}\\). \\(\\beta_{2}\\). \\(\\tau_{1}\\). Posteriormente la herramienta generará los siguientes resultados, Nuevos parámetros seleccionados. Verificación de que los parámetros cuamplan con las restricciones necesarias. Precios teóricos de los instrumentos seleccionados. Curva de rendimientos obtenida en base a los parámetros seleccionados. Figura 6.9: Opción elegir parámetros Parámetros optimizados: en esta sección los parámetros a utilizar serán los obtenidos luego de realizar un proceso de optimización sobre los mismos (Ver Figura 6.10). Posteriormente se mostrarán los precios teóricos y curva de rendimientos obtenida para estos parámetros (Ver Figuras 6.11 y 6.12). Figura 6.10: Opción optimizar parámetros Figura 6.11: Precios optimizados Figura 6.12: Parámetros optimizados 6.1.3 Metodología Svensson Con el fin de proceder a realizar los cálculos mediante esta metodología el usuario deberá seguir los siguientes pasos, Seleccionar una fecha para la cual se calcularán los precios estimados (Ver Figura 6.13). Figura 6.13: Sección Svensson individual Selecionar los instrumentos a considerar ya sean TIF ó VEBONO, para este fin el usuario podrá elegir los mismos selecionando su nombre corto o ingresando una archivo plano (Ver Figuras 6.5 y 6.6). Cabe destacar que este archivo debera tener las siguientes características, Debe ser un archivo plano (txt). Debe contener dos columnas. En la primera columna debe contener el ISIN ó coódigo del instrumento. En la segunda columna debe contener el nombre corto del instrumento. Ejemplo estructura fila: Para el TIF de ISIN “DPBS04686-0040”, cuyo vencimiento es el “02/08/2019”. En este caso el nombre corto de este instrumento es “TIF082019”. Así para cada fila debe existir la siguiente información “DPBS04686-0040” “TIF082019”. El archivo debe contener tantas filas como instrumentos desee considerar el usuario. Posteriormente se mostraran los títulos seleccionados por el usuario así como su respectivo precio promedio es caso de existir, de lo contrario el precio asignado será de 0. Se mostrará también el documento de las características de los instrumentos financieros venezolanos, con el fin de que el usuario pueda observar en detalle cada instrumento. El usuario debera seleccionar un método para calcular los precios teóricos, los cuales se ubican es estas secciones, Parámetros iniciales: en esta sección los parámetros a considerar serán tomados por defecto y los cálculos del los precios y curva de rendimientos se realizarán con los mismos (Ver Figuras 6.14 y 6.15). Figura 6.14: Parámetros iniciales Figura 6.15: Curva de rendimientos parámetros iniciales Elegir parámetros: en esta sección el usuario podrá elegir los parámetros de está metodología siempre y cuando los mismos cumplan con ciertas restricciones (Ver Figura 6.16). El usuario deberá seleccionar un valor para las siguientes variables, \\(\\beta_{0}\\). \\(\\beta_{1}\\). \\(\\beta_{2}\\). \\(\\beta_{3}\\). \\(\\tau_{1}\\). \\(\\tau_{2}\\). Posteriormente la herramienta generará los siguientes resultados, Nuevos parámetros seleccionados. Verificación de que los parámetros cuamplan con las restricciones necesarias. Precios teóricos de los instrumentos seleccionados. Curva de rendimientos obtenida en base a los parámetros seleccionados. Figura 6.16: Elegir parámetros Parámetros optimizados: en esta sección los parámetros a utilizar serán los obtenidos luego de realizar un proceso de optimización sobre los mismos. Posteriormente se mostrarán los precios teóricos y curva de rendimientos obtenida para estos parámetros (Ver Figuras 6.17 y 6.18). Figura 6.17: Precios optimizados Figura 6.18: Parámetros optimizados 6.1.4 Metodología Diebold-Li Con el fin de proceder a realizar los cálculos mediante esta metodología el usuario deberá seguir los siguientes pasos, Seleccionar una fecha para la cual se calcularán los precios estimados (Ver Figura 6.19). Selecionar los instrumentos a considerar ya sean TIF ó VEBONO, para este fin el usuario podrá elegir los mismos selecionando su nombre corto o ingresando una archivo plano. Cabe destacar que este archivo debera tener las siguientes características, Debe ser un archivo plano (txt). Debe contener dos columnas. En la primera columna debe contener el ISIN ó coódigo del instrumento. En la segunda columna debe contener el nombre corto del instrumento. Ejemplo estructura fila: Para el TIF de ISIN “DPBS04686-0040”, cuyo vencimiento es el “02/08/2019”. En este caso el nombre corto de este instrumento es “TIF082019”. Así para cada fila debe existir la siguiente información “DPBS04686-0040” “TIF082019”. El archivo debe contener tantas filas como instrumentos desee considerar el usuario. Posteriormente se mostraran los títulos seleccionados por el usuario así como su respectivo precio promedio es caso de existir, de lo contrario el precio asignado será de 0. Se mostrará también el documento de las características de los instrumentos financieros venezolanos, con el fin de que el usuario pueda observar en detalle cada instrumento. Figura 6.19: Sección Diebold-Li Posteriormente, el usuario en caso de no observar ninguna curva graficada deberá seleccionar una mayor cantidad de observaciones (Ver Figura 6.20), esto se debe a que no se están considerando la suficiente cantidad de operaciones para graficar la curva de rendimientos a partir de la cual se obtendrán los rendimientos teóricos, los cuales son necesarios para el cálculo de los precios teóricos según esta metodología. Figura 6.20: Poca cantidad de observaciones Una vez visualizada la curva ajustada mediante el spline, es necesario calibrar el parámetro de suavizamiento, el mismo controla la suavidad de la curva mostrada (Ver Figura 6.21). Figura 6.21: Curva spline Luego de seleccionar este valor, se mostrará el spline a utilizar en el cálculo de los precios teóricos (Ver Figura 6.22). Posteriormente se mostrarán los precios estimados y la curva de rendimientos obtenida, la cual en este caso es una superficie (Ver Figura 6.23). Esto debido a considerar los parámetros de Diebold-Li dinámicos con respecto al tiempo. Figura 6.22: Precios estimados Figura 6.23: Curva de rendimiento Diebold-Li 6.1.5 Metodología Splines Con el fin de proceder a realizar los cálculos mediante esta metodología el usuario deberá seguir los siguientes pasos, Seleccionar una fecha para la cual se calcularán los precios estimados (Ver Figura 6.24). Selecionar los instrumentos a considerar ya sean TIF ó VEBONO, para este fin el usuario podrá elegir los mismos selecionando su nombre corto o ingresando una archivo plano. Cabe destacar que este archivo debera tener las siguientes características, Debe ser un archivo plano (txt). Debe contener dos columnas. En la primera columna debe contener el ISIN ó coódigo del instrumento. En la segunda columna debe contener el nombre corto del instrumento. Ejemplo estructura fila: Para el TIF de ISIN “DPBS04686-0040”, cuyo vencimiento es el “02/08/2019”. En este caso el nombre corto de este instrumento es “TIF082019”. Así para cada fila debe existir la siguiente información “DPBS04686-0040” “TIF082019”. El archivo debe contener tantas filas como instrumentos desee considerar el usuario. Posteriormente se mostraran los títulos seleccionados por el usuario así como su respectivo precio promedio es caso de existir, de lo contrario el precio asignado será de 0. Se mostrará también el documento de las características de los instrumentos financieros venezolanos, con el fin de que el usuario pueda observar en detalle cada instrumento. Figura 6.24: Sección splines Posteriormente, el usuario en caso de no observar ninguna curva graficada deberá seleccionar una mayor cantidad de observaciones (Ver Figura 6.25), esto se debe a que no se están considerando la suficiente cantidad de operaciones para graficar la curva de rendimientos a partir de la cual se obtendrán los rendimientos teóricos, los cuales son necesarios para el cálculo de los precios teóricos según esta metodología. Figura 6.25: Pocas observaciones Una vez visualizada la curva ajustada mediante el spline, es necesario calibrar el parámetro de suavizamiento, el mismo controla la suavidad de la curva mostrada (Ver Figura 6.26). Figura 6.26: Parámetro de suavizamiento A continuación se mostraran los títulos candidatos a partir de los cuales la curva de rendimientos será trazada (Ver Figura 6.27). Posteriormente se mostrarán los precios estimados y la curva de rendimientos obtenida (Ver Figuras 6.28 y 6.29). Figura 6.27: Títulos candidatos Figura 6.28: Precios spline Figura 6.29: Curva de rendimientos spline Luego en caso de que el usuario desee eliminar alguna observación en específico, la misma debe ser elegida a partir de la lista desplegable que se muestra (Ver Figura 6.30). Al seleccionar una observación, se mostrará la nueva data con la que se trabajará (títulos candidatos, ver figura 6.31) así como los nuevos precios teóricos calculados y nueva curva de rendimientos obtenida (Ver Figura 6.32). Figura 6.30: Opción eliminar observaciones Figura 6.31: Nuevos títulos candidatos Figura 6.32: Nuevos precios y curva de rendimientos 6.1.6 Curva de rendimientos comparativo En esta sección permite obtener un comparativo de los precios teóricos obtenidos mediante las diferentes metodologías disponibles, las cuales son Nelson y Siegel, Svensson, Diebold-Li y Splines cúbicos de suavizado (Ver Figura 6.33). Figura 6.33: Sección comparativo 6.1.7 Metodologías Con el fin de proceder a realizar los cálculos en esta sección el usuario deberá seguir los siguientes pasos, Seleccionar una fecha para la cual se calcularán los precios estimados (Ver Figura 6.34). Selecionar los instrumentos a considerar ya sean TIF ó VEBONO, para este fin el usuario podrá elegir los mismos selecionando su nombre corto o ingresando una archivo plano. Cabe destacar que este archivo debera tener las siguientes características, Debe ser un archivo plano (txt). Debe contener dos columnas. En la primera columna debe contener el ISIN ó coódigo del instrumento. En la segunda columna debe contener el nombre corto del instrumento. Ejemplo estructura fila: Para el TIF de ISIN “DPBS04686-0040”, cuyo vencimiento es el “02/08/2019”. En este caso el nombre corto de este instrumento es “TIF082019”. Así para cada fila debe existir la siguiente información “DPBS04686-0040” “TIF082019”. El archivo debe contener tantas filas como instrumentos desee considerar el usuario. Posteriormente se mostraran los títulos seleccionados por el usuario así como su respectivo precio promedio es caso de existir, de lo contrario el precio asignado será de 0. Se mostrará también el documento de las características de los instrumentos financieros venezolanos, con el fin de que el usuario pueda observar en detalle cada instrumento. Figura 6.34: Selección de instrumentos Luego de esto, se deberá rellenar la información correspondiente a cada metodología como ya se explico en la sección anterior, esto con el fin de obtener todos los parámetros necesarios para calcular los precios teóricos según cada metodología (Ver Figuras 6.35, 6.36, 6.37, 6.38 y 6.39). Figura 6.35: Metodologías Figura 6.36: Metodología Nelson y Siegel Figura 6.37: Metodología Svensson Figura 6.38: Metodología Diebold-Li Figura 6.39: Metodología splines 6.1.8 Precios Estimados Una vez completados todos los parámetros necesarios, los precios obtenidos se presentan en esta sección (Ver Figura 6.40). Figura 6.40: Comparativo de precios 6.1.9 Curvas En esta sección se presenta un grafico comparativo donde se grafican las curvas obtenidas mediante las diferentes metodologías. El usuario podrá descargar un reporte en PDF, con los resultados de los precios teóricos obtenidos en cada metodología (Ver Figura 6.41). Figura 6.41: Comparativo de curvas 6.2 Valor en Riesgo En esta sección el usuario podrá calcular el Valor en Riesgo de los instrumentos financieros que considere, el mismo será calculado mediante tres maneras diferentes las cuales se basan en tres distintas metodologías (Ver Figura 6.42). El primer VaR a calcular es el VaR paramétrico el cuál se basa en asumir una distribucuión normal de los instrumentos sconsiderados. El segundo VaR es el calculado por simulación histórica. El tercer y último VaR es el calculado mediante la simulación de MonteCarlo, este VaR se subdividirá en dos casos, el primero es asumiendo una distribución Normal para todos los instrumentos, mientras que el segundo es elegiendo una distribución en específico. Figura 6.42: Sección valor en riesgo 6.2.1 Datos En esta sección el usuario debe ingresar el histórico de precios, así como la posición de cada uno de los instrumentos a considerar (Ver Figura 6.43), dichos documentos deben poseer las siguientes características, Para el archivo de precios: El formato del archivo debe ser txt. El archivo debe contener al menos 252 observaciones por cada instrumento. El archivo debe contener tantas columnas como instrumentos se consideren. La primera columna del archivo deberá contener las fechas de las observaciones (precios), dichas observaciones deben estar ordenadas en forma decreciente. El formatod de la fecha debe ser: Año-Mes_Día (ej: 2019-06-07). El resto de las columnas deben representar información para cada instrumento, es decir, cada columna debe contener las observaciones de cada título. Figura 6.43: Data precios Para el archivo de posiciones: El formato del archivo debe ser txt. El archivo debe tener dos columnas, la primera debe contener el nombre corto del instrumentos. Por su parte la segunda columna deberá contener la posición del instrumento. En caso de existir decimales usar “.” como separación (Ver Figura 6.44). Figura 6.44: Data posiciones Es importante que la información sobre los instrumentos considerada en el archivo de precios y el archivo de posiciones sea la misma, es decir, los títulos que aparecen en el archivo de precios deben ser los mismos que aparecen en el archivo de posiciones. Una ventana ubicada en esta sección (Aviso) realizará esta validación, en caso de existir una discrepancia no se realizará ningún cálculo (Ver Figura 6.45). Figura 6.45: Aviso data 6.2.2 Distribución En está sección el usuario debe seleccionar las distribuciones asociadas a los rendimientos cada instrumento, con el fin de proceder y realizaf los cáculos del VaR de simulación de MonteCarlo. Para esto existen dos opciones: Elegir Distribución: En esta subsección al inicio se muestra una advertencia referente a si existe problemas con los precios ingresados, usualmente estos problemas surgen cuando existen dos precios iguales y ellos están seguidos. En caso de existir este problema, el instrumento ó instrumentos involucrados serán excluidos del estudio (Ver Figura 6.46). Posteriormente el usuario debe: Seleccionar el instrumento, para el cual se realizará la prueba de bondad de ajuste. Una vez seleccionado el instrumento se desplegará una ventana que mostrará las distribuciones que más se asemejan a los rendimientos de los precios del instrumento en estudio. Figura 6.46: Elección instrumento Una vez generado las posibles opciones el usuario debe seleccionar una distribución, luego de esto se mostrarán los parámetros obtenidos a partir de dicho ajuste (Ver Figura 6.47). Figura 6.47: Ajuste de distribución Se debe seleccionar “Si”, en el recuadro que pregunta si desea guardar su distribución. Luego de esto la distribución de cada instrumento se guardará para su posterior uso. Esta serie de pasos debe repetirse por cada instrumento considerado (Ver Figura 6.48). Figura 6.48: Distribuciones Seleccionar un archivo que contenga las distribuciones: En caso de elegir esta opción el usuario debe cargar un archivo con las siguientes características: Debe ser un archivo de texto txt. El archivo constará de dos filas, la primera fila debe contener el nombre corto del instrumento a considerar. La segunda fila debe contener el nombre de la distribución que mejor se ajuste, la misma debe ser alguna de las siguientes y debe ser expresada como sigue: Normal = Distribución Normal. Logistic = Distribución Logística. Exponential = Distribución Exponencial. Cauchy = Distribución Cauchy. Gamma = Distribución Gamma. Lognormal = Distribución Lognormal. Weibull = Distribución Weibull. Student = Distribución t-student. 6.2.3 VaR En esta sección se realizan todos los cálculos referentes al VaR, el mismo se calculará por tres metodologías, la primera es la metodología del VaR paramétrico, la segunda es la metodología del VaR por simulación histórica y la tercera es la metodología de VaR por simulación de MonteCarlo. En esta sección se muestran tres pestañas en donde se cálculan las metodologías explicadas anteriormente. Figura 6.49: Pestañas sección VaR Pestaña Paramétrico: en esta pestaña se muestra la siguiente información, Rendimientos de cada instrumento, en caso de existir algún problema con un instrumento se mostrará un mensaje de advertencia (Ver Figura 6.50). Figura 6.50: Rendimientos Parámetros seleccionados para cada instrumento suponiendo una distribución Normal (Ver Figura 6.51). Figura 6.51: Advertencias y parámetros Lista de selección, donde el usuario debe elegir el nivel de confianza del VaR, ya sea 90, 95 ó 99% (Ver Figura 6.52). Figura 6.52: Nivel de confianza Luego se seleccionar este valor, se mostrará una tabla con los vares individuales, desviación estándar y el porcentaje que representa cada VaR individual sobre el portafolio. Finalmente se muestra el valor del VaR de portafolio (Ver Figura 6.53). Figura 6.53: VaRes Pestaña Histórico: en esta pestaña se muestra la siguiente información, Una advertencia en caso de existir algún problema con algún instrumento. Pesos de cada instrumento, calculado a partir de su proporción de valor nominal con respecto al total de instrumentos (Ver Figura 6.54). Figura 6.54: Pesos Valor nominal del portafolio. Suma de pesos (Ver Figura 6.55). Figura 6.55: Valor nominal y suma de pesos Escenarios, se refiere a las variaciones que ha sufrido el valor de portafolio de manera diaria esto debido a la variación de los precios de los instrumentos (Ver Figura 6.56). Figura 6.56: Escenarios Lista de selección, donde el usuario debe elegir el nivel de confianza del VaR, ya sea 90, 95 ó 99%. Ubicación de la observación a ser considerada para el cálculo del VaR por simulación Histórica (Ver Figura 6.57). Figura 6.57: Nivel de confianza y ubicación Resultados de los vares individuales, donde se mostrará el valor nominal y el VaR para cada instrumento. VaR del portafolio (Ver Figura 6.58). Figura 6.58: VaRes Pestaña Simulación MonteCarlo: esta pestaña se subdivide en dos secciones (Ver Figura 6.59), Figura 6.59: Pestañas simulación MonteCarlo VaR Simulación MonteCarlo asumiendo normalidad: en esta sección se muestra la siguiente información, Rendimientos de cada instrumento, en caso de existir algún problema con un instrumento se mostrará un mensaje de advertencia (Ver Figura 6.60). Figura 6.60: Rendimientos Parámetros seleccionados para cada instrumento suponiendo una distribución Normal. Lista de selección, donde el usuario debe elegir el nivel de confianza del VaR, ya sea 90, 95 ó 99% (Ver Figura 6.61). Figura 6.61: Parámetros y nivel de confianza Lista de selección, donde el usuario debe elegir la cantidad de simulaciones que desea realizar (Ver Figura 6.62). Figura 6.62: Cantidad de simulaciones Resultados de los vares individuales obtenidos mediante esta metodología. Donde se muestra el nombre del instrumento, el valor nominal, el var individual y el porcentaje de cada VaR con respecto al portafolio. Resultado del VaR de portafolio (Ver Figura 6.63). Figura 6.63: VaRes VaR Simulación MonteCarlo considerando mejor distribución: en esta sección se muestra la siguiente información, Rendimientos de cada instrumento, en caso de existir algún problema con un instrumento se mostrará un mensaje de advertencia (Ver Figura 6.64). Figura 6.64: Rendimientos Distribuciones seleccionadas para cada instrumento, ya sea seleccionadas mediante la aplicación o mediante un archivo ingresado por el usuario. Lista de selección, donde el usuario debe elegir el nivel de confianza del VaR, ya sea 90, 95 ó 99% (Ver Figura 6.65). Figura 6.65: Distribuciones y nivel de confianza Lista de selección, donde el usuario debe elegir la cantidad de simulaciones que desea realizar (Ver Figura 6.66). Figura 6.66: Cantidad de simulaciones Resultados del VaR individual. Resultado del VaR de portafolio (Ver Figura 6.67). Figura 6.67: VaRes 6.2.4 Gráficos En esta sección se subdivide en (Ver Figura 6.68), Figura 6.68: Pestañas sección Gráficos Pestaña Valor nominal: en esta subsección se muestra un gráfico de torta donde se muestra el valor nominal de cada instrumento (Ver Figura 6.69). Figura 6.69: Gráfico valor nominal VaRes: esta subsección presenta los resultados de los VaRes obtenidos mediante las diferentes metodologías, la misma se divide en (Ver Figura 6.70), Figura 6.70: Pestañs VaRes VaR Paramétrico: en esta subsección se muestran los gráficos correspondientes a esta metodología. La misma se subdivide en (Ver Figura 6.71), Figura 6.71: Pestañas VaR paramétrico VaRes individuales: se muestra un gráfico de torta donde se muestra información sobre los VaRes individuales obtenidos (Ver Figura 6.72). Figura 6.72: VaRes individuales Comparativo: se muestra un gráfico comparativo donde se compara el VaR de portafolio y la suma de los VaRes individuales (Ver Figura 6.73). Figura 6.73: Comparativo VaR Simulación Histórica: esta subsección muestra los siguientes gráficos (Ver Figura 6.74), Figura 6.74: Pestañas VaR simulación por histórica Escenarios: muestra un histograma mediante el cual se muestran los diferentes escenarios obtenidos (Ver Figura 6.75). Figura 6.75: Histograma escenarios VaRes individuales: muestra un gráfico de torta donde se muestra información sobre los Vares individuales obtenidos (Ver Figura 6.76). Figura 6.76: VaRes individuales Comparativo: se muestra un gráfico comparativo donde se compara el VaR de portafolio y la suma de los VaRes individuales (Ver Figura 6.77). Figura 6.77: Comparativo VaR Simulación MonteCarlo: esta subsección se divide en dos (Ver Figura 6.78), Figura 6.78: Pestañas VaR simulación MonteCarlo Distribución Normal: se muestra los resultados, asumiendo una distribución normal (Ver Figura 6.79), Figura 6.79: Pestañas VaR simulación MonteCarlo normal Escenarios: muestra un histograma mediante el cual se muestran los diferentes escenarios obtenidos (Ver Figura 6.80). Figura 6.80: Histograma escenarios VaRes individuales: muestra un gráfico de torta donde se muestra información sobre los Vares individuales obtenidos (Ver Figura 6.81). Figura 6.81: VaRes individuales Comparativo: se muestra un gráfico comparativo donde se compara el VaR de portafolio y la suma de los VaRes individuales (Ver Figura 6.82). Figura 6.82: Comparativo Distribución Elegida: se muestran los resultados, asumiendo la mejor distribución (Ver Figura 6.83), Figura 6.83: Pestañas VaR simulación MonteCarlo mejor distribución Escenarios: muestra un histograma mediante el cual se muestran los diferentes escenarios obtenidos (Ver Figura 6.84). Figura 6.84: Histograma escenarios VaRes individuales: muestra un gráfico de torta donde se muestra información sobre los Vares individuales obtenidos (Ver Figura 6.85). Figura 6.85: VaRes individuales Comparativo: se muestra un gráfico comparativo donde se compara el VaR de portafolio y la suma de los VaRes individuales (Ver Figura 6.86). Figura 6.86: Comparativo Comparativo Vares: en esta subsección se muestra un gráfico comparativo del VaR de portafolio obtenido para cada metodología (Ver Figura 6.87). Figura 6.87: Comparativo VaRes 6.2.5 Históricos Esta sección permite calcular el VaR para un rango de fechas en específico, siempre y cuando las mismas se encuentren disponibles en la data histórica considerada. La misma cuenta con las siguientes secciones (Ver Figura 6.88), Figura 6.88: Pestañas sección Históricos VaR Paramétrico: esta subsección permite calcular el histórico de los VaRes mediante el uso de la metodología del VaR paramétrico, la cual se basa en asumir una distribución normal para cada instrumento. Dentro de esta subsección se encuentra (Ver Figura 6.89), Rango de fechas disponibles, el cual es obtenido a partir de la data cargada en la sección Datos. Opción para seleccionar el rango de fechas deseado, es importante acotar que la fecha inicial debe ser siempre menor que la fecha final, y las mismas no se pueden salir del rango establecido en el punto anterior. Elección realizada por el usuario. Histórico generado por la metodología del Var paramétrico, consta de dos columnas la primera indica la fecha, mientras que la segunda indica el valor del VaR para ese día en específico. Figura 6.89: Sección Históricos VaR Histórico: esta subsección permite calcular el histórico de los Vares mediante el uso de la metodología del VaR por simulación histórica, la cual se caracteriza por no asignar ninguna distribución conocida, sino que trabaja con la distribución empírica de los datos. Dentro de esta subsección se encuentra: Rango de fechas disponibles, el cual es obtenido a partir de la data cargada en la sección Datos. Opción para seleccionar el rango de fechas deseado, es importante acotar que la fecha inicial debe ser siempre menor que la fecha final, y las mismas no se pueden salir del rango establecido en el punto anterior. Elección realizada por el usuario. Histórico generado por la metodología del Var por simulación histórica, consta de dos columnas la primera indica la fecha, mientras que la segunda indica el valor del VaR para ese día en específico. VaR SMC Normal: esta subsección permite calcular el histórico de los Vares mediante el uso de la metodología del VaR por simulación de MonteCarlo asumiendo una distribicón normal para cada instrumento, esta metodología se caracteriza por el empleo de simulaciones y el cálculo de diferentes escenarios con el fin de calcular el valor del VaR. Dentro de esta subsección se encuentra: Rango de fechas disponibles, el cual es obtenido a partir de la data cargada en la sección Datos. Opción para seleccionar el rango de fechas deseado, es importante acotar que la fecha inicial debe ser siempre menor que la fecha final, y las mismas no se pueden salir del rango establecido en el punto anterior. Elección realizada por el usuario. Histórico generado por la metodología del Var por simulación de MonteCarlo asumiendo una distribución normal, consta de dos columnas la primera indica la fecha, mientras que la segunda indica el valor del VaR para ese día en específico. VaR SMC Mejor Distribución: esta subsección permite calcular el histórico de los Vares mediante el uso de la metodología del VaR por simulación de MonteCarlo considerando la mejor distribución para cada instrumento, las distribuciones consideradas serán las elegidas por el usuario en la sección Distribución. Dentro de esta subsección se encuentra: Rango de fechas disponibles, el cual es obtenido a partir de la data cargada en la sección Datos. Opción para seleccionar el rango de fechas deseado, es importante acotar que la fecha inicial debe ser siempre menor que la fecha final, y las mismas no se pueden salir del rango establecido en el punto anterior. Elección realizada por el usuario. Histórico generado por la metodología del Var por simulación de MonteCarlo considerando la mejor distribución, consta de dos columnas la primera indica la fecha, mientras que la segunda indica el valor del VaR para ese día en específico. 6.3 Backtesting Esta sección permite realizar diversas pruebas al Valor en Riesgo y así poder calibrar y analizar su comportamiento, entre estas pruebas se encuentran el test de Kupiec y el test de Haas (Ver Figura 6.90). Figura 6.90: Pestañas sección Backtesting 6.3.1 Datos En esta subsección el usuario debe ingresar los datos con los cuales se va a trabajar (Ver Figura 6.91). Este archivo debe contar con las siguientes características: Debe ser un archivo de texto txt. Debe contar con tres columnas, la primera debe proporcionar información sobre la fecha (ej: 27/03/2018), la segunda columna debe contener información sobre el VaR para ese día. Finalmente, la tercera columna debe indicar información sobre el valor nominal del portafolio para el día considerado. Es importante mencionar que el archivo debe contener 252 observaciones (Ver Figura 6.92). Figura 6.91: Datos Figura 6.92: Data de prueba 6.3.2 Resultados En esta subsección se muestran los resultados obtenidos para las diferentes pruebas. El usuario debe seleccionar el nivel de confianza para realizar la prueba. Una vez seleccionado este valor, los resultados se muestran con la siguiente estructura (Ver Figura 6.93), Número de excepciones negativas, número de excepciones dentro del rango VaR y número total de excepciones. Tiempo entre excepciones. Resultados de los estadísticos para las pruebas de Kupiec, Haas y prueba mixta. Valor crítico asociado a cada prueba. Resultado de cada prueba, obtenido al realizar una comparación entre el valor crítico y el valor del estadístico de cada prueba. Figura 6.93: Resultados 6.4 Valoración Esta sección permite realizar los cáculos de valoración de un portafolio, a partir de sus precios y valor nominal para un día determinado en términos de utilidad y pérdida. De igual manera, esta sección permite realizar una prueba de estrés y saber como se comportará la utilidad o périda del portafolio en caso de una posible caida de los precios (Ver Figura 6.94). Figura 6.94: Pestañas sección Valoración 6.4.1 Datos En esta subsección el usuario debe ingresar la data (Ver Figura 6.95) con la cual se va a trabajar, la misma debe tener la siguiente estructura (Ver Figura 6.96), Deber ser un archivo plano txt. Debe contener cinco columnas, cada una se explica a continuación, La primera columna, debe indicar el nombre corto del instrumento en consideración. La segunda columna, debe indicar el tipo de instrumento que se está considerando. La tercera columna, debe indicar el valor nominal asociado a cada instrumento. La cuarta columna debe indicar el precio al día de hoy de cada instrumento. La quinta columna debe indicar el precio de mercado (precio obtenido por alguna metodología de estimación de precios teóricos). El archivo debe contener tantas filas como instrumentos se desee considerar. Figura 6.95: Datos Figura 6.96: Datos de prueba 6.4.2 Resultados En esta subsección se muestra los resultados de la valoración, la información está distribuida en dos tablas, La primera tabla muestra información acerca del monto invertido en cada instrumento y su valor actual, considerando el precio teórico elegido. También se muestra la ganancia ó perdida que se tiene actualmente con un instrumento. En esta tabla se presenta la información para cada instrumento por separado (Ver Figura 6.97). Figura 6.97: Resultados individuales La segunda tabla, muestra un resumen para todo el portafolio, se presenta la suma del valor nominal por tipo de instrumento, el precio promedio ponderado por el valor nominal dpara cada isntrumento, y la ganancia ó pérdida obtenida para cada tipo de instrumento considerado (Ver Figura 6.98). Figura 6.98: Resultados portafolio 6.4.3 Resultados prueba de estrés En esta subsección se muestra los resultados de la valoración luego de estresar en forma negativa los precios de mercado de los instrumentos financieros considerados. Con el fin de estresar los precios de mercado es necesario incluir una data de precios históricos (Ver Figura 6.99), dicha data debe poseer la siguiente estructura (Ver Figura 6.100), Debe ser un archivo plano txt. La primera columna debe señalar la fecha del precio en cuestión. Cada columna debe representar la historia para cada instrumento. El archivo debe contener la misma cantidad de instrumentos considerados en la data anterior. Figura 6.99: Datos Figura 6.100: Datos de prueba Figura 6.101: Advertencia La información está distribuida en dos tablas, La primera tabla muestra información acerca del monto invertido en cada instrumento, su valor actual, el precio teórico elegido, la desviación estandar de cada instrumento, el precio estresado, el Mark to Market (MTM), y la ganancia o pérdida considerando los precios teóricos iniciales y los precios teóricos estresados (Ver Figura 6.102). Figura 6.102: Resultados individuales estresados La segunda tabla, muestra un resumen para todo el portafolio, se presenta la suma del valor nominal por tipo de instrumento, el precio promedio ponderado por el valor nominal para cada instrumento, y la ganancia ó pérdida obtenida para cada tipo de instrumento considerado los precios de mercado y los precios estresados (Ver Figura 6.103). Figura 6.103: Resultados portafolio estresados "],
["software-tools.html", "A Software Tools A.1 R and R packages A.2 Pandoc A.3 LaTeX", " A Software Tools For those who are not familiar with software packages required for using R Markdown, we give a brief introduction to the installation and maintenance of these packages. A.1 R and R packages R can be downloaded and installed from any CRAN (the Comprehensive R Archive Network) mirrors, e.g., https://cran.rstudio.com. Please note that there will be a few new releases of R every year, and you may want to upgrade R occasionally. To install the bookdown package, you can type this in R: install.packages(&quot;bookdown&quot;) This installs all required R packages. You can also choose to install all optional packages as well, if you do not care too much about whether these packages will actually be used to compile your book (such as htmlwidgets): install.packages(&quot;bookdown&quot;, dependencies = TRUE) If you want to test the development version of bookdown on GitHub, you need to install devtools first: if (!requireNamespace(&#39;devtools&#39;)) install.packages(&#39;devtools&#39;) devtools::install_github(&#39;rstudio/bookdown&#39;) R packages are also often constantly updated on CRAN or GitHub, so you may want to update them once in a while: update.packages(ask = FALSE) Although it is not required, the RStudio IDE can make a lot of things much easier when you work on R-related projects. The RStudio IDE can be downloaded from https://www.rstudio.com. A.2 Pandoc An R Markdown document (*.Rmd) is first compiled to Markdown (*.md) through the knitr package, and then Markdown is compiled to other output formats (such as LaTeX or HTML) through Pandoc. This process is automated by the rmarkdown package. You do not need to install knitr or rmarkdown separately, because they are the required packages of bookdown and will be automatically installed when you install bookdown. However, Pandoc is not an R package, so it will not be automatically installed when you install bookdown. You can follow the installation instructions on the Pandoc homepage (http://pandoc.org) to install Pandoc, but if you use the RStudio IDE, you do not really need to install Pandoc separately, because RStudio includes a copy of Pandoc. The Pandoc version number can be obtained via: rmarkdown::pandoc_version() ## [1] &#39;2.3.1&#39; If you find this version too low and there are Pandoc features only in a later version, you can install the later version of Pandoc, and rmarkdown will call the newer version instead of its built-in version. A.3 LaTeX LaTeX is required only if you want to convert your book to PDF. The typical choice of the LaTeX distribution depends on your operating system. Windows users may consider MiKTeX (http://miktex.org), Mac OS X users can install MacTeX (http://www.tug.org/mactex/), and Linux users can install TeXLive (http://www.tug.org/texlive). See https://www.latex-project.org/get/ for more information about LaTeX and its installation. Most LaTeX distributions provide a minimal/basic package and a full package. You can install the basic package if you have limited disk space and know how to install LaTeX packages later. The full package is often significantly larger in size, since it contains all LaTeX packages, and you are unlikely to run into the problem of missing packages in LaTeX. LaTeX error messages may be obscure to beginners, but you may find solutions by searching for the error message online (you have good chances of ending up on StackExchange). In fact, the LaTeX code converted from R Markdown should be safe enough and you should not frequently run into LaTeX problems unless you introduced raw LaTeX content in your Rmd documents. The most common LaTeX problem should be missing LaTeX packages, and the error may look like this: ! LaTeX Error: File `titling.sty&#39; not found. Type X to quit or &lt;RETURN&gt; to proceed, or enter new name. (Default extension: sty) Enter file name: ! Emergency stop. &lt;read *&gt; l.107 ^^M pandoc: Error producing PDF Error: pandoc document conversion failed with error 43 Execution halted This means you used a package that contains titling.sty, but it was not installed. LaTeX package names are often the same as the *.sty filenames, so in this case, you can try to install the titling package. Both MiKTeX and MacTeX provide a graphical user interface to manage packages. You can find the MiKTeX package manager from the start menu, and MacTeX’s package manager from the application “TeX Live Utility”. Type the name of the package, or the filename to search for the package and install it. TeXLive may be a little trickier: if you use the pre-built TeXLive packages of your Linux distribution, you need to search in the package repository and your keywords may match other non-LaTeX packages. Personally, I find it frustrating to use the pre-built collections of packages on Linux, and much easier to install TeXLive from source, in which case you can manage packages using the tlmgr command. For example, you can search for titling.sty from the TeXLive package repository: tlmgr search --global --file titling.sty # titling: # texmf-dist/tex/latex/titling/titling.sty Once you have figured out the package name, you can install it by: tlmgr install titling # may require sudo LaTeX distributions and packages are also updated from time to time, and you may consider updating them especially when you run into LaTeX problems. You can find out the version of your LaTeX distribution by: system(&#39;pdflatex --version&#39;) ## pdfTeX 3.14159265-2.6-1.40.19 (TeX Live 2018) ## kpathsea version 6.3.0 ## Copyright 2018 Han The Thanh (pdfTeX) et al. ## There is NO warranty. Redistribution of this software is ## covered by the terms of both the pdfTeX copyright and ## the Lesser GNU General Public License. ## For more information about these matters, see the file ## named COPYING and the pdfTeX source. ## Primary author of pdfTeX: Han The Thanh (pdfTeX) et al. ## Compiled with libpng 1.6.34; using libpng 1.6.34 ## Compiled with zlib 1.2.11; using zlib 1.2.11 ## Compiled with xpdf version 4.00 "],
["referencias.html", "Referencias", " Referencias Abramovich, F. y Steinberg, D., Improved Inference in Nonparametric Regression using Lk Smoothing Splines. Journal of Statistical Planning, 49: 327-341, 1996. Maita B. Miriam A., Estimación de una curva de rendimientos para los bonos de la deuda pública interna en Venezuela, (trabajo de grado Maestría en Administración de Empresas mención Finanzas). Universidad Católica Andrés Bello, Caracas, Venezuela, 2010. Nelson, C. y Siegel, A. Parsimonius Modeling of Yield Curves. Journal of Business, \\(60\\): \\(473\\)-\\(489\\), 1987. Douglas, L.G. Yield Curve Analysis. New York Institute of Finance, 1988. Svensson, L. Estimating and Interpreting Forward Interest Rates: Sweden 1992-1994, NBER Working Papers, 4871. Estocolmo: National Bureau of Economic Research, 1994. Hunt, B. y Terry, C. Zero-Coupon Yield Curve Estimation: A Principal Component-Polynomial Approach, Technical report 81. Sydney: University of Technology Sydney - School of Finance and Economics, 1998. Hunt, B. Modelling the Yields on Australian Coupon Paying Bonds, Technical report 9. Sydney: University of Technology Sydney - School of Finance and Economics, 1995. Haykin, S. Neural Networks. New York: McMillan College Publishing Company, 1994. Isasi, P. y Galván, I. Redes neuronales artificiales: un enfoque práctico.. Madrid: Pearson-Prentice Hall, 2004. Kaastra, I. y Boyd, M. Design a Neural Network for Forecasting Financial and Economic Time Series. Neurocomputing 10: 215-236, 1996. Nocedal, J. y Wright, S. Numerical Optimization. New York: Springer-Verlag, 1999. Fan, J. y Gijbels, I. Local Polynomial Modelling and Its Applications. New York: Chapman and Hall, 1996. B.W. Silverman. Some Aspects of the Spline Smoothing Approach to Non-Parametric Regression Curve Fitting. Journal of the Royal Statistical Society. Series B (Methodological), Vol. 47, No. 1, 1(1985), pp. 1-52, 1984. Friedman, J. H. A Variable Span Smoother, Technical report 5. Standford: Standford University - Departament of Statistics, 1984. Sharda, R. Neural networks for the MS/OR analyst: An application bibliography. Interfaces, 24(2): 116-130, 1994. Suykens, J.; Vandewalle, J. y Moor, B. D. Artificial Neural Networks for Modelling and Control of Nonlinear Systems.. Boston: Kluwer Academic Publishers, 1996. Fernández J.L., Robles, M.D. Teoría de las Expectativas y Cambio Estructural: nueva evidencia en los tipos a corto plazo españoles. Tribuna de Economía No. 827. ICE, 2005. Yung-Shi Liau, Jack J.W. Yang. The Expectations Hypothesis of Term Structure of Interest Rates in Taiwan’s Money Market. International Research Journal of Finance and Economics, 2009. Zhang, G.; Patuwo, B. y Hu, Y. Forecasting with Artificial Neural Networks: The State of Art. International Journal of Forecasting, 14: 35-62, 1998. "]
]
